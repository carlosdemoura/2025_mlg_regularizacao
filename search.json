[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regularização em MLGs",
    "section": "",
    "text": "Sobre\n\n“Eu tô te explicando Pra te confundir Eu tô te confundindo Pra te esclarecer” Tom Zé\n\nEste é o material auxiliar da apresentação do trabalho final do curso de Modelos Lineares Generalizados (MLGs) (DEST-UFMG, 2025/2). O tema é regularização em MLGs, em específico os métodos de regularização ridge, lasso e elastic net. Este quarto book está dividido da seguinte forma:\n\nDefinição de regularização e apresentação dos métodos de shrinkage, com exemplos de regressão normal;\nEstimação dos parâmetros em MLG com penalização;\nTunagem dos parâmetros de regularização;\nExemplo prático com dados reais no R.\n\nReferências bibliográficas importantes que foram usadas para a feitura desse trabalho são citadas ao final do documento.\nAlguns pacotes que usaremos estão abaixo listados.\n\nif (!{\"pak\" %in% rownames(installed.packages())}) install.packages(\"pak\")\n\npak::pak(c(\"matrixcalc\", \"glmnet\", \"tidyverse\", \"mvtnorm\", \"ggplot2\"))\n\n\n\nMotivação\nPorquê fazer seleção de modelos?\n\nCom o advento da big data, cada vez mais aparecem problemas de regressão com muitas variáveis;\nMuitas variáveis podem estar altamente correlacionadas de modo que elas dizem a mesma coisa sobre o problema: precisamos de uma técnica para selecionar variáveis (preferência por modelos parcimoniosos);\nSe há muitas covariáveis correlacionadas, os modelos de estimação convencionais podem ter problemas e confundir a origem dos efeitos de regressão (falta de acurácia);\nA inclusão de muitas covariáveis no modelo tem o efeito conhecido de aumento da variância, fazendo com que as previsões sejam mais incertas.\n\nPorquê regularizar coeficientes é uma boa ideia?\nA regularização - regularization ou shrinkage (diminuição) em inglês - de coeficientes é feita ajustando-se um modelo com todos os coeficientes possíveis, mas há algum mecanismo que que dificulta fazer com que todos eles possam estar longe de zero. Por isso os coeficientes diminuem (honey, i shrunk the kids!) em relação à estimação de mínimos quadrados ordinária. Em alguns casos eles são estimados como zero mesmo! Isso tem o efeito de reduzir a variância das estimações e produzir modelos mais parcimoniosos.\n\n\n\n\nFriedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1–22. Obtido de https://www.jstatsoft.org/index.php/jss/article/view/v033i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., et al. (2025). glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. Stanford University. Obtido de https://glmnet.stanford.edu/\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55–67. Obtido de https://www.jstor.org/stable/1267351\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. Springer Texts em Statistics. Springer. Obtido de https://link.springer.com/book/10.1007/978-1-0716-1418-1\n\n\nKuhn, M., & Silge, J. (2022). Tidy Modeling with R. O’Reilly Media. Obtido de https://www.tmwr.org/\n\n\nR Core Team. (2024). R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. Obtido de https://www.r-project.org/\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic Net Regularization Paths for All Generalized Linear Models. Journal of Statistical Software, 106(1), 1–31. Obtido de https://www.jstatsoft.org/article/view/v106i01\n\n\nTibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Obtido de https://www.jstor.org/stable/2346178\n\n\nZou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2), 301–320. Obtido de https://www.jstor.org/stable/3647580",
    "crumbs": [
      "Sobre"
    ]
  },
  {
    "objectID": "qmd/regularização.html",
    "href": "qmd/regularização.html",
    "title": "1  Regularização",
    "section": "",
    "text": "1.1 Seleção de variáveis naïve\nA primeira ideia que alguém pode ter ao querer fazer seleção de modelo é uma ideia estupidamente simples: ajustar cada modelo possível e escolher o melhor. Essa abordagem tem alguns problemas.\nCódigo\nf = function(n) {\n  s = 0\n  for (i in 1:n) {\n    s = s + choose(n, i)\n  }\n  s\n}\n\nseq(1, 20, 1) |&gt;\n  lapply(f) |&gt;\n  unlist() |&gt;\n  plot(type = 'l',\n       xlab = \"número de covariáveis numéricas\",\n       ylab = \"número de possíveis modelos\",\n       main = \"grau de complexidade da seleção de modelos naïve\"\n       )\nA seleção de variáveis stepwise lida com esse problema usando o fato que muito desses modelos são encaixados, e em seguida verificando o efeito que a inclusão e/ou exclusão de covariáveis tem nas medidas de ajuste do modelo. Isso evita o ajuste de uma quantidade muito grande de modelos, mas em casos patológicos pode não ser suficiente - e nem é garantido que o melhor modelo será selecionado.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regularização</span>"
    ]
  },
  {
    "objectID": "qmd/regularização.html#seleção-de-variáveis-naïve",
    "href": "qmd/regularização.html#seleção-de-variáveis-naïve",
    "title": "1  Regularização",
    "section": "",
    "text": "O número de modelos possíveis é dado por \\(\\sum_{i=0}^{n} {n \\choose i} \\approx \\mathcal{O} (n!)\\) (\\(n\\) é o número de covariáveis disponíveis), que cresce exponencialmente com \\(n\\).\nAlguma métrica de ajuste deve ser eleita como a métrica de seleção de modelo: \\(R^2\\), \\(EQM_{pred}\\), \\(C_p\\) de Mallow, \\((A/B)IC\\). Se todas as métricas apontam para o mesmo modelo, mazel tov!; se não, em qual confiar?\nAcúmulo de erros: alguma correção (Bonferroni, Tukey, etc.) deveria ser feita ao analisar os p-valores.\nEngenheiros (e, pior, economistas) gostam dessa ideia.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regularização</span>"
    ]
  },
  {
    "objectID": "qmd/regularização.html#ridge",
    "href": "qmd/regularização.html#ridge",
    "title": "1  Regularização",
    "section": "1.2 Ridge",
    "text": "1.2 Ridge\nA técnica Ridge foi a primeira das três técnicas a surgir, no trabalho de Hoerl & Kennard (1970). Originalmente, os autores buscavam entender como lidar com problemas em que a matriz de covariáveis \\(X\\) estava mal-especificada. Relembrando que na regressão linear normal, temos que\n\\[\nY = X \\beta + \\epsilon, \\hspace{1cm} \\epsilon \\sim N_n\\bigr( 0, \\sigma^2 I_n \\bigr)\n\\] O estimador de máxima verossimilhança (EMV) de \\(\\beta\\) será o mesmo estimador de mínimos quadrados (EMQ).\nSeja \\(SQRes = (Y - X\\beta)^\\top(Y - X\\beta) = \\sum_{i=1}^n \\left( y_i - X_i \\beta \\right)^2 = \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2\\). Assim,\n\\[\n\\hat\\beta_{OLS} = \\arg\\min_{\\beta} \\left\\{ SQRes \\right\\}= (X^\\top X)^{-1} X^\\top Y,\n\\]\nse:\n\n\\(X^\\top X\\) for inversível;\nA matriz de covariáveis é ortogonalizável, i.e., os dados foram coletados de maneira independente;\nhá menos betas que observações, isto é ncol(X) &lt;&lt; nrow(X).\n\nAlém disso, \\(\\hat\\beta\\) é não viciado, consistente e tem matriz de covariâncias dada por\n\\[\ncov\\bigr(\\hat\\beta_{OLS}\\bigr) = \\sigma^2 (X^\\top X)^{-1}.\n\\]\nUma vez que temos a distribuição do estimador, fica fácil fazer inferência via intervalos de confiança, por exemplo.\n\\[\n\\hat\\beta_{OLS} \\sim N_p\\left(\\beta,\\; \\sigma^2 (X^\\top X)^{-1}\\right).\n\\]\nNesse sentido, se temos uma matriz de dados problemática - no sentido em que \\((X^\\top X)^{-1}\\) não está bem definida, teremos problema de estimação via EMQ.\nVeja o exemplo numérico abaixo.\n\nset.seed(12345)\n\nn = 10\nbeta = c(1, 0)\nX = cbind(1:n, 2*(1:n))\nY = X %*% beta + rnorm(n)\n\n\n\n\n\n\n\nDicaVer matriz X\n\n\n\n\n\n\nhead(X)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    4\n[3,]    3    6\n[4,]    4    8\n[5,]    5   10\n[6,]    6   12\n\nmatrixcalc::is.singular.matrix(t(X)%*%X)\n\n[1] TRUE\n\n\n\n\n\n\n\nCódigo\npar(mfrow=c(1,2))\nplot(Y~X[,1])\nplot(Y~X[,2])\n\n\n\n\n\n\n\n\n\n\nlm(Y~0+X)\n\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n    X1      X2  \n0.9544      NA  \n\n\n\n\n\n\n\n\nDicaE se a regressão estivesse na outra covariável?\n\n\n\n\n\n\nset.seed(12345)\n\nn = 10\nbeta = c(0, 1)\nX = cbind(1:n, 2*(1:n))\nY = X %*% beta + rnorm(n)\n\nlm(Y~0+X)\n\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n   X1     X2  \n1.954     NA  \n\n\nNote que, embora a estrutura de regressão esteja na segunda covariável, o modelo só conseguiu estimar um beta - e o alocou na primeira covariável. Apesar de esse exemplo ser simples - e mal especificado, uma vez que há dois modelos idêncicos com parâmetros diferentes -, serve de exemplo de como a matriz \\(X\\) é importante para a boa especificação do modelo.\n\n\n\n\nset.seed(12345)\n\nn = 10\nbeta = c(1, 0)\nX = cbind(1:n, 2*(1:n))\nruido = cbind(rep(0,n), rnorm(n,0,.1))\nX = X + ruido\nY = X %*% beta + rnorm(n)\n\n\n\n\n\n\n\nDicaVer matriz X\n\n\n\n\n\n\nhead(X)\n\n     [,1]      [,2]\n[1,]    1  2.058553\n[2,]    2  4.070947\n[3,]    3  5.989070\n[4,]    4  7.954650\n[5,]    5 10.060589\n[6,]    6 11.818204\n\nhead(ruido)\n\n     [,1]        [,2]\n[1,]    0  0.05855288\n[2,]    0  0.07094660\n[3,]    0 -0.01093033\n[4,]    0 -0.04534972\n[5,]    0  0.06058875\n[6,]    0 -0.18179560\n\nmatrixcalc::is.singular.matrix(t(X)%*%X)\n\n[1] FALSE\n\neigen(t(X)%*%X)$values\n\n[1] 1.918025e+03 1.070613e-02\n\n\n\n\n\n\nlm(Y~0+X) |&gt; summary()\n\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8382 -0.4545  0.1118  0.3098  1.9804 \n\nCoefficients:\n   Estimate Std. Error t value Pr(&gt;|t|)\nX1    6.658      7.724   0.862    0.414\nX2   -2.820      3.871  -0.729    0.487\n\nResidual standard error: 0.8939 on 8 degrees of freedom\nMultiple R-squared:  0.9846,    Adjusted R-squared:  0.9808 \nF-statistic: 256.5 on 2 and 8 DF,  p-value: 5.558e-08\n\n\nNesse segundo cenário, após a inclusão de um ruído pequeno, a colinearidade deixa de ser exata, mas permanece quase perfeita. Embora exista \\((X^\\top X)\\), os autovalores dassa matriz estão desbalanceados.\nDo ponto de vista da modelagem, esses exemplos reforçam a importância da parcimônia: incluir mais covariáveis não implica num modelo mais bem ajustado. Quando novas covariáveis carregam essencialmente a mesma informação já contida nas demais, o modelo se torna artificialmente mais complexo e mais imprevisível.\nA proposta de Hoerl & Kennard (1970) é adicionar um certo múltiplo da matriz identidade à \\(X^\\top X\\), de modo que seja possível superar os problemas de uma matriz \\(X\\) mal especificada. James, Witten, Hastie, & Tibshirani (2021) mostra que isso é equivalente a adicionar uma punição no processo de estimação por mínimos quadrados.\n\\[\n\\hat\\beta_{ridge} =\n\\arg\\min_{\\beta} \\left\\{ SQRes + \\lambda_{ridge}\\sum_{j=1}^p \\beta_j^2 \\right\\},\n\\]\nem que \\(\\lambda_{ridge}\\) é uma constante positiva (escolhida pelo pesquisador) que dá o peso dessa punição.\nNote que:\n\nÀ medida que \\(\\lambda\\) cresce, menores ficam as estimativas dos betas, \\(\\hat\\beta_{ridge} \\rightarrow 0,\\ \\lambda_{ridge} \\rightarrow \\infty\\).\nNão faz sentido incluir o intercepto no processo de shrinkage, pois \\(g(\\beta_0)\\) é o valor esperado da variável resposta dado que as demais covariáveis são zero, em que \\(g\\) é uma função de ligação.\nFica claro que a depender do valor de \\(\\lambda\\) que escolhemos, estamos incluindo algum viés no modelo.\nA estimação ridge diminui as estimativas dos betas na mesma proporção (se comparada com o MMQ tradicional).\nEm diminuindo a estimativa dos betas, diminui-se também a variância envolvida nas estimativas dos parâmetros.\n\n\n\n\n\n\n\nAviso\n\n\n\nAo usar métodos de regularização, estamos fazendo um trade-off entre variância e viés. Grosso modo, estamos inserido algum viés em nossas estimativas, a fim de diminuir a variância na estimação dos parâmetros e assim fazer inferências mais precisas.\nUm bom método de escolha do parâmetro de shrinkage é crucial para o bom funcionamento da estimação ridge. Falaremos disso nas próximas seções. Spoiler: faremos validação cruzada.\n\n\n\n\n\nAnalogia dos dardos para pensar variância e viés.\n\n\nO exemplo abaixo está no livro James et al. (2021), Credit é um conjunto de dados incluído no pacote ISLR, contendo informações sobre 4000 clientes de um banco, com o objetivo de modelar e entender fatores associados ao limite de crédito de cada pessoa.\n\n\n\nExemplo de regressão ridge.\n\n\n\n\n\n\n\n\nDicaPadronização das covariáveis\n\n\n\nA técnica ridge pode ser muito sensível à escala das covariáveis. Por isso, vale a pena padronizar as covariáveis pelo desvio-padrão para evitar a influência desse efeito.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regularização</span>"
    ]
  },
  {
    "objectID": "qmd/regularização.html#lasso",
    "href": "qmd/regularização.html#lasso",
    "title": "1  Regularização",
    "section": "1.3 Lasso",
    "text": "1.3 Lasso\nA regressão lasso surgiu com o artigo de Tibshirani (1996). No caso da regressão linear normal, a diferença entre lasso e ridge pode ser vista na função de perda da estimação mínimos quadrados. Em vez de usar a norma \\(\\ell_2\\) (como faz a regressão ridge), a regressão lasso usa a norma \\(\\ell_1\\) de beta.\n\\[\n\\hat\\beta_{lasso} =\n\\arg\\min_{\\beta} \\left\\{ SQRes + \\lambda_{lasso}\\sum_{j=1}^p |\\beta_j| \\right\\},\n\\]\nMuitas das observações que fizemos para a estimação ridge se aplicam também à regressão lasso. A principal diferença entre essas técnicas está no fato de que a regressão lasso faz, de fato, uma seleção de variáveis. Isso se dá pois a regressão lasso pode zerar os coeficientes de algumas covariáveis.\n\n\n\n\n\n\nNota\n\n\n\n\n\nO fato de a regressão lasso poder zerar os coeficientes das covariáveis está associado com a geometria imposta no espaço paramétrico. \n\n\n\nNo mesmo banco de dados Credit, James et al. (2021) aplicou a regressão lasso:\n\n\n\nExemplo de regressão lasso.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regularização</span>"
    ]
  },
  {
    "objectID": "qmd/regularização.html#comparação-das-técnicas",
    "href": "qmd/regularização.html#comparação-das-técnicas",
    "title": "1  Regularização",
    "section": "1.4 Comparação das técnicas",
    "text": "1.4 Comparação das técnicas\n\n“You Can’t Always Get What You Want”\n\nApesar de partirem de ideias semelhantes, há algumas diferenças práticas entre elas. De acordo com J. H. Friedman, Hastie, & Tibshirani (2010), a regressão ridge é conhecida por diminuir os coeficientes de covariáveis muito correlacionadas, permitindo que eles “tomem emprestada a força uns dos outros”. No caso extremo de \\(k\\) preditores iguais estimados juntos, a regressão ridge faria uma estimativa igual a \\(1/k\\) do que teríamos se fosse ajustado um modelo só com um deles (grouping effect). Nesse sentido, ela vai ter um melhor desempenho no caso em que temos muitos preditores com coeficientes pequenos (mas não nulos).\nA regressão lasso, por sua vez tende a tratar covariáveis muito correlacionadas como uma só fonte de variabilidade (escolhendo uma - nem sempre a melhor - para estimar o coeficiente e ignorando as outras). Assim, ela funciona melhor quando há esparsidade nos coeficientes, isto é, muitos deles são próximos a zero e só numa parte deles há de fato regressão. Isso é mais comum em grandes bancos de dados.\n\n1.4.1 Estudo sobre MSE com dados sintéticos\n\nsqres = function(beta, X, Y) {\n  res = Y - X %*% beta\n  drop(crossprod(res))\n}\n\nsqres_ridge = function(beta, X, Y, lambda) {\n  res = Y - X %*% beta\n  drop(crossprod(res) + lambda * crossprod(beta))\n}\n\nsqres_lasso = function(beta, X, Y, lambda) {\n  res = Y - X %*% beta\n  drop(crossprod(res) + lambda * sum(abs(beta)))\n}\n\n\n\n\n\n\n\nNota\n\n\n\n\n\nNote que minimizar sqres deve dar o mesmo resultado que lm.\n\nfit = optim(\n  par    = rep(0,ncol(X)),\n  fn     = sqres,\n  X      = X,\n  Y      = Y,\n  method = \"BFGS\"\n)\n\nround(fit$par, 5) |&gt; `names&lt;-`(paste0(\"X\", 1:ncol(X)))\n\n      X1       X2 \n 6.65825 -2.81988 \n\nlm(Y~0+X)\n\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n    X1      X2  \n 6.658  -2.820  \n\n\n\n\n\n\n\nCódigo\nset.seed(12345)\nn = 20\nbeta = exp(-seq(-2,2,.5)^2)\nX = rnorm(n*length(beta)) |&gt; matrix(nrow = n, ncol = length(beta))\nY = X %*% beta + rnorm(n,0,1)\n\nmse_ridge = mse_lasso = numeric()\n\nfor (lambda in 0:20) {\n  beta_ridge =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_ridge,\n      X      = X,\n      Y      = Y,\n      lambda = lambda,\n      method = \"BFGS\"\n    )$par\n\n    mse_ridge[lambda + 1] = mean((beta_ridge - beta)^2)\n\n  beta_lasso =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_lasso,\n      X      = X,\n      Y      = Y,\n      lambda = lambda,\n      method = \"BFGS\"\n    )$par\n\n    mse_lasso[lambda + 1] = mean((beta_lasso - beta)^2)\n}\n\nplot(0:(length(mse_ridge)-1), mse_ridge,  type=\"l\", lwd = 2, col = 1,\n     ylim = c(0, 0.1),\n     main = \"alguns parâmetros muito pequenos\",\n     ylab = \"MSE\", xlab = \"lambda\")\nlines(0:(length(mse_lasso)-1), mse_lasso, type=\"l\", lwd = 2, col = 2, lty = \"aa\")\n\nmse_lm = mean((lm(Y~0+X)$coeff - beta)^2)\nabline(h = mse_lm, lwd = 2, col = 3, lty = \"44\")\n\nlegend(\"topright\",\n       legend = c(\"Ridge\", \"Lasso\", \"OLS\"),\n       col = c(1, 2, 3),\n       lty = c(1, 2, 4),\n       lwd = 2,\n       bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDicaComparar estimativas\n\n\n\n\n\nDados gerados a partir de coeficientes pequenos (não nulos).\n\nbeta_lasso =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_lasso,\n      X      = X,\n      Y      = Y,\n      lambda = which.min(mse_lasso)-1,\n      method = \"BFGS\"\n    )$par\nbeta_ridge =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_ridge,\n      X      = X,\n      Y      = Y,\n      lambda = which.min(mse_ridge)-1,\n      method = \"BFGS\"\n    )$par\nbeta_lm = lm(Y~0+X)$coef\n\ncbind(beta, beta_lasso, beta_ridge, beta_lm) |&gt;\n  `colnames&lt;-`(c(\"real\", \"lasso\", \"ridge\", \"lm\")) |&gt;\n  knitr::kable(digits = 3, caption = \"Estimativas para betas (caso betas pequenos)\")\n\n\nEstimativas para betas (caso betas pequenos)\n\n\n\nreal\nlasso\nridge\nlm\n\n\n\n\nX1\n0.018\n0.000\n-0.072\n0.009\n\n\nX2\n0.105\n0.062\n0.107\n0.168\n\n\nX3\n0.368\n0.307\n0.344\n0.241\n\n\nX4\n0.779\n1.022\n0.936\n1.213\n\n\nX5\n1.000\n1.070\n0.966\n1.299\n\n\nX6\n0.779\n0.536\n0.515\n0.561\n\n\nX7\n0.368\n0.270\n0.277\n0.390\n\n\nX8\n0.105\n0.001\n0.024\n0.285\n\n\nX9\n0.018\n0.000\n-0.056\n-0.027\n\n\n\n\n\n\n\n\n\n\nCódigo\nset.seed(12345)\nn = 20\nbeta = c( rep(1,3), rep(0,5) )\nX = rnorm(n*length(beta)) |&gt; matrix(nrow = n, ncol = length(beta))\nY = X %*% beta + rnorm(n,0,1)\n\nmse_ridge = mse_lasso = numeric()\n\nfor (lambda in 0:20) {\n  beta_ridge =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_ridge,\n      X      = X,\n      Y      = Y,\n      lambda = lambda,\n      method = \"BFGS\"\n    )$par\n\n    mse_ridge[lambda + 1] = mean((beta_ridge - beta)^2)\n\n  beta_lasso =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_lasso,\n      X      = X,\n      Y      = Y,\n      lambda = lambda,\n      method = \"BFGS\"\n    )$par\n\n    mse_lasso[lambda + 1] = mean((beta_lasso - beta)^2)\n}\n\nplot(0:(length(mse_ridge)-1), mse_ridge,  type=\"l\", lwd = 2, col = 1,\n     main = \"esparsidade nos parâmetros reais\",\n     ylim = c(0, 0.1),\n     ylab = \"MSE\", xlab = \"lambda\")\nlines(0:(length(mse_lasso)-1), mse_lasso, type=\"l\", lwd = 2, col = 2, lty = \"aa\")\nlegend(\"topright\",\n       legend = c(\"Ridge\", \"Lasso\"),\n       col = c(1, 2),\n       lty = c(1, 2),\n       lwd = 2,\n       bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDicaComparar estimativas\n\n\n\n\n\nDados gerados a partir de coeficientes esparsos.\n\nbeta_lasso =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_lasso,\n      X      = X,\n      Y      = Y,\n      lambda = which.min(mse_lasso)-1,\n      method = \"BFGS\"\n    )$par\nbeta_ridge =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_ridge,\n      X      = X,\n      Y      = Y,\n      lambda = which.min(mse_ridge)-1,\n      method = \"BFGS\"\n    )$par\nbeta_lm = lm(Y~0+X)$coef\n\ncbind(beta, beta_lasso, beta_ridge, beta_lm) |&gt;\n  `colnames&lt;-`(c(\"real\", \"lasso\", \"ridge\", \"lm\")) |&gt;\n  knitr::kable(digits = 3, caption = \"Estimativas para betas (caso betas esparsos)\")\n\n\nEstimativas para betas (caso betas esparsos)\n\n\n\nreal\nlasso\nridge\nlm\n\n\n\n\nX1\n1\n0.832\n0.821\n1.228\n\n\nX2\n1\n1.046\n0.997\n1.144\n\n\nX3\n1\n1.104\n1.027\n1.200\n\n\nX4\n0\n0.000\n-0.005\n0.057\n\n\nX5\n0\n0.000\n-0.030\n0.033\n\n\nX6\n0\n-0.001\n-0.174\n-0.379\n\n\nX7\n0\n-0.232\n-0.292\n-0.380\n\n\nX8\n0\n0.000\n0.123\n0.215\n\n\n\n\n\n\n\n\nEsse exemplo simples mostra como nem a regressão lasso nem a regressão ridge vão ser uniformemente superiores uma a outra em todos os cenários. Isso evidencia a necessidade do uso de validação cruzada, tanto para sabermos qual técnica usar quanto para acharmos o \\(\\lambda\\) ótimo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regularização</span>"
    ]
  },
  {
    "objectID": "qmd/regularização.html#elastic-net",
    "href": "qmd/regularização.html#elastic-net",
    "title": "1  Regularização",
    "section": "1.5 Elastic net",
    "text": "1.5 Elastic net\n\n“Can you?”\n\nO artigo Zou & Hastie (2005) introduz o elastic net, que nada mais é que uma mistura das técnicas ridge e lasso. A ideia inicial considera o seguinte estimador:\n\\[\n\\begin{aligned}\n\\hat\\beta_{en}^{naïve}\n&= \\arg\\min_{\\beta} \\left\\{ SQRes + \\lambda_{ridge}\\sum_{j=1}^p \\beta_j^2 + \\lambda_{lasso}\\sum_{j=1}^p |\\beta_j| \\right\\}\\\\\n&= \\arg\\min_{\\beta} \\left\\{ SQRes + \\lambda_{en} \\left( \\alpha \\sum_{j=1}^p \\beta_j^2 + (1 - \\alpha)\\sum_{j=1}^p |\\beta_j| \\right) \\right\\},\n\\end{aligned}\n\\]\nem que:\n\n\\(\\lambda_{ridge}\\) e \\(\\lambda_{lasso}\\) são os coeficientes associados às penalidades \\(\\ell_2\\) e \\(\\ell_1\\), respectivamente;\n\\(\\alpha \\in [0,1]\\) é o peso da penalidade \\(\\ell_2\\) e \\(\\lambda_en\\) é uma média das penalidades somadas.\n\nA regressão elastic net goza das seguintes propriedades:\n\nÉ possível que todas as covariáveis sejam selecionadas, como a regressão ridge;\nAssim como a regressão lasso, alguns coeficientes podem ser estimados como zero (seleção de variáveis);\nEm cenários com preditores muito parecidos, a elastic net pode agir como a regressão ridge (grouping effect).\n\n\n\n\n\nFriedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1–22. Obtido de https://www.jstatsoft.org/index.php/jss/article/view/v033i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., et al. (2025). glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. Stanford University. Obtido de https://glmnet.stanford.edu/\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55–67. Obtido de https://www.jstor.org/stable/1267351\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. Springer Texts em Statistics. Springer. Obtido de https://link.springer.com/book/10.1007/978-1-0716-1418-1\n\n\nKuhn, M., & Silge, J. (2022). Tidy Modeling with R. O’Reilly Media. Obtido de https://www.tmwr.org/\n\n\nR Core Team. (2024). R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. Obtido de https://www.r-project.org/\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic Net Regularization Paths for All Generalized Linear Models. Journal of Statistical Software, 106(1), 1–31. Obtido de https://www.jstatsoft.org/article/view/v106i01\n\n\nTibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Obtido de https://www.jstor.org/stable/2346178\n\n\nZou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2), 301–320. Obtido de https://www.jstor.org/stable/3647580",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regularização</span>"
    ]
  },
  {
    "objectID": "qmd/estimação.html",
    "href": "qmd/estimação.html",
    "title": "2  Estimação",
    "section": "",
    "text": "2.1 Regularização como um limiar suave\nAté agora vimos exemplos de como a regularização pode ser vista como uma função de perda na estimação de mínimos quadrados. Essa ideia pode ser estendida para a estimação via máxima verossimilhança (como veremos adiante). Antes disso, mostraremos um exemplo (retirado de James, Witten, Hastie, & Tibshirani (2021)) de como a regressão ridge e lasso diminuem os coeficiente na prática.\nConsidere o caso em que \\(n = p\\), \\(X = I_n\\), e não temos intercepto. Assim \\(\\hat\\beta_{OLS} = Y\\). Nesse caso, é possível mostrar que\n\\[\n\\hat\\beta_j^{ridge} = \\frac{Y_j}{1 + \\lambda}\n\\] e\n\\[\n\\hat\\beta_j^{lasso} =\n\\begin{cases}\nY_j - \\lambda/2, & \\text{se } Y_j &gt; \\lambda/2;\\\\\nY_j + \\lambda/2, & \\text{se } Y_j &lt; - \\lambda/2;\\\\\n0, & \\text{se } -\\lambda/2 \\leq Y_j \\leq \\lambda/2.\n\\end{cases}\n\\]\nCom esse exemplo simples, podemos entender porque a regressão ridge nunca iguala os coeficientes a zero, coisa que a lasso pode fazer. Essa característica de zerar coeficientes a depender do valor que os dados assumem é conhecido como soft-tresholding (limiar suave). De acordo com Zou & Hastie (2005), a elastic net mistura tanto a shinkage da regressão ridge quanto o soft tresholding da lasso.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "qmd/estimação.html#regularização-como-um-limiar-suave",
    "href": "qmd/estimação.html#regularização-como-um-limiar-suave",
    "title": "2  Estimação",
    "section": "",
    "text": "Gráfico das condições acima.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "qmd/estimação.html#regularização-como-uma-restrição-do-espaço-paramétrico",
    "href": "qmd/estimação.html#regularização-como-uma-restrição-do-espaço-paramétrico",
    "title": "2  Estimação",
    "section": "2.2 Regularização como uma restrição do espaço paramétrico",
    "text": "2.2 Regularização como uma restrição do espaço paramétrico\nÉ possível entender cada um dos processos de regularização descritos anteriormente como uma restrição do espaço paramétrico dos coeficientes de regressão. Se não fazer seleção é considerer que \\(\\beta \\in \\mathbb R^d\\), é possível mostrar que - escolhidos os parâmetros de shrinkage - então minimar a soma de quadrados do resíduo penalizada é a mesma coisa que minimizar a soma de quadrado da regressão num espaço paramétrico menor (que depende dos parâmetros de shrinkage escolhidos).\nAssim, (na regressão linear normal) vale que:\n\\[\n\\hat\\beta_{ridge} =\n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n\\sum_{j=1}^p \\beta_j^2 \\le t_{ridge},\n\\]\n\\[\n\\hat\\beta_{lasso} =\n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n\\sum_{j=1}^p |\\beta_j| \\le t_{lasso},\n\\]\n\\[\n\\hat\\beta_{elastic\\ net} =\n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n(1-\\alpha)\\sum_{j=1}^p |\\beta_j| + \\alpha \\sum_{j=1}^p \\beta_j^2 \\le t_{elastic\\ net}.\n\\]\n\n\nCódigo\ndesenhar_espaço_paramétrico = function(alpha, t, título = NULL) {\n  stopifnot(\n    \"alpha deve estar entre 0 e 1\" = all(alpha &gt;= 0, alpha &lt;= 1),\n    \"t deve ser positivo\"          = t &gt; 0\n  )\n  \n  F = function(x, y) alpha*(x^2 + y^2 - t) + (1-alpha)*(abs(x) + abs(y) - t)\n  \n  x = seq(-2, 2, length = 400)\n  y = seq(-2, 2, length = 400)\n  g = outer(x, y, F)\n  \n  contour(x, y, g,\n          levels = 0,\n          drawlabels = FALSE,\n          lwd = 2, asp = 1,\n          main = título,\n          cex.main = 2\n          )\n}\n\npar(mfrow = c(1,3))\ndesenhar_espaço_paramétrico(1,   1, \"ridge\")\ndesenhar_espaço_paramétrico(0,   1, \"lasso\")\ndesenhar_espaço_paramétrico(1/2, 1, \"elastic net\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "qmd/estimação.html#regularização-nos-mlgs",
    "href": "qmd/estimação.html#regularização-nos-mlgs",
    "title": "2  Estimação",
    "section": "2.3 Regularização nos MLGs",
    "text": "2.3 Regularização nos MLGs\nO modelo linear normal pode ser especificado diretamente por meio de seus resíduos, mas - em geral - isso não é possível em todos os MLGs. Aqui, apresentaremos a técnica descrita em Tay, Narasimhan, & Hastie (2023), que generaliza a regularização elastic net para os MLGs. Grosso modo, em vez de minimizar a SQRes penalizada (que nem sempre está bem definida), vamos minimizar o inverso aditivo da log-verossimilhança penalizada (que sempre está bem definida num MLG).\n\\[\n\\hat{\\beta}\n= \\arg\\min_{\\beta}\n\\left\\{\n  -\\frac{1}{n}\\sum_{i=1}^n \\text{loglik}\\left(y_i, X \\beta \\right) + P_{\\alpha, \\lambda}(\\beta)\n\\right\\},\n\\]\nonde \\(P_{\\alpha, \\lambda}(\\beta)\\) é a função de penalização elastic net, ou seja \\[\nP_{\\alpha, \\lambda}(\\beta) =\n\\lambda \\left( \\frac{1 - \\alpha}{2}  \\sum_{j=1}^p \\beta_j^2 + \\alpha\\sum_{j=1}^p |\\beta_j| \\right).\n\\]\nCom isso, temos um algoritmo de mínimos quadrados ponderados (IRLS) dado a seguir.\n\n\n\n\n\n\nDicaIRLS elastic net (simplificado)\n\n\n\nSelecione um valor de \\(\\alpha \\in [0,1]\\) e valor de \\(\\lambda \\in \\mathbb R\\).\nInicialize o algritmo de maneira razoável - com \\(\\hat\\beta^{(0)} = 0\\), ou \\(\\eta^{(0)} = Y\\), ou qualquer coisa que faça sentido. Doravante para \\(t = 1, 2, \\dots\\) (até que se atinja convergência) faça:\n\nCompute \\(\\eta_i^{(t)} = X\\hat{\\beta}^{(t)}\\) e também \\(\\ \\mu_i^{(t)} = g^{-1}(\\eta_i^{(t)})\\), para \\(i \\in 1:n\\);\nCompute \\(z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)}) \\left( \\frac{d\\mu_i^{(t)}}{d\\eta_i^{(t)}} \\right)^{-1}\\) e também \\(w_i^{(t)} = \\frac{1}{V(\\mu_i^{(t)})} \\left( \\frac{d\\mu_i^{(t)}}{d\\eta_i^{(t)}} \\right)^2\\), para \\(i \\in 1:n\\);\nResolva\n\n\\[\n\\hat{\\beta}^{(t+1)}\n=\n\\arg\\min_{\\beta}\n\\left\\{\n\\frac{1}{2n}\n\\sum_{i=1}^n w_i^{(t)}\n\\bigl( z_i^{(t)} - X_i \\beta \\bigr)^2\n+ P_{\\alpha, \\lambda}(\\beta)\n\\right\\}.\n\\]\nO algoritmo de Tay et al. (2023) (que aqui simplificamos) faz uma validação cruzada para estimar o melhor \\(\\lambda\\). Isso é feito considerando um “caminho de lambdas” (regularization path) e otimizando o algoritmo acima com warm-starts, o que será tratado na próxima seção.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "qmd/estimação.html#estudo-com-dados-simulados",
    "href": "qmd/estimação.html#estudo-com-dados-simulados",
    "title": "2  Estimação",
    "section": "2.4 Estudo com dados simulados",
    "text": "2.4 Estudo com dados simulados\n\nset.seed(12345)\n\nn = 200\nbloco = 10\n\nX1 = matrix(rnorm(n*bloco*2), ncol = bloco*2)\nbeta1 = c( runif(bloco,-0.7,0.7), rep(0,bloco) )\n\nsigma =\n  matrix(.9, nrow = bloco, ncol = bloco) |&gt;\n  `diag&lt;-`(1)\nX2 = mvtnorm::rmvnorm(n, sigma = sigma)  # covariáveis com alta dependência, mas só a primeira é significativa\nbeta2 = c(0.6, rep(0,bloco-1))\n\nX = cbind(1,X1,X2)\nbeta = c(0.1,beta1,beta2)\neta = exp(X %*% beta)\n\nindex_outliers = sample(1:n, 5)\neta[index_outliers] = eta[index_outliers] * 3\n\nY = rpois(n, eta)\n\ndf =\n  cbind(Y, X[,-1]) |&gt;\n  tibble::as_tibble() |&gt;\n  `colnames&lt;-`(c(\"Y\", paste0(\"X\", 1:(3*bloco))))\n\n\n\n\n\n\n\nNotaResultado do stats::glm\n\n\n\n\n\n\nmod = glm(Y~., data = df, family = poisson())\nsummary(mod)\n\n\nCall:\nglm(formula = Y ~ ., family = poisson(), data = df)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.147716   0.078848   1.873  0.06101 .  \nX1          -0.439625   0.048811  -9.007  &lt; 2e-16 ***\nX2           0.256128   0.051740   4.950 7.41e-07 ***\nX3          -0.009407   0.058204  -0.162  0.87160    \nX4          -0.090574   0.059411  -1.525  0.12738    \nX5          -0.034130   0.055911  -0.610  0.54157    \nX6          -0.579245   0.049715 -11.651  &lt; 2e-16 ***\nX7          -0.368905   0.056426  -6.538 6.24e-11 ***\nX8           0.703791   0.053220  13.224  &lt; 2e-16 ***\nX9          -0.445391   0.053580  -8.313  &lt; 2e-16 ***\nX10          0.473092   0.048391   9.776  &lt; 2e-16 ***\nX11         -0.095314   0.045754  -2.083  0.03724 *  \nX12          0.107682   0.059583   1.807  0.07072 .  \nX13         -0.068017   0.059022  -1.152  0.24916    \nX14          0.003372   0.068797   0.049  0.96091    \nX15         -0.040952   0.059557  -0.688  0.49169    \nX16          0.149384   0.058604   2.549  0.01080 *  \nX17          0.073612   0.058512   1.258  0.20837    \nX18          0.038654   0.051843   0.746  0.45590    \nX19          0.067018   0.049151   1.364  0.17272    \nX20         -0.155441   0.057215  -2.717  0.00659 ** \nX21          0.467298   0.161213   2.899  0.00375 ** \nX22          0.174466   0.146951   1.187  0.23513    \nX23         -0.147347   0.130179  -1.132  0.25769    \nX24          0.072172   0.150662   0.479  0.63192    \nX25          0.094166   0.153625   0.613  0.53990    \nX26         -0.075417   0.137824  -0.547  0.58425    \nX27          0.258111   0.143846   1.794  0.07276 .  \nX28         -0.357209   0.168157  -2.124  0.03365 *  \nX29          0.006988   0.181175   0.039  0.96923    \nX30          0.082610   0.147340   0.561  0.57502    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2661.99  on 199  degrees of freedom\nResidual deviance:  209.12  on 169  degrees of freedom\nAIC: 660.56\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\n\n\n\n\n\n\nDica\n\n\n\nEm modelos com função de ligação canônica, o algoritmo acima simplica. O segundo passo da iteração fica: Compute \\(z_i^{(t)} = \\eta_i^{(t)} + \\frac{(y_i - \\mu_i^{(t)})}{V(\\mu_i^{})}\\) e também \\(w_i^{(t)} = V(\\mu_i^{(t)})\\), para \\(i \\in 1:n\\).\n\n\nSabemos que no caso da Poisson, a função de ligação canônica é o log e a função de variância é \\(\\mu_i\\). Assim, nosso algoritmo IRLS-EN fica da seguinte forma.\n\n\n\n\n\n\nAvisoUma pequena trapaça\n\n\n\n\n\nComo dito acima, a estimação elastic net depende de uma boa escolha dos parâmetros de regularização, especialmente \\(\\lambda\\). Como isso é feita via validação cruzada - assunto da próxima seção - demos uma trapaceada didática ao escolher o melhor \\(\\lambda\\) usando o glmnet, pacote que falaremos mais à frente.\n\ncv = glmnet::cv.glmnet(\n  X[,-1], Y,\n  family = \"poisson\",\n  intercept = TRUE,\n  alpha = 0.9,\n  nfolds = 10\n)\n\ncv$lambda.1se\n\n[1] 0.2828693\n\n\n\n\n\n\nsqres_en = function(beta, w, z) {\n  sum(  w * (z - X %*% beta)^2  ) / (2 * nrow(X)) +\n    lambda * (  alpha * sum(abs(beta[-1])) + ((1 - alpha)/2) * crossprod(beta[-1])  )\n}\n\niter = 20\nalpha = 0.9; lambda = cv$lambda.1se\nbeta_hat = matrix(1, nrow = iter, ncol = 3*bloco+1)\n\nfor (t in 2:iter) {\n  eta = X %*% beta_hat[t-1,]\n  mu = exp(eta)\n  z = eta + (Y - mu)/mu\n  beta_hat[t,] = \n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_en,\n      w      = mu,\n      z      = z,\n      method = \"BFGS\"\n    )$par\n}\n\n\n\nCódigo\nestimativas = \n  cbind(  0:(3*bloco),  beta_hat[iter,],  beta,  unname(mod$coefficients)  ) |&gt;\n  tibble::as_tibble() |&gt;\n  `colnames&lt;-`(c(\"index\", \"estimativa en\", \"real\", \"estimativa glm\"))\n\nestimativas[0:10+1,] |&gt;\n  knitr::kable(digits = 3,\n    caption = \"Estimativas vs. beta real\\n(originalmente significativos)\")\n\n\n\nEstimativas vs. beta real (originalmente significativos)\n\n\nindex\nestimativa en\nreal\nestimativa glm\n\n\n\n\n0\n0.134\n0.100\n0.148\n\n\n1\n-0.098\n-0.492\n-0.440\n\n\n2\n0.617\n0.272\n0.256\n\n\n3\n0.172\n-0.102\n-0.009\n\n\n4\n0.305\n-0.088\n-0.091\n\n\n5\n0.038\n0.100\n-0.034\n\n\n6\n-0.556\n-0.587\n-0.579\n\n\n7\n-0.379\n-0.347\n-0.369\n\n\n8\n0.566\n0.584\n0.704\n\n\n9\n-0.160\n-0.479\n-0.445\n\n\n10\n0.548\n0.457\n0.473\n\n\n\n\n\n\n\nCódigo\nestimativas[11:20+1,] |&gt;\n  knitr::kable(digits = 3,\n    caption = \"Estimativas vs. real\\n(originalmente NÃO significativos)\")\n\n\n\nEstimativas vs. real (originalmente NÃO significativos)\n\n\nindex\nestimativa en\nreal\nestimativa glm\n\n\n\n\n11\n0.013\n0\n-0.095\n\n\n12\n0.102\n0\n0.108\n\n\n13\n0.001\n0\n-0.068\n\n\n14\n0.002\n0\n0.003\n\n\n15\n0.590\n0\n-0.041\n\n\n16\n-0.007\n0\n0.149\n\n\n17\n0.001\n0\n0.074\n\n\n18\n0.097\n0\n0.039\n\n\n19\n-0.664\n0\n0.067\n\n\n20\n-0.314\n0\n-0.155\n\n\n\n\n\n\n\nCódigo\nestimativas[21:30+1,] |&gt;\n  knitr::kable(digits = 3,\n    caption = \"Estimativas vs. real\\n(originalmente só o 1º significativo e com covariáveis correlacionadas)\")\n\n\n\nEstimativas vs. real (originalmente só o 1º significativo e com covariáveis correlacionadas)\n\n\nindex\nestimativa en\nreal\nestimativa glm\n\n\n\n\n21\n0.332\n0.6\n0.467\n\n\n22\n0.963\n0.0\n0.174\n\n\n23\n0.001\n0.0\n-0.147\n\n\n24\n0.001\n0.0\n0.072\n\n\n25\n0.000\n0.0\n0.094\n\n\n26\n0.000\n0.0\n-0.075\n\n\n27\n0.400\n0.0\n0.258\n\n\n28\n-0.001\n0.0\n-0.357\n\n\n29\n0.000\n0.0\n0.007\n\n\n30\n0.001\n0.0\n0.083\n\n\n\n\n\n\n\n\n\n\n\nImportanteLidando com outliers\n\n\n\n\n\n\n\nCódigo\nmod_sem_outliers = glm(Y~., data = df[-index_outliers,], family = poisson())\n# summary(mod_sem_outliers)\ncol = 11\n\nx_seq = seq(min(X[,col+1]), max(X[,col+1]), length.out = 200)\ny_hat_sem_outlier = exp(summary(mod_sem_outliers)$coef[1] + summary(mod_sem_outliers)$coef[col+1] * x_seq)\ny_hat_com_outlier = exp(summary(mod)$coef[1] + summary(mod)$coef[col+1] * x_seq)\n\npar(mfrow = c(1,1))\nplot(Y~X[,col+1],\n     #ylim = c(0,10),\n     main = \"predição com outliers vs. sem outliers\",\n     xlab = paste(\"X\", col + 1),\n     ylab = \"Y\"\n     )\npoints(\n  X[index_outliers,col+1],\n  Y[index_outliers],\n  col = \"red\",\n  pch = 4,\n  cex = 1.5,\n  lwd = 2\n)\nlines(x_seq, y_hat_sem_outlier, col = \"blue\", lwd = 2)\nlines(x_seq, y_hat_com_outlier, col = \"red\",  lwd = 2)\n\n\n\n\n\n\n\n\n\nPra ver depois.\n\nset.seed(12345)\nx = rnorm(20)\ne = rt(20, 1)\ny = 2*x + e\nplot(y~x)\n\n\n\n\n\n\n\n#lm(y~0+x) |&gt; summary()\n\n\n\n\nComentários !!!\n\n2.4.1 Bootstrap\nWhy !!!\n\nset.seed(12345)\n\niter = 400\nn = 200\nbloco = 10\n\nbeta1 = c( runif(bloco,-0.7,0.7), rep(0,bloco) )\nbeta2 = c(0.6, rep(0,bloco-1))\nbeta = c(0.1,beta1,beta2)\n\nestimativas = array(dim = c(2,length(beta),iter), dimnames = list(c(\"glm\", \"en\")))\n\nsigma =\n  matrix(.9, nrow = bloco, ncol = bloco) |&gt;\n  `diag&lt;-`(1)\nX1 = matrix(rnorm(n*bloco*2), ncol = bloco*2)\nX2 = mvtnorm::rmvnorm(n, sigma = sigma)\nX = cbind(1,X1,X2)\neta = exp(X %*% beta)\n\nfor (i in 1:iter) {\n  # X1 = matrix(rnorm(n*bloco*2), ncol = bloco*2)\n  # X2 = mvtnorm::rmvnorm(n, sigma = sigma)\n  # X = cbind(1,X1,X2)\n  # eta = exp(X %*% beta)\n  Y = rpois(n, eta)\n  \n  df =\n    cbind(Y, X[,-1]) |&gt;\n    tibble::as_tibble() |&gt;\n    `colnames&lt;-`(c(\"Y\", paste0(\"X\", 1:(3*bloco))))\n  \n  coef_en =\n    glmnet::cv.glmnet(\n      X[,-1], Y,\n      family = \"poisson\",\n      intercept = TRUE,\n      alpha = 0.9,\n      nfolds = 10\n    ) |&gt;\n    coef(s = \"lambda.1se\") |&gt;\n    as.matrix() |&gt;\n    `dimnames&lt;-`(NULL) |&gt;\n    c()\n  \n  coef_glm = \n    glm(Y~., data = df, family = poisson())$coef |&gt;\n    unname()\n  \n  estimativas[\"en\",,i]  = coef_en\n  estimativas[\"glm\",,i] = coef_glm\n}\n\n\n\nCódigo\nplot_hist_block = function(idxs, xlims) {\n  par(mfrow = c(2, 3))\n  for (metodo in c(\"glm\", \"en\")) {\n    for (i in seq_along(idxs)) {\n      beta_idx = idxs[i]\n      beta_number = beta_idx - 1   # como pedido\n      \n      hist(\n        estimativas[metodo, beta_idx, ],\n        main = paste0(metodo, \" - β\", beta_number),\n        xlab = \"\", ylab = \"\",\n        xlim = xlims[[i]]\n      )\n    }\n    \n  }\n}\n\nplot_hist_block(\n  idxs = c(1, 4, 11),\n  xlims = list(c(-0.2, 0.5), c(0.1, 0.5), c(0.5, 0.9))\n)\n\n\n\n\n\nVariâncias dos betas do grupo 1 (significativos)\n\n\n\n\n\n\nCódigo\nplot_hist_block(\n  idxs = c(12, 14, 19),\n  xlims = list(c(-0.2, 0.2), c(-0.2, 0.2), c(-0.2, 0.2))\n)\n\n\n\n\n\nVariâncias dos betas do grupo 2 (não significativos)\n\n\n\n\n\n\nCódigo\nplot_hist_block(\n  idxs = c(22, 23, 27),\n  xlims = list(c(0, 1.2), c(-0.5, 0.5), c(-0.5, 0.5))\n)\n\n\n\n\n\nVariâncias dos betas do grupo 3 (covariáveis autocorrelacionadas)\n\n\n\n\n\n\nCódigo\nvariancias =\n  apply(estimativas, c(1,2), var) |&gt;\n  t() |&gt;\n  tibble::as_tibble() |&gt;\n  cbind(c(\"intercepto\", rep(c(1,2,3), each=10))) |&gt;\n  tibble::as_tibble() |&gt;\n  `colnames&lt;-`(c(\"glm\", \"en\", \"grupo\"))\n\nlibrary(ggplot2)\n\nvariancias_long =\n  variancias[-1,] |&gt;\n  tidyr::pivot_longer(\n    cols = c(\"glm\", \"en\"),\n    names_to = \"metodo\",\n    values_to = \"variancia\")\n\nggplot(variancias_long,\n       aes(x = grupo, y = variancia, fill = metodo)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(x = \"Grupo\", y = \"Variância\",\n       title = \"Variância dos Betas por Grupo e Método\") +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nComentários !!!\nViés? !!!\n\n\nCódigo\nvies2 =\n  sweep(estimativas, 2, beta, FUN = \"-\")^2 |&gt;\n  apply(c(1,2), mean) |&gt;\n  t() |&gt;\n  tibble::as_tibble() |&gt;\n  cbind(c(\"intercepto\", rep(c(1,2,3), each=10))) |&gt;\n  tibble::as_tibble() |&gt;\n  `colnames&lt;-`(c(\"glm\", \"en\", \"grupo\"))\n\nvies2_long =\n  vies2[-1,] |&gt;\n  tidyr::pivot_longer(\n    cols = c(\"glm\", \"en\"),\n    names_to = \"metodo\",\n    values_to = \"vies2\")\n\nggplot(vies2_long,\n       aes(x = grupo, y = vies2, fill = metodo)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(x = \"Grupo\", y = \"Viés²\",\n       title = \"Viés² dos Betas por Grupo e Método\") +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFriedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1–22. Obtido de https://www.jstatsoft.org/index.php/jss/article/view/v033i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., et al. (2025). glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. Stanford University. Obtido de https://glmnet.stanford.edu/\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55–67. Obtido de https://www.jstor.org/stable/1267351\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. Springer Texts em Statistics. Springer. Obtido de https://link.springer.com/book/10.1007/978-1-0716-1418-1\n\n\nKuhn, M., & Silge, J. (2022). Tidy Modeling with R. O’Reilly Media. Obtido de https://www.tmwr.org/\n\n\nR Core Team. (2024). R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. Obtido de https://www.r-project.org/\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic Net Regularization Paths for All Generalized Linear Models. Journal of Statistical Software, 106(1), 1–31. Obtido de https://www.jstatsoft.org/article/view/v106i01\n\n\nTibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Obtido de https://www.jstor.org/stable/2346178\n\n\nZou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2), 301–320. Obtido de https://www.jstor.org/stable/3647580",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "qmd/tunagem.html",
    "href": "qmd/tunagem.html",
    "title": "3  Tunagem",
    "section": "",
    "text": "3.1 Validação cruzada\n1\n\n[1] 1",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tunagem</span>"
    ]
  },
  {
    "objectID": "qmd/tunagem.html#warm-start",
    "href": "qmd/tunagem.html#warm-start",
    "title": "3  Tunagem",
    "section": "3.2 Warm start",
    "text": "3.2 Warm start\nTay, Narasimhan, & Hastie (2023)\n\n\n\n\nFriedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1–22. Obtido de https://www.jstatsoft.org/index.php/jss/article/view/v033i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., et al. (2025). glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. Stanford University. Obtido de https://glmnet.stanford.edu/\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55–67. Obtido de https://www.jstor.org/stable/1267351\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. Springer Texts em Statistics. Springer. Obtido de https://link.springer.com/book/10.1007/978-1-0716-1418-1\n\n\nKuhn, M., & Silge, J. (2022). Tidy Modeling with R. O’Reilly Media. Obtido de https://www.tmwr.org/\n\n\nR Core Team. (2024). R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. Obtido de https://www.r-project.org/\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic Net Regularization Paths for All Generalized Linear Models. Journal of Statistical Software, 106(1), 1–31. Obtido de https://www.jstatsoft.org/article/view/v106i01\n\n\nTibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Obtido de https://www.jstor.org/stable/2346178\n\n\nZou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2), 301–320. Obtido de https://www.jstor.org/stable/3647580",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tunagem</span>"
    ]
  },
  {
    "objectID": "qmd/exemplo.html",
    "href": "qmd/exemplo.html",
    "title": "4  Exemplo prático",
    "section": "",
    "text": "“Minha jangada vai sair por mar”\n\nEsparsidade, multicolinearidade e outliers\n\n1\n\n[1] 1\n\n\n\n\n\n\nFriedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1–22. Obtido de https://www.jstatsoft.org/index.php/jss/article/view/v033i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., et al. (2025). glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. Stanford University. Obtido de https://glmnet.stanford.edu/\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55–67. Obtido de https://www.jstor.org/stable/1267351\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. Springer Texts em Statistics. Springer. Obtido de https://link.springer.com/book/10.1007/978-1-0716-1418-1\n\n\nKuhn, M., & Silge, J. (2022). Tidy Modeling with R. O’Reilly Media. Obtido de https://www.tmwr.org/\n\n\nR Core Team. (2024). R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. Obtido de https://www.r-project.org/\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic Net Regularization Paths for All Generalized Linear Models. Journal of Statistical Software, 106(1), 1–31. Obtido de https://www.jstatsoft.org/article/view/v106i01\n\n\nTibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Obtido de https://www.jstor.org/stable/2346178\n\n\nZou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2), 301–320. Obtido de https://www.jstor.org/stable/3647580",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exemplo prático</span>"
    ]
  },
  {
    "objectID": "qmd/referências.html",
    "href": "qmd/referências.html",
    "title": "Referências",
    "section": "",
    "text": "Friedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization\npaths for generalized linear models via coordinate descent. Journal\nof Statistical Software, 33(1), 1–22. Retrieved from https://www.jstatsoft.org/index.php/jss/article/view/v033i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., et al. (2025). Glmnet:\nLasso and elastic-net regularized generalized linear models.\nStanford University. Retrieved from https://glmnet.stanford.edu/\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge regression: Biased\nestimation for nonorthogonal problems. Technometrics,\n12(1), 55–67. Retrieved from https://www.jstor.org/stable/1267351\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An\nintroduction to statistical learning: With applications in r.\nSpringer texts in statistics. Springer. Retrieved from https://link.springer.com/book/10.1007/978-1-0716-1418-1\n\n\nKuhn, M., & Silge, J. (2022). Tidy modeling with r.\nO’Reilly Media. Retrieved from https://www.tmwr.org/\n\n\nR Core Team. (2024). R: A language and environment for statistical\ncomputing. Vienna, Austria: R Foundation for Statistical Computing.\nRetrieved from https://www.r-project.org/\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic net\nregularization paths for all generalized linear models. Journal of\nStatistical Software, 106(1), 1–31. Retrieved from https://www.jstatsoft.org/article/view/v106i01\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso.\nJournal of the Royal Statistical Society. Series B\n(Methodological), 58(1), 267–288. Retrieved from https://www.jstor.org/stable/2346178\n\n\nZou, H., & Hastie, T. (2005). Regularization and variable selection\nvia the elastic net. Journal of the Royal Statistical Society.\nSeries B (Statistical Methodology), 67(2), 301–320.\nRetrieved from https://www.jstor.org/stable/3647580",
    "crumbs": [
      "Referências"
    ]
  }
]