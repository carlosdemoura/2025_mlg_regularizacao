[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regularização em MLGs",
    "section": "",
    "text": "Sobre\n\n“Eu tô te explicando Pra te confundir Eu tô te confundindo Pra te esclarecer” Tom Zé\n\nEste é o material auxiliar da apresentação do trabalho final do curso de Modelos Lineares Generalizados (MLGs) (DEST-UFMG, 2025/2). O tema é regularização em MLGs, em específico os métodos de regularização ridge, lasso e elastic net e este quarto book está dividido da seguinte forma:\n\nDefinição de regularização e apresentação dos métodos de shrinkage, com exemplos de regressão normal;\nEstimação dos parâmetros em MLG com penalização;\nTunagem dos parâmetros de regularização;\nExemplo prático com dados reais no R.\n\nReferências bibliográficas importantes que foram usadas para a feitura desse trabalho são citadas ao final do documento.\nAlguns pacotes que usaremos estão abaixo listados.\n\nif (!{\"pak\" %in% rownames(installed.packages())}) install.packages(\"pak\")\n\npak::pak(c(\"matrixcalc\", \"glmnet\"))\n\n\n\nMotivação\nPorquê fazer seleção de modelos? Porquê regularizar coeficientes é uma boa ideia?\n\n\n\n\nFriedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1–22. Obtido de https://www.jstatsoft.org/index.php/jss/article/view/v033i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., et al. (2025). glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. Stanford University. Obtido de https://glmnet.stanford.edu/\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55–67. Obtido de https://www.jstor.org/stable/1267351\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. Springer Texts em Statistics. Springer. Obtido de https://link.springer.com/book/10.1007/978-1-0716-1418-1\n\n\nKuhn, M., & Silge, J. (2022). Tidy Modeling with R. O’Reilly Media. Obtido de https://www.tmwr.org/\n\n\nR Core Team. (2024). R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. Obtido de https://www.r-project.org/\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic Net Regularization Paths for All Generalized Linear Models. Journal of Statistical Software, 106(1), 1–31. Obtido de https://www.jstatsoft.org/article/view/v106i01\n\n\nTibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Obtido de https://www.jstor.org/stable/2346178\n\n\nZou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2), 301–320. Obtido de https://www.jstor.org/stable/3647580",
    "crumbs": [
      "Sobre"
    ]
  },
  {
    "objectID": "qmd/regularização.html",
    "href": "qmd/regularização.html",
    "title": "1  Regularização",
    "section": "",
    "text": "1.1 Seleção de variáveis naïve\nStepwise\nCódigo\nf = function(n) {\n  s = 0\n  for (i in 1:n) {\n    s = s + choose(n, i)\n  }\n  s\n}\n\nseq(1, 20, 1) |&gt;\n  lapply(f) |&gt;\n  unlist() |&gt;\n  plot(type = 'l',\n       main = \"\",\n       xlab = \"número de covariáveis numéricas\",\n       ylab = \"número de possíveis modelos\"\n       )",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regularização</span>"
    ]
  },
  {
    "objectID": "qmd/regularização.html#ridge",
    "href": "qmd/regularização.html#ridge",
    "title": "1  Regularização",
    "section": "1.2 Ridge",
    "text": "1.2 Ridge\nA técnica Ridge foi a primeira das três técnicas a surgir, no trabalho de Hoerl & Kennard (1970). Originalmente, os autores buscavam entender como superar problemas em que a matriz de covariáveis \\(X\\) estava mal-especificada. Relembrando que na regressão linear normal, temos que\n\\[\nY = X \\beta + \\epsilon, \\hspace{1cm} \\epsilon \\sim N_n\\bigr( 0, \\sigma^2 I_n \\bigr)\n\\] O estimador de máxima verossimilhança (EMV) de \\(\\beta\\) será o mesmo estimador de mínimos quadrados (EMQ)\n\\[\n\\hat\\beta = (X^\\top X)^{-1} X^\\top Y,\n\\]\nse:\n\n\\(X^\\top X\\) for inversível;\nA matriz de covariáveis é ortogonalizável, i.e., os dados foram coletados de maneira independente;\nhá menos betas que observações, isto é ncol(X) &lt;&lt; nrow(X).\n\nAlém disso, \\(\\hat\\beta\\) é não viciado, consistente e tem matriz de covariâncias dada por\n\\[\ncov\\bigr(\\hat\\beta\\bigr) = \\sigma^2 (X^\\top X)^{-1}.\n\\]\nUma vez que temos a distribuição do estimador, fica fácil fazer inferência via intervalos de confiança, por exemplo.\n\\[\n\\hat\\beta \\sim N_p\\left(\\beta,\\; \\sigma^2 (X^\\top X)^{-1}\\right).\n\\]\nNesse sentido, se temos uma matriz de dados problemática - no sentido em que \\((X^\\top X)^{-1}\\) não está bem definida, teremos problema de estimação via EMQ.\nVeja o exemplo numérico abaixo.\n\nset.seed(12345)\n\nn = 10\nbeta = c(1, 0)\nX = cbind(1:n, 2*(1:n))\nY = X %*% beta + rnorm(n)\n\n\n\nVer matriz X\n\n\n\nhead(X)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    4\n[3,]    3    6\n[4,]    4    8\n[5,]    5   10\n[6,]    6   12\n\nmatrixcalc::is.singular.matrix(t(X)%*%X)\n\n[1] TRUE\n\n\n\n\n\n\nCódigo\npar(mfrow=c(1,2))\nplot(Y~X[,1])\nplot(Y~X[,2])\n\n\n\n\n\n\n\n\n\n\nlm(Y~0+X)\n\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n    X1      X2  \n0.9544      NA  \n\n\n\n\nE se a regressão estivesse na outra covariável?\n\n\n\nset.seed(12345)\n\nbeta = c(0, 1)\nX = cbind(1:n, 2*(1:n))\nY = X %*% beta + rnorm(n)\n\nlm(Y~0+X)\n\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n   X1     X2  \n1.954     NA  \n\n\nDetalhes\n\n\nComentários sobre o modelo não saber selecionar variáveis.\n\nset.seed(12345)\n\nbeta = c(1, 0)\nX = cbind(1:n, 2*(1:n))\nruido = cbind(rep(0,n), rnorm(n,0,.1))\nX = X + ruido\nY = X %*% beta + rnorm(n)\n\n\n\nVer matriz X\n\n\n\nhead(X)\n\n     [,1]      [,2]\n[1,]    1  2.058553\n[2,]    2  4.070947\n[3,]    3  5.989070\n[4,]    4  7.954650\n[5,]    5 10.060589\n[6,]    6 11.818204\n\nmatrixcalc::is.singular.matrix(t(X)%*%X)\n\n[1] FALSE\n\neigen(t(X)%*%X)$values\n\n[1] 1.918025e+03 1.070613e-02\n\n\n\n\n\nlm(Y~0+X)\n\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n    X1      X2  \n 6.658  -2.820  \n\n\nComentários sobre combinações lineares, parcimônia e inflação da variância na inferência.\nA proposta de Hoerl & Kennard (1970) é adicionar um certo múltiplo da matriz identidade à \\(X^\\top X\\), de modo que seja possível superar os problemas de uma matriz \\(X\\) mal especificada. James, Witten, Hastie, & Tibshirani (2021) mostra que isso é equivalente a adicionar uma punição no processo de estimação por mínimos quadrados. Seja \\(SQRes = (Y - X\\beta)^\\top(Y - X\\beta)= \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2\\), assim:\n\\[\n\\hat\\beta_{ridge} =\n\\arg\\min_{\\beta} \\left\\{ SQRes + \\lambda_{ridge}\\sum_{j=1}^p \\beta_j^2 \\right\\},\n\\]\nem que \\(\\lambda_{ridge}\\) é uma constante positiva (escolhida pelo pesquisador) que dá o peso dessa punição.\nNote que:\n\nÀ medida que \\(\\lambda\\) cresce menores ficam as estimativas dos betas, \\(\\hat\\beta_{ridge} \\rightarrow 0,\\ \\lambda_{ridge} \\rightarrow \\infty\\).\nNão faz sentido incluir o intercepto no processo de shrinkage, pois \\(g(\\beta_0)\\) é o valor esperado da variável resposta dado que as demais covariáveis são zero, em que \\(g\\) é uma função de ligação.\nFica claro que a depender do valor de \\(\\lambda\\) que escolhemos, estamos incluindo algum viés no modelo.\nA estimação ridge diminui as estimativas dos betas na mesma proporção (se comparada com o MMQ tradicional).\nEm diminuindo a estimativa dos betas, diminui-se também a variância envolvida nas estimativas dos parâmetros.\n\n\n\n\n\n\n\nAviso\n\n\n\nAo usar métodos de regularização, estamos fazendo um trade-off entre variância e viés. Grosso modo, estamos inserido algum viés em nossas estimativas, a fim de diminuir a variância na estimação dos parâmetros e assim fazer inferências mais precisas.\nUm bom método de escolha do parâmetro de shrinkage é crucial para o bom funcionamento da estimação ridge. Falaremos disso nas próximas seções. Spoiler: faremos validação cruzada.\n\n\n\n\n\nAnalogia dos dardos para pensar variância e viés.\n\n\nO exemplo abaixo está no livro James et al. (2021), Credit é um conjunto de dados incluído no pacote ISLR, contendo informações sobre 4000 clientes de um banco, com o objetivo de modelar e entender fatores associados ao limite de crédito de cada pessoa.\n\n\n\nExemplo de regressão ridge.\n\n\n\n\n\n\n\n\nDicaPadronização das covariáveis\n\n\n\nA técnica ridge pode ser muito sensível à escala das covariáveis. Nesse sentido, vale a pena padronizar as covariáveis pelo desvio-padrão para evitar a influência desse efeito.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regularização</span>"
    ]
  },
  {
    "objectID": "qmd/regularização.html#lasso",
    "href": "qmd/regularização.html#lasso",
    "title": "1  Regularização",
    "section": "1.3 Lasso",
    "text": "1.3 Lasso\nA regressão lasso surgiu com o artigo de Tibshirani (1996). No caso da regressão linear normal, a diferença entre lasso e ridge pode ser vista na função de perda da estimação mínimos quadrados. Em vez de usar a norma \\(\\ell_2\\) (como faz a regressão ridge), a regressão lasso usa a norma \\(\\ell_1\\) de beta.\n\\[\n\\hat\\beta_{lasso} =\n\\arg\\min_{\\beta} \\left\\{ SQRes + \\lambda_{lasso}\\sum_{j=1}^p |\\beta_j| \\right\\},\n\\]\nMuitas das observações que fizemos para a estimação ridge se aplicam tambpem à regressão lasso. A principal diferênça entre essas técnicas está no fato que a regressão lasso faz, de fato, uma seleção de variáveis. Isso se dá pois a regressão lasso pode zerar os coeficientes de algumas covariáveis.\n\n\n\n\n\n\nNota\n\n\n\n\n\nO fato de a regressão lasso poder zerar os coeficientes das covariáveis está associado com a geometria imposta no espaço paramétrico. \n\n\n\nNo mesmo banco de dados Credit, James et al. (2021) aplicou a regressão lasso:\n\n\n\nExemplo de regressão lasso.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regularização</span>"
    ]
  },
  {
    "objectID": "qmd/regularização.html#comparação-das-técnicas",
    "href": "qmd/regularização.html#comparação-das-técnicas",
    "title": "1  Regularização",
    "section": "1.4 Comparação das técnicas",
    "text": "1.4 Comparação das técnicas\n\n“You Can’t Always Get What You Want”\n\nA\n\n1.4.1 Estudo sobre MSE com dados sintéticos\n\nsqres = function(beta, X, Y) {\n  res = Y - X %*% beta\n  drop(crossprod(res))\n}\n\nsqres_ridge = function(beta, X, Y, lambda) {\n  res = Y - X %*% beta\n  drop(crossprod(res) + lambda * crossprod(beta))\n}\n\nsqres_lasso = function(beta, X, Y, lambda) {\n  res = Y - X %*% beta\n  drop(crossprod(res) + lambda * sum(abs(beta)))\n}\n\nA\n\n\n\n\n\n\nNota\n\n\n\n\n\nNote que minimizar sqres deve dar o mesmo resultado que lm.\n\nfit = optim(\n  par    = rep(0,ncol(X)),\n  fn     = sqres,\n  X      = X,\n  Y      = Y,\n  method = \"BFGS\"\n)\n\nround(fit$par, 5) |&gt; `names&lt;-`(paste0(\"X\", 1:ncol(X)))\n\n      X1       X2 \n 6.65825 -2.81988 \n\nlm(Y~0+X)\n\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n    X1      X2  \n 6.658  -2.820  \n\n\n\n\n\n\n\nCódigo\nset.seed(12345)\nn = 20\nbeta = exp(-seq(-2,2,.5)^2)\nX = rnorm(n*length(beta)) |&gt; matrix(nrow = n, ncol = length(beta))\nY = X %*% beta + rnorm(n,0,1)\n\nmse_ridge = mse_lasso = numeric()\n\nfor (lambda in 0:20) {\n  beta_ridge =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_ridge,\n      X      = X,\n      Y      = Y,\n      lambda = lambda,\n      method = \"BFGS\"\n    )$par\n\n    mse_ridge[lambda + 1] = mean((beta_ridge - beta)^2)\n\n  beta_lasso =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_lasso,\n      X      = X,\n      Y      = Y,\n      lambda = lambda,\n      method = \"BFGS\"\n    )$par\n\n    mse_lasso[lambda + 1] = mean((beta_lasso - beta)^2)\n}\n\nplot(mse_ridge,  type=\"l\", lwd = 2, col = 1,\n     ylim = c(0, 0.1),\n     ylab = \"MSE\", xlab = \"lambda\")\nlines(mse_lasso, type=\"l\", lwd = 2, col = 2, lty = \"aa\")\n\nmse_lm = mean((lm(Y~0+X)$coeff - beta)^2)\nabline(h = mse_lm, lwd = 2, col = 3, lty = \"44\")\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nset.seed(12345)\nn = 20\nbeta = c( rep(1,3), rep(0,10) )\nX = rnorm(n*length(beta)) |&gt; matrix(nrow = n, ncol = length(beta))\nY = X %*% beta + rnorm(n,0,1)\n\nmse_ridge = mse_lasso = numeric()\n\nfor (lambda in 0:20) {\n  beta_ridge =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_ridge,\n      X      = X,\n      Y      = Y,\n      lambda = lambda,\n      method = \"BFGS\"\n    )$par\n\n    mse_ridge[lambda + 1] = mean((beta_ridge - beta)^2)\n\n  beta_lasso =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_lasso,\n      X      = X,\n      Y      = Y,\n      lambda = lambda,\n      method = \"BFGS\"\n    )$par\n\n    mse_lasso[lambda + 1] = mean((beta_lasso - beta)^2)\n}\n\nplot(mse_ridge,  type=\"l\", lwd = 2, col = 1,\n     ylim = c(0, 0.1),\n     ylab = \"MSE\", xlab = \"lambda\")\nlines(mse_lasso, type=\"l\", lwd = 2, col = 2, lty = \"aa\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regularização</span>"
    ]
  },
  {
    "objectID": "qmd/regularização.html#elastic-net",
    "href": "qmd/regularização.html#elastic-net",
    "title": "1  Regularização",
    "section": "1.5 Elastic net",
    "text": "1.5 Elastic net\n\n“Can you?”\n\nO artigo Zou & Hastie (2005) introduz o elastic net, que nada mais é que uma mistura das técnicas ridge e lasso. A ideia inicial considera o seguinte estimador:\n\\[\n\\begin{aligned}\n\\hat\\beta_{en}^{naïve}\n&= \\arg\\min_{\\beta} \\left\\{ SQRes + \\lambda_{ridge}\\sum_{j=1}^p \\beta_j^2 + \\lambda_{lasso}\\sum_{j=1}^p |\\beta_j| \\right\\}\\\\\n&= \\arg\\min_{\\beta} \\left\\{ SQRes + \\lambda_{en} \\left( \\alpha \\sum_{j=1}^p \\beta_j^2 + (1 - \\alpha)\\sum_{j=1}^p |\\beta_j| \\right) \\right\\},\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regularização</span>"
    ]
  },
  {
    "objectID": "qmd/regularização.html#lidando-com-outliers",
    "href": "qmd/regularização.html#lidando-com-outliers",
    "title": "1  Regularização",
    "section": "1.6 Lidando com outliers",
    "text": "1.6 Lidando com outliers\n\nset.seed(12345); x = rnorm(20); e = rt(20, 1); y = 2*x + e; plot(y~x); lm(y~0+x)\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = y ~ 0 + x)\n\nCoefficients:\n    x  \n6.136  \n\n\n\n\n\n\nFriedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1–22. Obtido de https://www.jstatsoft.org/index.php/jss/article/view/v033i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., et al. (2025). glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. Stanford University. Obtido de https://glmnet.stanford.edu/\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55–67. Obtido de https://www.jstor.org/stable/1267351\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. Springer Texts em Statistics. Springer. Obtido de https://link.springer.com/book/10.1007/978-1-0716-1418-1\n\n\nKuhn, M., & Silge, J. (2022). Tidy Modeling with R. O’Reilly Media. Obtido de https://www.tmwr.org/\n\n\nR Core Team. (2024). R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. Obtido de https://www.r-project.org/\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic Net Regularization Paths for All Generalized Linear Models. Journal of Statistical Software, 106(1), 1–31. Obtido de https://www.jstatsoft.org/article/view/v106i01\n\n\nTibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Obtido de https://www.jstor.org/stable/2346178\n\n\nZou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2), 301–320. Obtido de https://www.jstor.org/stable/3647580",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regularização</span>"
    ]
  },
  {
    "objectID": "qmd/estimação.html",
    "href": "qmd/estimação.html",
    "title": "2  Estimação",
    "section": "",
    "text": "2.1 Regularização como uma função de perda",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "qmd/estimação.html#regularização-como-uma-restrição-do-espaço-paramétrico",
    "href": "qmd/estimação.html#regularização-como-uma-restrição-do-espaço-paramétrico",
    "title": "2  Estimação",
    "section": "2.2 Regularização como uma restrição do espaço paramétrico",
    "text": "2.2 Regularização como uma restrição do espaço paramétrico\nÉ possível entender cada um dos processos de regularização descritos anteriormente como uma restrição do espaço paramétrico dos coeficientes de regressão. Se não fazer seleção é considerer que \\(\\beta \\in \\mathbb R^d\\), é possível mostrar que - escolhidos os parâmetros de shrinkage - então minimar a soma de quadrados do resíduo penalizada é a mesma coisa que minimizar a soma de quadrado da regressão num espaço paramétrico menor (que depende dos parâmetros de shrinkage escolhidos).\nAssim, (na regressão linear normal) vale que:\n\\[\n\\hat\\beta_{ridge} =\n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n\\sum_{j=1}^p \\beta_j^2 \\le t_{ridge},\n\\]\n\\[\n\\hat\\beta_{lasso} =\n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n\\sum_{j=1}^p |\\beta_j| \\le t_{lasso},\n\\]\n\\[\n\\hat\\beta_{elastic\\ net} =\n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n(1-\\alpha)\\sum_{j=1}^p |\\beta_j| + \\alpha \\sum_{j=1}^p \\beta_j^2 \\le t_{elastic\\ net}.\n\\]\n\n\nCódigo\ndesenhar_espaço_paramétrico = function(alpha, t, título = NULL) {\n  stopifnot(\n    \"alpha deve estar entre 0 e 1\" = all(alpha &gt;= 0, alpha &lt;= 1),\n    \"t deve ser positivo\"          = t &gt; 0\n  )\n  \n  F = function(x, y) alpha*(x^2 + y^2 - t) + (1-alpha)*(abs(x) + abs(y) - t)\n  \n  x = seq(-2, 2, length = 400)\n  y = seq(-2, 2, length = 400)\n  g = outer(x, y, F)\n  \n  contour(x, y, g,\n          levels = 0,\n          drawlabels = FALSE,\n          lwd = 2, asp = 1,\n          main = título,\n          cex.main = 2\n          )\n}\n\n\npar(mfrow = c(1,3))\ndesenhar_espaço_paramétrico(1,   1, \"ridge\")\ndesenhar_espaço_paramétrico(0,   1, \"lasso\")\ndesenhar_espaço_paramétrico(1/2, 1, \"elastic net\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "qmd/estimação.html#regularização-nos-mlgs",
    "href": "qmd/estimação.html#regularização-nos-mlgs",
    "title": "2  Estimação",
    "section": "2.3 Regularização nos MLGs",
    "text": "2.3 Regularização nos MLGs\nFS vs shrinkage\n\n1+ 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "qmd/estimação.html#estudo-bootstrap",
    "href": "qmd/estimação.html#estudo-bootstrap",
    "title": "2  Estimação",
    "section": "2.4 Estudo Bootstrap",
    "text": "2.4 Estudo Bootstrap\nA\n\n\n\n\nFriedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1–22. Obtido de https://www.jstatsoft.org/index.php/jss/article/view/v033i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., et al. (2025). glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. Stanford University. Obtido de https://glmnet.stanford.edu/\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55–67. Obtido de https://www.jstor.org/stable/1267351\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. Springer Texts em Statistics. Springer. Obtido de https://link.springer.com/book/10.1007/978-1-0716-1418-1\n\n\nKuhn, M., & Silge, J. (2022). Tidy Modeling with R. O’Reilly Media. Obtido de https://www.tmwr.org/\n\n\nR Core Team. (2024). R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. Obtido de https://www.r-project.org/\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic Net Regularization Paths for All Generalized Linear Models. Journal of Statistical Software, 106(1), 1–31. Obtido de https://www.jstatsoft.org/article/view/v106i01\n\n\nTibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Obtido de https://www.jstor.org/stable/2346178\n\n\nZou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2), 301–320. Obtido de https://www.jstor.org/stable/3647580",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "qmd/tunagem.html",
    "href": "qmd/tunagem.html",
    "title": "3  Tunagem",
    "section": "",
    "text": "Validação cruzada\n\n1\n\n[1] 1\n\n\n\n\n\n\nFriedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1–22. Obtido de https://www.jstatsoft.org/index.php/jss/article/view/v033i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., et al. (2025). glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. Stanford University. Obtido de https://glmnet.stanford.edu/\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55–67. Obtido de https://www.jstor.org/stable/1267351\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. Springer Texts em Statistics. Springer. Obtido de https://link.springer.com/book/10.1007/978-1-0716-1418-1\n\n\nKuhn, M., & Silge, J. (2022). Tidy Modeling with R. O’Reilly Media. Obtido de https://www.tmwr.org/\n\n\nR Core Team. (2024). R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. Obtido de https://www.r-project.org/\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic Net Regularization Paths for All Generalized Linear Models. Journal of Statistical Software, 106(1), 1–31. Obtido de https://www.jstatsoft.org/article/view/v106i01\n\n\nTibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Obtido de https://www.jstor.org/stable/2346178\n\n\nZou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2), 301–320. Obtido de https://www.jstor.org/stable/3647580",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tunagem</span>"
    ]
  },
  {
    "objectID": "qmd/exemplo.html",
    "href": "qmd/exemplo.html",
    "title": "4  Exemplo prático",
    "section": "",
    "text": "“Minha jangada vai sair por mar”\n\nEsparsidade, multicolinearidade e outliers\n\n1\n\n[1] 1\n\n\n\n\n\n\nFriedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1–22. Obtido de https://www.jstatsoft.org/index.php/jss/article/view/v033i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., et al. (2025). glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. Stanford University. Obtido de https://glmnet.stanford.edu/\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55–67. Obtido de https://www.jstor.org/stable/1267351\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. Springer Texts em Statistics. Springer. Obtido de https://link.springer.com/book/10.1007/978-1-0716-1418-1\n\n\nKuhn, M., & Silge, J. (2022). Tidy Modeling with R. O’Reilly Media. Obtido de https://www.tmwr.org/\n\n\nR Core Team. (2024). R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. Obtido de https://www.r-project.org/\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic Net Regularization Paths for All Generalized Linear Models. Journal of Statistical Software, 106(1), 1–31. Obtido de https://www.jstatsoft.org/article/view/v106i01\n\n\nTibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Obtido de https://www.jstor.org/stable/2346178\n\n\nZou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2), 301–320. Obtido de https://www.jstor.org/stable/3647580",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exemplo prático</span>"
    ]
  },
  {
    "objectID": "qmd/referências.html",
    "href": "qmd/referências.html",
    "title": "Referências",
    "section": "",
    "text": "Friedman, J. H., Hastie, T., & Tibshirani, R. (2010). Regularization\npaths for generalized linear models via coordinate descent. Journal\nof Statistical Software, 33(1), 1–22. Retrieved from https://www.jstatsoft.org/index.php/jss/article/view/v033i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., et al. (2025). Glmnet:\nLasso and elastic-net regularized generalized linear models.\nStanford University. Retrieved from https://glmnet.stanford.edu/\n\n\nHoerl, A. E., & Kennard, R. W. (1970). Ridge regression: Biased\nestimation for nonorthogonal problems. Technometrics,\n12(1), 55–67. Retrieved from https://www.jstor.org/stable/1267351\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An\nintroduction to statistical learning: With applications in r.\nSpringer texts in statistics. Springer. Retrieved from https://link.springer.com/book/10.1007/978-1-0716-1418-1\n\n\nKuhn, M., & Silge, J. (2022). Tidy modeling with r.\nO’Reilly Media. Retrieved from https://www.tmwr.org/\n\n\nR Core Team. (2024). R: A language and environment for statistical\ncomputing. Vienna, Austria: R Foundation for Statistical Computing.\nRetrieved from https://www.r-project.org/\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic net\nregularization paths for all generalized linear models. Journal of\nStatistical Software, 106(1), 1–31. Retrieved from https://www.jstatsoft.org/article/view/v106i01\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso.\nJournal of the Royal Statistical Society. Series B\n(Methodological), 58(1), 267–288. Retrieved from https://www.jstor.org/stable/2346178\n\n\nZou, H., & Hastie, T. (2005). Regularization and variable selection\nvia the elastic net. Journal of the Royal Statistical Society.\nSeries B (Statistical Methodology), 67(2), 301–320.\nRetrieved from https://www.jstor.org/stable/3647580",
    "crumbs": [
      "Referências"
    ]
  }
]