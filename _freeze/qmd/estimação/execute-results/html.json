{
  "hash": "826b74360995f4c45c60be8a34b5570b",
  "result": {
    "engine": "knitr",
    "markdown": "# Estimação\n\n## Regularização como um limiar suave\n\nAté agora vimos exemplos de como a regularização pode ser vista como uma função de perda na estimação de mínimos quadrados. Essa ideia pode ser estendida para a estimação via máxima verossimilhança (como veremos adiante).\nAntes disso, mostraremos um exemplo (retirado de @islr) de como a regressão ridge e lasso diminuem os coeficiente na prática.\n\nConsidere o caso em que $n = p$, $X = I_n$, e não temos intercepto. Assim $\\hat\\beta_{OLS} = Y$. Nesse caso, é possível mostrar que \n\n$$\n\\hat\\beta_j^{ridge} = \\frac{Y_j}{1 + \\lambda}\n$$\ne\n\n$$\n\\hat\\beta_j^{lasso} = \n\\begin{cases}\nY_j - \\lambda/2, & \\text{se } Y_j > \\lambda/2;\\\\\nY_j + \\lambda/2, & \\text{se } Y_j < - \\lambda/2;\\\\\n0, & \\text{se } -\\lambda/2 \\leq Y_j \\leq \\lambda/2.\n\\end{cases}\n$$\n\nCom esse exemplo simples, podemos entender porque a regressão ridge nunca iguala os coeficientes a zero, coisa que a lasso pode fazer. Essa característica de zerar coeficientes a depender do valor que os dados assumem é conhecido como _soft-tresholding_ (limiar suave). De acordo com @en, a elastic net mistura tanto a shinkage da regressão ridge quanto o soft tresholding da lasso.\n\n\n[![Gráfico das condições acima.](../img/islr_soft-tresholding.png){width=90%}](https://www.statlearning.com/)\n\n\n\n\n\n\n\n\n\n## Regularização como uma restrição do espaço paramétrico\n\nÉ possível entender cada um dos processos de regularização descritos anteriormente como uma restrição do espaço paramétrico dos coeficientes de regressão. Se não fazer seleção é considerer que $\\beta \\in \\mathbb R^d$, é possível mostrar que - escolhidos os parâmetros de shrinkage - então minimar a soma de quadrados do resíduo penalizada é a mesma coisa que minimizar a soma de quadrado da regressão num espaço paramétrico menor (que depende dos parâmetros de shrinkage escolhidos).\n\nAssim, (na regressão linear normal) vale que:\n\n\n$$\n\\hat\\beta_{ridge} = \n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n\\sum_{j=1}^p \\beta_j^2 \\le t_{ridge},\n$$\n\n$$\n\\hat\\beta_{lasso} = \n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n\\sum_{j=1}^p |\\beta_j| \\le t_{lasso},\n$$\n\n$$\n\\hat\\beta_{elastic\\ net} = \n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n(1-\\alpha)\\sum_{j=1}^p |\\beta_j| + \\alpha \\sum_{j=1}^p \\beta_j^2 \\le t_{elastic\\ net}.\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndesenhar_espaço_paramétrico = function(alpha, t, título = NULL) {\n  stopifnot(\n    \"alpha deve estar entre 0 e 1\" = all(alpha >= 0, alpha <= 1),\n    \"t deve ser positivo\"          = t > 0\n  )\n  \n  F = function(x, y) alpha*(x^2 + y^2 - t) + (1-alpha)*(abs(x) + abs(y) - t)\n  \n  x = seq(-2, 2, length = 400)\n  y = seq(-2, 2, length = 400)\n  g = outer(x, y, F)\n  \n  contour(x, y, g,\n          levels = 0,\n          drawlabels = FALSE,\n          lwd = 2, asp = 1,\n          main = título,\n          cex.main = 2\n          )\n}\n\npar(mfrow = c(1,3))\ndesenhar_espaço_paramétrico(1,   1, \"ridge\")\ndesenhar_espaço_paramétrico(0,   1, \"lasso\")\ndesenhar_espaço_paramétrico(1/2, 1, \"elastic net\")\n```\n\n::: {.cell-output-display}\n![](estimação_files/figure-html/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n## Regularização nos MLGs\n\nO modelo linear normal pode ser especificado diretamente por meio de seus resíduos, mas - em geral - isso não é possível em todos os MLGs. Aqui, apresentaremos a técnica descrita em @en_glm, que generaliza a regularização elastic net para os MLGs.\nGrosso modo, em vez de minimizar a SQRes penalizada (que nem sempre está bem definida), vamos minimizar o inverso aditivo da log-verossimilhança penalizada (que sempre está bem definida num MLG).\n\n$$\n\\hat{\\beta}\n= \\arg\\min_{\\beta}\n\\left\\{\n  -\\frac{1}{n}\\sum_{i=1}^n \\text{loglik}\\left(y_i, X \\beta \\right) + P_{\\alpha, \\lambda}(\\beta)\n\\right\\},\n$$\n\nonde $P_{\\alpha, \\lambda}(\\beta)$ é a função de penalização elastic net, ou seja\n$$\nP_{\\alpha, \\lambda}(\\beta) = \n\\lambda \\left( \\frac{1 - \\alpha}{2}  \\sum_{j=1}^p \\beta_j^2 + \\alpha\\sum_{j=1}^p |\\beta_j| \\right).\n$$\n\nCom isso, temos um algoritmo de mínimos quadrados ponderados (IRLS) dado a seguir.\n\n::: {.callout-tip}\n## IRLS elastic net (simplificado)\n\nSelecione um valor de $\\alpha \\in [0,1]$ e valor de $\\lambda \\in \\mathbb R$.\n\nInicialize o algritmo de maneira razoável - com $\\hat\\beta^{(0)} = 0$, ou $\\eta^{(0)} = Y$, ou qualquer coisa que faça sentido. Doravante para $t = 1, 2, \\dots$ (até que se atinja convergência) faça:\n\n1. Compute $\\eta_i^{(t)} = X\\hat{\\beta}^{(t)}$ e também $\\ \\mu_i^{(t)} = g^{-1}(\\eta_i^{(t)})$, para $i \\in 1:n$;\n\n2. Compute $z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)}) \\left( \\frac{d\\mu_i^{(t)}}{d\\eta_i^{(t)}} \\right)^{-1}$\ne também\n$w_i^{(t)} = \\frac{1}{V(\\mu_i^{(t)})} \\left( \\frac{d\\mu_i^{(t)}}{d\\eta_i^{(t)}} \\right)^2$,\npara $i \\in 1:n$;\n\n3. Resolva\n\n$$\n\\hat{\\beta}^{(t+1)}\n=\n\\arg\\min_{\\beta}\n\\left\\{\n\\frac{1}{2n}\n\\sum_{i=1}^n w_i^{(t)}\n\\bigl( z_i^{(t)} - X_i \\beta \\bigr)^2\n+ P_{\\alpha, \\lambda}(\\beta)\n\\right\\}.\n$$\n\nO algoritmo de @en_glm (que aqui simplificamos) faz uma validação cruzada para estimar o melhor $\\lambda$. Isso é feito considerando um \"caminho de lambdas\" (_regularization path_) e otimizando o algoritmo acima com warm-starts, o que será tratado na próxima seção.\n:::\n\n\n\n\n\n\n\n\n\n## Estudo com dados simulados\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\nn = 200\nbloco = 10\n\nX1 = matrix(rnorm(n*bloco*2), ncol = bloco*2)\nbeta1 = c( runif(bloco,-0.7,0.7), rep(0,bloco) )\n\nsigma =\n  matrix(.9, nrow = bloco, ncol = bloco) |>\n  `diag<-`(1)\nX2 = mvtnorm::rmvnorm(n, sigma = sigma)  # covariáveis com alta dependência, mas só a primeira é significativa\nbeta2 = c(0.6, rep(0,bloco-1))\n\nX = cbind(1,X1,X2)\nbeta = c(0.1,beta1,beta2)\neta = exp(X %*% beta)\n\nindex_outliers = sample(1:n, 5)\neta[index_outliers] = eta[index_outliers] * 3\n\nY = rpois(n, eta)\n\ndf =\n  cbind(Y, X[,-1]) |>\n  tibble::as_tibble() |>\n  `colnames<-`(c(\"Y\", paste0(\"X\", 1:(3*bloco))))\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Resultado do `stats::glm`\n\n::: {.cell}\n\n```{.r .cell-code}\nmod = glm(Y~., data = df, family = poisson())\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Y ~ ., family = poisson(), data = df)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.147716   0.078848   1.873  0.06101 .  \nX1          -0.439625   0.048811  -9.007  < 2e-16 ***\nX2           0.256128   0.051740   4.950 7.41e-07 ***\nX3          -0.009407   0.058204  -0.162  0.87160    \nX4          -0.090574   0.059411  -1.525  0.12738    \nX5          -0.034130   0.055911  -0.610  0.54157    \nX6          -0.579245   0.049715 -11.651  < 2e-16 ***\nX7          -0.368905   0.056426  -6.538 6.24e-11 ***\nX8           0.703791   0.053220  13.224  < 2e-16 ***\nX9          -0.445391   0.053580  -8.313  < 2e-16 ***\nX10          0.473092   0.048391   9.776  < 2e-16 ***\nX11         -0.095314   0.045754  -2.083  0.03724 *  \nX12          0.107682   0.059583   1.807  0.07072 .  \nX13         -0.068017   0.059022  -1.152  0.24916    \nX14          0.003372   0.068797   0.049  0.96091    \nX15         -0.040952   0.059557  -0.688  0.49169    \nX16          0.149384   0.058604   2.549  0.01080 *  \nX17          0.073612   0.058512   1.258  0.20837    \nX18          0.038654   0.051843   0.746  0.45590    \nX19          0.067018   0.049151   1.364  0.17272    \nX20         -0.155441   0.057215  -2.717  0.00659 ** \nX21          0.467298   0.161213   2.899  0.00375 ** \nX22          0.174466   0.146951   1.187  0.23513    \nX23         -0.147347   0.130179  -1.132  0.25769    \nX24          0.072172   0.150662   0.479  0.63192    \nX25          0.094166   0.153625   0.613  0.53990    \nX26         -0.075417   0.137824  -0.547  0.58425    \nX27          0.258111   0.143846   1.794  0.07276 .  \nX28         -0.357209   0.168157  -2.124  0.03365 *  \nX29          0.006988   0.181175   0.039  0.96923    \nX30          0.082610   0.147340   0.561  0.57502    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2661.99  on 199  degrees of freedom\nResidual deviance:  209.12  on 169  degrees of freedom\nAIC: 660.56\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n:::\n\n::: {.callout-tip}\nEm modelos com função de ligação canônica, o algoritmo acima simplica. O segundo passo da iteração fica:\nCompute $z_i^{(t)} = \\eta_i^{(t)} + \\frac{(y_i - \\mu_i^{(t)})}{V(\\mu_i^{})}$\ne também\n$w_i^{(t)} = V(\\mu_i^{(t)})$,\npara $i \\in 1:n$.\n:::\n\nSabemos que no caso da Poisson, a função de ligação canônica é o log e a função de variância é $\\mu_i$. Assim, nosso algoritmo IRLS-EN fica da seguinte forma.\n\n::: {.callout-warning collapse=\"true\"}\n\n## Uma pequena trapaça\n\nComo dito acima, a estimação elastic net depende de uma boa escolha dos parâmetros de regularização, especialmente $\\lambda$. Como isso é feita via validação cruzada - assunto da próxima seção - demos uma trapaceada didática ao escolher o melhor $\\lambda$ usando o `glmnet`, pacote que falaremos mais à frente.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv = glmnet::cv.glmnet(\n  X[,-1], Y,\n  family = \"poisson\",\n  intercept = TRUE,\n  alpha = 0.9,\n  nfolds = 10\n)\n\ncv$lambda.1se\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2828693\n```\n\n\n:::\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimativa_en = function(Y, X, alpha, lambda, iter = 20, init = 1) {\n  sqres_en = function(beta, w, z) {\n    sum(  w * (z - X %*% beta)^2  ) / (2 * nrow(X)) +\n      lambda * (  alpha * sum(abs(beta[-1])) + ((1 - alpha)/2) * crossprod(beta[-1])  )\n  }\n  \n  beta_hat = matrix(nrow = iter, ncol = 3*bloco+1)\n  beta_hat[1,] = init\n  \n  for (t in 2:iter) {\n    eta = X %*% beta_hat[t-1,]\n    mu = exp(eta)\n    z = eta + (Y - mu)/mu\n    beta_hat[t,] = \n      optim(\n        par    = rep(0,ncol(X)),\n        fn     = sqres_en,\n        w      = mu,\n        z      = z,\n        method = \"BFGS\"\n      )$par\n  }\n  \n  beta_hat[t,]\n}\n\nbeta_en = estimativa_en(Y, X, alpha = 0.9, lambda = cv$lambda.1se)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nestimativas = \n  cbind(  0:(3*bloco),  beta_en,  beta,  unname(mod$coefficients)  ) |>\n  tibble::as_tibble() |>\n  `colnames<-`(c(\"index\", \"estimativa en\", \"real\", \"estimativa glm\"))\n\nestimativas[0:10+1,] |>\n  knitr::kable(digits = 3,\n    caption = \"Estimativas vs. beta real\\n(originalmente significativos)\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Estimativas vs. beta real\n(originalmente significativos)\n\n| index| estimativa en|   real| estimativa glm|\n|-----:|-------------:|------:|--------------:|\n|     0|         0.134|  0.100|          0.148|\n|     1|        -0.098| -0.492|         -0.440|\n|     2|         0.617|  0.272|          0.256|\n|     3|         0.172| -0.102|         -0.009|\n|     4|         0.305| -0.088|         -0.091|\n|     5|         0.038|  0.100|         -0.034|\n|     6|        -0.556| -0.587|         -0.579|\n|     7|        -0.379| -0.347|         -0.369|\n|     8|         0.566|  0.584|          0.704|\n|     9|        -0.160| -0.479|         -0.445|\n|    10|         0.548|  0.457|          0.473|\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nestimativas[11:20+1,] |>\n  knitr::kable(digits = 3,\n    caption = \"Estimativas vs. real\\n(originalmente NÃO significativos)\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Estimativas vs. real\n(originalmente NÃO significativos)\n\n| index| estimativa en| real| estimativa glm|\n|-----:|-------------:|----:|--------------:|\n|    11|         0.013|    0|         -0.095|\n|    12|         0.102|    0|          0.108|\n|    13|         0.001|    0|         -0.068|\n|    14|         0.002|    0|          0.003|\n|    15|         0.590|    0|         -0.041|\n|    16|        -0.007|    0|          0.149|\n|    17|         0.001|    0|          0.074|\n|    18|         0.097|    0|          0.039|\n|    19|        -0.664|    0|          0.067|\n|    20|        -0.314|    0|         -0.155|\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nestimativas[21:30+1,] |>\n  knitr::kable(digits = 3,\n    caption = \"Estimativas vs. real\\n(originalmente só o 1º significativo e com covariáveis correlacionadas)\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Estimativas vs. real\n(originalmente só o 1º significativo e com covariáveis correlacionadas)\n\n| index| estimativa en| real| estimativa glm|\n|-----:|-------------:|----:|--------------:|\n|    21|         0.332|  0.6|          0.467|\n|    22|         0.963|  0.0|          0.174|\n|    23|         0.001|  0.0|         -0.147|\n|    24|         0.001|  0.0|          0.072|\n|    25|         0.000|  0.0|          0.094|\n|    26|         0.000|  0.0|         -0.075|\n|    27|         0.400|  0.0|          0.258|\n|    28|        -0.001|  0.0|         -0.357|\n|    29|         0.000|  0.0|          0.007|\n|    30|         0.001|  0.0|          0.083|\n\n\n:::\n:::\n\n\nNos primeiro grupo (betas reais não nulos e covariáveis independentes), tanto o `glm` quanto o EN recuperam bem tanto os sinais quanto as magnitudes dos betas reais e podemos ver bem a ocorrência da _shrinkage_. Já no segundo bloco (betas reais nulos e covariáveis independentes), a EN se destaca ao manter as estimativas próximas de zero, enquanto o `glm` frequentemente gera pequenos falsos positivos. Finalmente, no terceiro bloco (betas reais não nulos e covariáveis fortemente dependentes), oEN novamente aplica _shrinkage_ forte nos coeficientes irrelevantes, enquanto o `glm` tem maior dificuldade em lidar com a colinearidade.\n\n\n\n\n### Lidando com outliers\n\nOs dados acima foram gerados com uma certa poluição de outliers, como podemos ver nos gráficos abaixo.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmod_sem_outliers = glm(Y~., data = df[-index_outliers,], family = poisson())\n# summary(mod_sem_outliers)\ncol = 11\n\nx_seq = seq(min(X[,col+1]), max(X[,col+1]), length.out = 200)\ny_hat_sem_outlier = exp(summary(mod_sem_outliers)$coef[1] + summary(mod_sem_outliers)$coef[col+1] * x_seq)\ny_hat_com_outlier = exp(summary(mod)$coef[1] + summary(mod)$coef[col+1] * x_seq)\n\nplot(Y~X[,col+1],\n     #ylim = c(0,10),\n     main = \"\",\n     xlab = paste(\"X\", col),\n     ylab = \"Y\"\n     )\npoints(\n  X[index_outliers,col+1],\n  Y[index_outliers],\n  col = \"red\",\n  pch = 4,\n  cex = 1.5,\n  lwd = 2\n)\nlines(x_seq, y_hat_sem_outlier, col = \"blue\", lwd = 2)\nlines(x_seq, y_hat_com_outlier, col = \"red\",  lwd = 2)\n```\n:::\n\n\n\n\n\n\n<img class=\"flip\"\n     src=\"../img/tmp/pois1.png\"\n     data-src1=\"../img/tmp/pois1.png\"\n     data-src2=\"../img/tmp/pois2.png\"\n     width=\"400\" height=\"500\"\n     onclick=\"trocaImagem(this)\">\n\n\n\nQuando estimamos usando a função `glm`, a estimativa do coeficiente da covariável 12 no modelo com os outliers é de -0.095, já no modelo sem outliers essa estimativa passa a ser de  -0.002. Além disso, esse coeficiente (originalmente nulo) tornou-se siginificativo com a inclusão dos outliers. Já na estimação EN, a estimativa pontual é 0.013, o que é mais razoável que a estimativa do `glm` com outliers (o único que temos acesso). Embora isso não indique se a variável é significativa (segundo a métrica EN), isso produz um modelo mais parcimonioso que o IRLS tradicional\n\nPodemos ver abaixo um exemplo mais comovente sobre o poder da regularização na contenção do efeito de outliers. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nx = cbind(1, rnorm(20))\ne = rt(20, 1)\nbeta = c(5, 2)\ny = x %*% beta + e\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(y~x[,-1])\n```\n\n::: {.cell-output-display}\n![](estimação_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y~x[,-1]) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x[, -1])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.624  -5.436  -3.624  -1.537  72.003 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    8.589      3.981   2.157   0.0448 *\nx[, -1]        5.724      4.877   1.174   0.2558  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.73 on 18 degrees of freedom\nMultiple R-squared:  0.07109,\tAdjusted R-squared:  0.01948 \nF-statistic: 1.378 on 1 and 18 DF,  p-value: 0.2558\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot::boot(\n  data = data.frame(y = y, x = x[,-1]),\n  statistic = {\\(data, indices) lm(y ~ x, data = data[indices, ])$coef},\n  R = 200\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot::boot(data = data.frame(y = y, x = x[, -1]), statistic = {\n    function(data, indices) lm(y ~ x, data = data[indices, ])$coef\n}, R = 200)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1* 8.588882 0.03144146    3.681207\nt2* 5.723713 0.25242782    3.843777\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Para entender melhor  o que está acontecendo no bootstrap\n\nset.seed(12345)\nB = 2e3\ncoef = matrix(nrow = B, ncol = 2)\ndata = data.frame(y = y, x = x[,-1])\nfun = {\\(indices) lm(y ~ x, data = data[indices, ])$coef}\n\nfor (b in 1:B) {\n  coef[b, ] = fun(  sample(1:length(y), length(y), replace = T)  )\n}\n\npar(mfrow = c(1,2))\nhist(coef[,1], main = \"estimativas bootstrap - β0\")\nabline(v = beta[1], lwd = 3, col = 2)\nhist(coef[,2], xlim = c(0,20), breaks = (-100):100,\n     main = \"estimativas bootstrap - β1\")\nabline(v = beta[2], lwd = 3, col = 2)\n```\n\n::: {.cell-output-display}\n![](estimação_files/figure-html/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqres_lasso = function(beta, X, Y, lambda) {\n  res = Y - X %*% beta\n  drop(crossprod(res) + lambda * sum(abs(beta)))\n}\n\noptim(\n  par    = rep(0,2),\n  fn     = sqres_lasso,\n  X      = x,\n  Y      = y,\n  lambda = 100,\n  method = \"BFGS\"\n)$par\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.356267 2.229249\n```\n\n\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nbeta_lasso = \n  optim(\n    par    = rep(0,2),\n    fn     = sqres_lasso,\n    X      = x,\n    Y      = y,\n    lambda = 100,\n    method = \"BFGS\"\n  )$par\nbeta_lm = lm(y~x[,-1])$coef |> unname()\n\nx_seq = seq(-2, 2, length.out = 100)\ny_hat_lm = beta_lm[1] + x_seq * beta_lm[2]\ny_hat_lasso = beta_lasso[1] + x_seq * beta_lasso[2]\n\nplot(y~x[,-1])\nlines(x_seq, y_hat_lasso, col = \"blue\", lwd = 2)\nlines(x_seq, y_hat_lm, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](estimação_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nÉ possível notar que o EMQ é fortemente impactado pela presença do outlier, que afeta não só a estimação pontual dos parâmetros como sua significância no modelo.\nCoisa parecida ocorre quando usamos o bootstrap para estimar os coeficientes de regressão.\nApesar disso, a regressão lasso produz estimativas razoáveis.\n\n\n\n\n\n\n\n\n### Análise de resíduos - parte 1\n\nÉ razoável que uma mudança na estimação dos coeficientes mude também a distribuição dos resíduos.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_en  = estimativa_en(Y, X, alpha = 0.9, lambda = cv$lambda.1se)\nbeta_glm = mod$coef |> unname()\n\nres_en  = ( Y - exp(X %*% beta_en) ) / sqrt(exp(X %*% beta_en))\nres_glm = ( Y - exp(X %*% beta_glm) ) / sqrt(exp(X %*% beta_glm))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow = c(1,2))\nplot(sort(res_glm))\nplot(sort(res_en))\n```\n:::\n\n\n\n\n<img class=\"flip\"\n     src=\"../img/tmp/env11.png\"\n     data-src1=\"../img/tmp/env11.png\"\n     data-src2=\"../img/tmp/env12.png\"\n     width=\"800\" height=\"400\"\n     onclick=\"trocaImagem(this)\">\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nB = 1000\n\nres_boot_glm = matrix(nrow = B, ncol = length(Y))\nfor (b in 1:B) {\n  Yb = rpois(length(Y), exp(X %*% beta_glm))\n  beta_b = glm(Yb ~ X[,-1], family = poisson())$coef |> unname()\n  res_boot_glm[b,] = ( Yb - exp(X %*% beta_b) ) / sqrt(exp(X %*% beta_b))\n}\nenvelope_glm =\n  res_boot_glm |>\n  apply(2,\n        \\(col) quantile(col, probs = c(0.05, 0.95))\n    ) |>\n  t()\n\n\nres_boot_en = matrix(nrow = B, ncol = length(Y))\nfor (b in 1:B) {\n  Yb = rpois(length(Y), exp(X %*% beta_en))\n  beta_b = estimativa_en(Yb, X, alpha = 0.9, lambda = cv$lambda.1se, iter = 10, init = beta_en)\n  #beta_b = estimativa_en(Yb, X, alpha = 0.9, lambda = cv$lambda.1se)\n  res_boot_en[b,] = ( Yb - exp(X %*% beta_b) ) / sqrt(exp(X %*% beta_b))\n}\nenvelope_en =\n  res_boot_en |>\n  apply(2,\n        \\(col) quantile(col, probs = c(0.05, 0.95), na.rm = T)\n    ) |>\n  t()\n\nsaveRDS(list(\"glm\" = envelope_glm, \"en\" = envelope_en), \"../data/envelopes.rds\")\n```\n:::\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow=c(1,2))\nplot(sort(res_glm), ylim = c(-5,5))\nlines(sort(envelope_glm[,1]))\nlines(sort(envelope_glm[,2]))\n\nplot(sort(res_en), ylim = c(-5,5))\nlines(sort(envelope_en[,1]))\nlines(sort(envelope_en[,2]))\n```\n:::\n\n\n\n\n<img class=\"flip\"\n     src=\"../img/tmp/env21.png\"\n     data-src1=\"../img/tmp/env21.png\"\n     data-src2=\"../img/tmp/env22.png\"\n     width=\"800\" height=\"400\"\n     onclick=\"trocaImagem(this)\">\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncat(\"porcentagem dentro do envelope\\n\",\n    \"EN\\t\",\n    sum(res_en > envelope_en[,1] & res_en < envelope_en[,2]) / length(res_en) * 100,\n    \"\\nMLE\\t\",\n    sum(res_glm > envelope_glm[,1] & res_glm < envelope_glm[,2]) / length(res_glm) * 100\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nporcentagem dentro do envelope\n EN\t 51.5 \nMLE\t 87.5\n```\n\n\n:::\n:::\n\n\nVoltando do exemplo de regressão linear com outlier (erros T-Student).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(12345)\nx = cbind(1, rnorm(20))\ne = rt(20, 1)\nbeta = c(5, 2)\ny = x %*% beta + e\n\nbeta_lasso = \n  optim(\n    par    = rep(0,2),\n    fn     = sqres_lasso,\n    X      = x,\n    Y      = y,\n    lambda = 100,\n    method = \"BFGS\"\n  )$par\nbeta_lm = lm(y~x[,-1])$coef |> unname()\n\nres_lasso = y - x %*% beta_lasso\nres_lm    = y - x %*% beta_lm\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow=c(1,2))\nqqnorm(res_lm, ylim = c(-10,10))\nqqnorm(res_lasso, ylim = c(-10,10))\n```\n:::\n\n\n\n\n<img class=\"flip\"\n     src=\"../img/tmp/env31.png\"\n     data-src1=\"../img/tmp/env31.png\"\n     data-src2=\"../img/tmp/env32.png\"\n     width=\"800\" height=\"400\"\n     onclick=\"trocaImagem(this)\">\n\n\n\n\n### Bootstrap\n\nComo um exemplo computaciona, optamos por fazer um bootstrap paramétrico a fim de entender o comportamento dos estimadores tradicional (IRLS via `glm`) e regularizado (IRLS-EN adaptado de @en_glm).\nOpatamos por manter a mesma configuração de blocos e parâmetros descrita anteriormente. Poderiamos ter considerando a aleatoriedade das covariáveis no processo (dentro das especificações anteriores) - o que pode ser feito simplesmente passando o sorteio dos xizes para dentro do loop -, mas vimos que isso produziu um resultado similar ao com $X$ fixado (só um pouco menos dramático).\n\nSeria interessante também fazer um estudo Monte Carlo, sorteando diferentes betas dentro da mesma estrutura e diferentes xizes para cada iteração Monte Carlo. Convidamos o leitor a fazer isso em seu tempo livre.\n\nPor fim, vale ressaltar que o bootstrap se faz necessário pois não sabemos qual é a distribuição analítica nem dos betas estimados via IRLS nem dos estimados via IRLS-EN.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\niter = 400\nn = 200\nbloco = 10\n\nbeta1 = c( runif(bloco,-0.7,0.7), rep(0,bloco) )\nbeta2 = c(0.6, rep(0,bloco-1))\nbeta = c(0.1,beta1,beta2)\n\nestimativas = array(dim = c(2,length(beta),iter), dimnames = list(c(\"glm\", \"en\")))\n\nsigma =\n  matrix(.9, nrow = bloco, ncol = bloco) |>\n  `diag<-`(1)\nX1 = matrix(rnorm(n*bloco*2), ncol = bloco*2)\nX2 = mvtnorm::rmvnorm(n, sigma = sigma)\nX = cbind(1,X1,X2)\neta = exp(X %*% beta)\n\nfor (i in 1:iter) {\n  Y = rpois(n, eta)\n  \n  coef_en =\n    glmnet::cv.glmnet(\n      X[,-1], Y,\n      family = \"poisson\",\n      intercept = TRUE,\n      alpha = 0.9,\n      nfolds = 10\n    ) |>\n    coef(s = \"lambda.1se\") |>\n    as.matrix() |>\n    `dimnames<-`(NULL) |>\n    c()\n  \n  coef_glm = \n    glm(Y~X[,-1], family = poisson())$coef |>\n    unname()\n  \n  estimativas[\"en\",,i]  = coef_en\n  estimativas[\"glm\",,i] = coef_glm\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_hist_block = function(idxs, xlims) {\n  par(mfrow = c(2, 3))\n  for (metodo in c(\"glm\", \"en\")) {\n    for (i in seq_along(idxs)) {\n      beta_idx = idxs[i]\n      beta_number = beta_idx - 1\n      \n      hist(\n        estimativas[metodo, beta_idx, ],\n        main = paste0(metodo, \" - β\", beta_number),\n        xlab = \"\", ylab = \"\",\n        xlim = xlims[[i]]\n      )\n      abline(v = beta[idxs[i]], col = 2, lwd = 2)\n    }\n    \n  }\n}\n\nplot_hist_block(\n  idxs = c(1, 4, 11),\n  xlims = list(c(-0.2, 0.5), c(0.1, 0.5), c(0.5, 0.9))\n)\n```\n\n::: {.cell-output-display}\n![Variâncias dos betas do grupo 1 (significativos), beta real em vermelho.](estimação_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_hist_block(\n  idxs = c(12, 14, 19),\n  xlims = list(c(-0.2, 0.2), c(-0.2, 0.2), c(-0.2, 0.2))\n)\n```\n\n::: {.cell-output-display}\n![Variâncias dos betas do grupo 2 (não significativos), beta real em vermelho.](estimação_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_hist_block(\n  idxs = c(22, 23, 27),\n  xlims = list(c(0, 1.2), c(-0.5, 0.5), c(-0.5, 0.5))\n)\n```\n\n::: {.cell-output-display}\n![Variâncias dos betas do grupo 3 (covariáveis autocorrelacionadas), beta real em vermelho.](estimação_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nOs histogramas acima vão ao encontro do que a teoria diz. No primeiro bloco de covariáveis (intercepto mais covariáveis de 1 a 10) os betas reais são significativos e ambos os modelos captam isso - com um grau similar de variabilidade.\n\nNo segundo bloco de covariáveis, ambos os modelos captam a insignificância real dos betas, mas as estimativas do elastic net têm bem menos variabilidade que aquelas do IRLS tradicional.\n\nNo terceiro bloco de covariáveis - alta correlação das covariáveis e apenas o $\\beta_{21}$ realmente diferente de zero -, ambos os modelos captam qual é a covariável significativa, mas - pelo menos nas variáveis não siginificativas - as estimativas do elastic net são menos dispersas que as do IRLS tradicional.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nvariancias =\n  apply(estimativas, c(1,2), var) |>\n  t() |>\n  tibble::as_tibble() |>\n  cbind(c(\"intercepto\", rep(c(1,2,3), each=10))) |>\n  tibble::as_tibble() |>\n  `colnames<-`(c(\"glm\", \"en\", \"grupo\"))\n\nlibrary(ggplot2)\n\nvariancias_long =\n  variancias[-1,] |>\n  tidyr::pivot_longer(\n    cols = c(\"glm\", \"en\"),\n    names_to = \"metodo\",\n    values_to = \"variancia\")\n\nggplot(variancias_long,\n       aes(x = grupo, y = variancia, fill = metodo)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(x = \"Grupo\", y = \"Variância\",\n       title = \"Variância dos Betas por Grupo e Método\") +\n  theme_minimal(base_size = 14)\n```\n\n::: {.cell-output-display}\n![](estimação_files/figure-html/unnamed-chunk-33-1.png){width=960}\n:::\n:::\n\n\nOs boxplots acima refletem o que já dissemos. As variabilidade das estimativas EN e tradicional são muito diferentes nos grupos 2 e 3 e razoavelmente similares no grupo 1.\nNesse plot foi ignorado o intercepto.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nvies2 =\n  sweep(estimativas, 2, beta, FUN = \"-\")^2 |>\n  apply(c(1,2), mean) |>\n  t() |>\n  tibble::as_tibble() |>\n  cbind(c(\"intercepto\", rep(c(1,2,3), each=10))) |>\n  tibble::as_tibble() |>\n  `colnames<-`(c(\"glm\", \"en\", \"grupo\"))\n\nvies2_long =\n  vies2[-1,] |>\n  tidyr::pivot_longer(\n    cols = c(\"glm\", \"en\"),\n    names_to = \"metodo\",\n    values_to = \"vies2\")\n\nggplot(vies2_long,\n       aes(x = grupo, y = vies2, fill = metodo)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(x = \"Grupo\", y = \"Viés²\",\n       title = \"Viés² dos Betas por Grupo e Método\") +\n  theme_minimal(base_size = 14)\n```\n\n::: {.cell-output-display}\n![](estimação_files/figure-html/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n\nO gráfico de boxplots acima mostra como os dois modelos se comportam em termos de viés ao quadrado. É possível notar que o viés da estatística EN é mais enviesada que a estatística tradicional no grupo com coeficientes diferentes de zero.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nnorma =\n  estimativas |>\n  apply(c(1,3),\n        function (x) c( l2 = crossprod(x[-1]), l1 = sum(abs(x[-1])) )) |>\n  tibble::as_tibble() |>\n  dplyr::mutate(\n    norma = c(\"L2^2\", \"L1\")\n  ) |>\n  tidyr::pivot_longer(\n    cols = -norma,\n    names_to = c(\"metodo\", \"iter\"),\n    names_sep = \"\\\\.\",\n    values_to = \"valor\"\n  ) |>\n  dplyr::mutate(iter = as.integer(iter))\n\nggplot(norma,\n       aes(x = norma, y = valor, fill = metodo)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(x = \"Tipo de norma\", y = \"Norma\",\n       title = \"Norma dos Betas (ao longo das iterações bootstrap)\") +\n  theme_minimal(base_size = 14)\n```\n\n::: {.cell-output-display}\n![](estimação_files/figure-html/unnamed-chunk-35-1.png){width=960}\n:::\n:::\n\n\nComo esperado, as normas de beta (em cada iteração e excluído o intercepto) são, em geral, menores na regressão EN comparado com o MLE.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(dplyr)\nlibrary(tidyr)\n\nsignificancia =\n  estimativas |>\n  apply(c(1,2),\n        function(x) {q = quantile(x, probs = c(.025, .975)); 0 > q[1] & 0 < q[2]}\n        ) |>\n  rbind(\"real\" = beta == 0) |>\n  t() |>\n  {\\(.) !.[,c(3,1,2)]}() |>\n  tibble::as_tibble()\n\nsignificancia_long=\n  significancia |>\n  mutate(sim = row_number()) |>\n  pivot_longer(-sim, names_to = \"modelo\", values_to = \"sig\") |>\n  mutate(sim = sim - 1)\n\nggplot(significancia_long, aes(x = modelo, y = sim, fill = sig)) +\n  geom_tile(color = \"white\") +\n  scale_fill_manual(\n    values = c(`TRUE` = \"darkgreen\", `FALSE` = \"red\"),\n    labels = c(\"Não significativo\", \"Significativo\")\n  ) +\n  scale_y_reverse() +\n  labs(x = \"Modelo\", y = \"Beta\", fill = \"\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 270, hjust = 1),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](estimação_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\nAcima temos uma pseudo-significância dos estimadores.",
    "supporting": [
      "estimação_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}