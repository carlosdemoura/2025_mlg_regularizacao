{
  "hash": "49d8d38633cebc493d931aabef7de4a1",
  "result": {
    "engine": "knitr",
    "markdown": "# Estimação\n\n::: {style=\"text-align: right\"}\n\"Se isto for possível,<br>\nPois, me contem,<br>\nComo escrever de novo,<br>\nUm jornal de ontem\"<br>\nTom Zé\n:::\n\n## Regularização como uma função de perda\n\nAté agora vimos exemplos de como a regularização pode ser vista como uma função de perda na estimação de mínimos quadrados. Essa ideia pode ser estendida para a estimação via máxima verossimilhança (como veremos adiante).\nAntes disso, mostraremos um exemplo (retirado de @islr) de como a regressão ridge e lasso diminuem os coeficiente na prática.\n\nConsidere o caso em que $n = p$, $X = I_n$, e não temos intercepto. Assim $\\hat\\beta_{OLS} = Y$. Nesse caso, é possível mostrar que \n\n$$\n\\hat\\beta_j^{ridge} = \\frac{Y_j}{1 + \\lambda}\n$$\ne\n\n$$\n\\hat\\beta_j^{lasso} = \n\\begin{cases}\nY_j - \\lambda/2, & \\text{se } Y_j > \\lambda/2;\\\\\nY_j + \\lambda/2, & \\text{se } Y_j < - \\lambda/2;\\\\\n0, & \\text{se } -\\lambda/2 \\leq Y_j \\leq \\lambda/2.\n\\end{cases}\n$$\n\nCom esse exemplo simples, podemos entender porque a regressão ridge nunca iguala os coeficientes a zero, coisa que a lasso pode fazer. Essa característica de zerar coeficientes a depender do valor que os dados assumem é conhecido como _soft-tresholding_ (limiar suave). De acordo com @en, a elastic net mistura tanto a shinkage da regressão ridge quanto o soft tresholding da lasso.\n\n\n[![Gráfico das condições acima.](../img/islr_soft-tresholding.png){width=90%}](https://www.statlearning.com/)\n\n\n\n\n\n\n\n\n\n## Regularização como uma restrição do espaço paramétrico\n\nÉ possível entender cada um dos processos de regularização descritos anteriormente como uma restrição do espaço paramétrico dos coeficientes de regressão. Se não fazer seleção é considerer que $\\beta \\in \\mathbb R^d$, é possível mostrar que - escolhidos os parâmetros de shrinkage - então minimar a soma de quadrados do resíduo penalizada é a mesma coisa que minimizar a soma de quadrado da regressão num espaço paramétrico menor (que depende dos parâmetros de shrinkage escolhidos).\n\nAssim, (na regressão linear normal) vale que:\n\n\n$$\n\\hat\\beta_{ridge} = \n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n\\sum_{j=1}^p \\beta_j^2 \\le t_{ridge},\n$$\n\n$$\n\\hat\\beta_{lasso} = \n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n\\sum_{j=1}^p |\\beta_j| \\le t_{lasso},\n$$\n\n$$\n\\hat\\beta_{elastic\\ net} = \n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n(1-\\alpha)\\sum_{j=1}^p |\\beta_j| + \\alpha \\sum_{j=1}^p \\beta_j^2 \\le t_{elastic\\ net}.\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndesenhar_espaço_paramétrico = function(alpha, t, título = NULL) {\n  stopifnot(\n    \"alpha deve estar entre 0 e 1\" = all(alpha >= 0, alpha <= 1),\n    \"t deve ser positivo\"          = t > 0\n  )\n  \n  F = function(x, y) alpha*(x^2 + y^2 - t) + (1-alpha)*(abs(x) + abs(y) - t)\n  \n  x = seq(-2, 2, length = 400)\n  y = seq(-2, 2, length = 400)\n  g = outer(x, y, F)\n  \n  contour(x, y, g,\n          levels = 0,\n          drawlabels = FALSE,\n          lwd = 2, asp = 1,\n          main = título,\n          cex.main = 2\n          )\n}\n\n\npar(mfrow = c(1,3))\ndesenhar_espaço_paramétrico(1,   1, \"ridge\")\ndesenhar_espaço_paramétrico(0,   1, \"lasso\")\ndesenhar_espaço_paramétrico(1/2, 1, \"elastic net\")\n```\n\n::: {.cell-output-display}\n![](estimação_files/figure-html/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n## Regularização nos MLGs\n\nO modelo linear normal pode ser especificado diretamente por meio de seus resíduos, mas - em geral - isso não é possível em todos os MLGs. Aqui, apresentaremos a técnica descrita em @en_glm, que generaliza a regularização elastic net para os MLGs.\nGrosso modo, em vez de minimizar a SQRes penalizada (que nem sempre está bem definida), vamos minimizar o inverso aditivo da log-verossimilhança penalizada (que sempre está bem definida num MLG).\n\n$$\n\\hat{\\beta}\n= \\arg\\min_{\\beta}\n\\left\\{\n  -\\frac{1}{n}\\sum_{i=1}^n \\text{loglik}\\left(y_i, X \\beta \\right) + P_{\\alpha, \\lambda}(\\beta)\n\\right\\},\n$$\n\nonde $P_{\\alpha, \\lambda}(\\beta)$ é a função de penalização elastic net, ou seja\n$$\nP_{\\alpha, \\lambda}(\\beta) = \n\\lambda \\left( \\frac{1 - \\alpha}{2}  \\sum_{j=1}^p \\beta_j^2 + \\alpha\\sum_{j=1}^p |\\beta_j| \\right).\n$$\n\nCom isso, temos um algoritmo de mínimos quadrados ponderados (IRLS) dado a seguir.\n\n::: {.callout-tip}\n## IRLS elastic net\n\nSelecione um valor de $\\alpha \\in [0,1]$ e valor de $\\lambda \\in \\mathbb R$.\n\nInicialize o algritmo de maneira razoável - com $\\hat\\beta^{(0)} = 0$, ou $\\eta^{(0)} = Y$, ou qualquer coisa que faça sentido. Doravante para $t = 1, 2, \\dots$ (até que se atinja convergência) faça:\n\n1. Compute $\\eta_i^{(t)} = X\\hat{\\beta}^{(t)}$ e também $\\ \\mu_i^{(t)} = g^{-1}(\\eta_i^{(t)})$, para $i \\in 1:n$;\n\n2. Compute $z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)}) \\left( \\frac{d\\mu_i^{(t)}}{d\\eta_i^{(t)}} \\right)^{-1}$\ne também\n$w_i^{(t)} = \\frac{1}{V(\\mu_i^{(t)})} \\left( \\frac{d\\mu_i^{(t)}}{d\\eta_i^{(t)}} \\right)^2$,\npara $i \\in 1:n$;\n\n3. Resolva\n\n$$\n\\hat{\\beta}^{(t+1)}\n=\n\\arg\\min_{\\beta}\n\\left\\{\n\\frac{1}{2n}\n\\sum_{i=1}^n w_i^{(t)}\n\\bigl( z_i^{(t)} - X_i \\beta \\bigr)^2\n+ P_{\\alpha, \\lambda}(\\beta)\n\\right\\}.\n$$\n:::\n\n\n\n\n\n\n\n\n\n## Estudo com dados simulados\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nn = 100\nbeta = c(1, 2, 0, 0)\nX = matrix(1, nrow = n, ncol = 4)\nX[,2] = rnorm(n)\nX[,3] = 2 * X[,2] + rnorm(n)\nX[,4] = rnorm(n)\nmu = exp(X %*% beta)\nY = rpois(n, mu)\n\ndf = \n  cbind(Y,X[,2:4]) |>\n  tibble::as_tibble() |>\n  `colnames<-`(c(\"ataques\", \"temperatura\", \"sorvete\", \"bobagem\"))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod = glm(ataques~temperatura+sorvete+bobagem, family=poisson(link = \"log\"), data = df)\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = ataques ~ temperatura + sorvete + bobagem, family = poisson(link = \"log\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.03466    0.05866  17.638   <2e-16 ***\ntemperatura  1.90265    0.05846  32.545   <2e-16 ***\nsorvete      0.04159    0.02118   1.964   0.0496 *  \nbobagem      0.01286    0.01634   0.787   0.4313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 7749.835  on 99  degrees of freedom\nResidual deviance:   94.349  on 96  degrees of freedom\nAIC: 431.4\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n::: {.callout-note collapse=\"true\"}\nApenas para ver o efeito que um tamanho de amostra menor teria.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm(ataques~temperatura+sorvete+bobagem, family=poisson(link = \"log\"), data = df[1:20,]) |>\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = ataques ~ temperatura + sorvete + bobagem, family = poisson(link = \"log\"), \n    data = df[1:20, ])\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.94735    0.15913   5.953 2.63e-09 ***\ntemperatura  1.84569    0.12416  14.866  < 2e-16 ***\nsorvete      0.11211    0.06978   1.607    0.108    \nbobagem      0.03987    0.08303   0.480    0.631    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 438.897  on 19  degrees of freedom\nResidual deviance:  23.168  on 16  degrees of freedom\nAIC: 88.601\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n:::\n\n::: {.callout-tip}\nEm modelos com função de ligação canônica, o algoritmo acima simplica. O segundo passo da iteração fica:\nCompute $z_i^{(t)} = \\eta_i^{(t)} + \\frac{(y_i - \\mu_i^{(t)})}{V(\\mu_i^{})}$\ne também\n$w_i^{(t)} = V(\\mu_i^{(t)})$,\npara $i \\in 1:n$;\n:::\n\nSabemos que no caso da Poisson, a função de ligação canônica é o log e a função de variância é $\\mu_i$. Assim, nosso algoritmo IRLS-EN fica da seguinte forma.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqres_en = function(beta, w, z) {\n  sum(  w * (z - X %*% beta)^2  ) / (2 * nrow(X)) +\n    lambda * (  alpha * sum(abs(beta[-1])) + ((1 - alpha)/2) * crossprod(beta[-1])  )\n}\n\niter = 20\nalpha = .5; lambda = 2\nbeta_hat = matrix(1, nrow = iter, ncol = 4)\n\nfor (t in 2:iter) {\n  eta = X %*% beta_hat[t-1,]\n  mu = exp(eta)\n  z = eta + (Y - mu)/mu\n  beta_hat[t,] = \n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_en,\n      w      = mu,\n      z      = z,\n      method = \"BFGS\"\n    )$par\n}\n\nbeta_hat[iter,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.3955786928 1.3117519002 0.2194140504 0.0004518438\n```\n\n\n:::\n:::\n\n\n\n### Bootstrap\n\nA\n\n\n### Lidando com outliers\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345); x = rnorm(20); e = rt(20, 1); y = 2*x + e; plot(y~x); lm(y~0+x)\n```\n\n::: {.cell-output-display}\n![](estimação_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ 0 + x)\n\nCoefficients:\n    x  \n6.136  \n```\n\n\n:::\n:::\n\n",
    "supporting": [
      "estimação_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}