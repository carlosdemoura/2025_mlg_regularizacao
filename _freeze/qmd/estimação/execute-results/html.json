{
  "hash": "7b8847dbc139cb88a663cd7f8971608d",
  "result": {
    "engine": "knitr",
    "markdown": "# Estimação\n\n::: {style=\"text-align: right\"}\n\"Se isto for possível,<br>\nPois, me contem,<br>\nComo escrever de novo,<br>\nUm jornal de ontem\"<br>\nTom Zé\n:::\n\n## Regularização como uma função de perda\n\nAté agora vimos exemplos de como a regularização pode ser vista como uma função de perda na estimação de mínimos quadrados. Essa ideia pode ser estendida para a estimação via máxima verossimilhança (como veremos adiante).\nAntes disso, mostraremos um exemplo de como a regressão ridge e lasso diminuem os coeficiente na prática.\n\n\n## Regularização como uma restrição do espaço paramétrico\n\nÉ possível entender cada um dos processos de regularização descritos anteriormente como uma restrição do espaço paramétrico dos coeficientes de regressão. Se não fazer seleção é considerer que $\\beta \\in \\mathbb R^d$, é possível mostrar que - escolhidos os parâmetros de shrinkage - então minimar a soma de quadrados do resíduo penalizada é a mesma coisa que minimizar a soma de quadrado da regressão num espaço paramétrico menor (que depende dos parâmetros de shrinkage escolhidos).\n\nAssim, (na regressão linear normal) vale que:\n\n\n$$\n\\hat\\beta_{ridge} = \n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n\\sum_{j=1}^p \\beta_j^2 \\le t_{ridge},\n$$\n\n$$\n\\hat\\beta_{lasso} = \n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n\\sum_{j=1}^p |\\beta_j| \\le t_{lasso},\n$$\n\n$$\n\\hat\\beta_{elastic\\ net} = \n\\arg\\min_{\\beta} \\{SQRes\\}\n\\quad \\text{com } \\beta \\text{ tal que} \\quad\n(1-\\alpha)\\sum_{j=1}^p |\\beta_j| + \\alpha \\sum_{j=1}^p \\beta_j^2 \\le t_{elastic\\ net}.\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndesenhar_espaço_paramétrico = function(alpha, t, título = NULL) {\n  stopifnot(\n    \"alpha deve estar entre 0 e 1\" = all(alpha >= 0, alpha <= 1),\n    \"t deve ser positivo\"          = t > 0\n  )\n  \n  F = function(x, y) alpha*(x^2 + y^2 - t) + (1-alpha)*(abs(x) + abs(y) - t)\n  \n  x = seq(-2, 2, length = 400)\n  y = seq(-2, 2, length = 400)\n  g = outer(x, y, F)\n  \n  contour(x, y, g,\n          levels = 0,\n          drawlabels = FALSE,\n          lwd = 2, asp = 1,\n          main = título,\n          cex.main = 2\n          )\n}\n\n\npar(mfrow = c(1,3))\ndesenhar_espaço_paramétrico(1,   1, \"ridge\")\ndesenhar_espaço_paramétrico(0,   1, \"lasso\")\ndesenhar_espaço_paramétrico(1/2, 1, \"elastic net\")\n```\n\n::: {.cell-output-display}\n![](estimação_files/figure-html/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n## Regularização nos MLGs\n\nO modelo linear normal pode ser especificado diretamente por meio de seus resíduos, mas - em geral - isso não é possível em todos os MLGs. Aqui, apresentaremos a técnica descrita em @en_glm, que generaliza a regularização elastic net para os MLGs.\nGrosso modo, em vez de minimizar a SQRes penalizada (que nem sempre está bem definida), vamos minimizar o inverso aditivo da log-verossimilhança penalizada (que sempre está bem definida num MLG).\n\n$$\n(\\hat{\\beta}_0, \\hat{\\beta})\n= \\arg\\min_{\\beta_0,\\beta}\n\\left\\{\n  -\\frac{1}{n}\\sum_{i=1}^n \\text{loglik}\\left(y_i, X \\beta \\right) + P_{\\alpha, \\lambda}(\\beta)\n\\right\\},\n$$\n\nonde $P_{\\alpha, \\lambda}(\\beta)$ é a função de penalização elastic net, ou seja\n$$\nP_{\\alpha, \\lambda}(\\beta) = \n\\lambda \\left( \\frac{1 - \\alpha}{2}  \\sum_{j=1}^p \\beta_j^2 + \\alpha\\sum_{j=1}^p |\\beta_j| \\right).\n$$\n\nCom isso, temos um algoritmo de mínimos quadrados ponderados (IRLS) dado a seguir.\n\n::: {.callout-tip}\n## IRLS elastic net\n\nSelecione um valor de $\\alpha \\in [0,1]$ e valor de $\\lambda \\in \\mathbb R$.\n\nInicialize o algritmo de maneira razoável - com $\\hat\\beta^{(0)} = 0$, ou $\\eta^{(0)} = Y$, ou qualquer coisa que faça sentido. Doravante para $t = 1, 2, \\dots$ (até que se atinja convergência) faça:\n\n1. Compute $\\eta_i^{(t)} = X\\hat{\\beta}^{(t)}$ e também $\\ \\mu_i^{(t)} = g^{-1}(\\eta_i^{(t)})$, para $i \\in 1:n$;\n\n2. Compute $z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)}) \\left( \\frac{d\\mu_i^{(t)}}{d\\eta_i^{(t)}} \\right)$\ne também\n$w_i^{(t)} = \\frac{1}{V(\\mu_i^{(t)})} \\left( \\frac{d\\mu_i^{(t)}}{d\\eta_i^{(t)}} \\right)^2$,\npara $i \\in 1:n$;\n\n3. Resolva\n\n$$\n\\hat{\\beta}^{(t+1)}\n=\n\\arg\\min_{\\beta}\n\\left\\{\n\\frac{1}{2n}\n\\sum_{i=1}^n w_i^{(t)}\n\\bigl( z_i^{(t)} - X \\beta \\bigr)^2\n+ P_{\\alpha, \\lambda}(\\beta)\n\\right\\}\n$$\n:::\n\n## Estudo com dados simulados\n\n\n::: {.cell}\n\n:::\n\n\n\n### Bootstrap\n\nA",
    "supporting": [
      "estimação_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}