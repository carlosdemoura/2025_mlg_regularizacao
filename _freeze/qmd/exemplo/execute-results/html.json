{
  "hash": "8825feeaa98792890465b401531b2b75",
  "result": {
    "engine": "knitr",
    "markdown": "# Exemplo prático\n\n::: {style=\"text-align: right\"}\n\"Minha jangada vai sair por mar\"<br>\nCaymmi\n:::\n\nPara consolidar os conceitos de regularização, aplicaremos em um cenário apresentado por Golub et al. (1999), um conjunto de dados de expressão gênica com respostas binárias que indicam o tipo de Leucemia, ALL (Leucemia Linfoblástica Aguda) e AML (Leucemia Mieloide Aguda).\n\nAssim como em @en_glm2, utilizaremos os dados pré-processados por Dettling (2004).\n\nEsse dataset é do tipo $p \\gg N$, ou seja, milhares de genes, poucos pacientes, tornando a regularização quase como obrigatória - visto que um modelo logístico tradicional não possui graus de liberdade suficientes para estimar os coeficientes.\n\n## Pré-processamento dos Dados\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(glmnet)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\n\"https://web.stanford.edu/~hastie/glmnet/glmnetData/Leukemia.RData\" |>\n  url() |>\n  load()\n\nX <- Leukemia$x\ny <- Leukemia$y\n\n\ncount_y0 <- sum(y == 0)\ncount_y1 <- sum(y == 1)\n\ntabela <- data.frame(N = nrow(X),\n          p = ncol(X),\n          y0 = count_y0,\n          y1 = count_y1)\nkable(tabela, \n      col.names = c(\"Observações$(N)$\",\n                    \"Preditores$(p)$\",\n                    \"$Y=0$\",\n                    \"$Y=1$\"),\n      align = c('l', 'l', 'c', 'c'))\n```\n\n::: {.cell-output-display}\n\n\n|Observações$(N)$ |Preditores$(p)$ | $Y=0$ | $Y=1$ |\n|:----------------|:---------------|:-----:|:-----:|\n|72               |3571            |  47   |  25   |\n\n\n:::\n:::\n\n\n## Ajuste dos Modelos\n\n1. **Ridge ($\\alpha = 0$):** Mantém todas os genes, encolhendo os coeficientes em direção a zero.\n\n2. **Lasso ($\\alpha=1$):** Zera a maioria dos coeficientes rapidamente, realizando seleção de genes mais informativos.\n\n3. **Elastic Net ($\\alpha=0.5$):** Combinação de ambas, agrupando variáveis correlacionadas.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_ridge <- glmnet(X, y, family = \"binomial\", alpha = 0)\n\nfit_lasso <- glmnet(X, y, family = \"binomial\", alpha = 1)\n\nfit_enet  <- glmnet(X, y, family = \"binomial\", alpha = 0.5)\n\npar(mfrow = c(1, 3), \n    mar = c(4, 4.5, 6, 2), \n    oma = c(0, 0, 2, 0))\n\nplot(fit_ridge, xvar = \"lambda\", main = \"Ridge\")\nplot(fit_enet, xvar = \"lambda\", main = \"Elastic Net\")\nplot(fit_lasso, xvar = \"lambda\", main = \"LASSO\")\n```\n\n::: {.cell-output-display}\n![](exemplo_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Escolha do Hiperparâmetro ($\\lambda$) via Validação Cruzada\n\nPara encontrar o melhor valor para o hiperparâmetro $\\lambda$ usaremos `cv.glmnet` que utiliza k-fold cross-validation (padrão $k=10$) e a métrica de desvio (deviance) ou erro de classificação.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\n\ncv_lasso <- cv.glmnet(X, y, family = \"binomial\", alpha = 1, type.measure = \"class\")\ncv_ridge <- cv.glmnet(X, y, family = \"binomial\", alpha = 0, type.measure = \"class\")\ncv_enet  <- cv.glmnet(X, y, family = \"binomial\", alpha = 0.5, type.measure = \"class\")\n\nmin_y <- min(c(cv_ridge$cvm, cv_lasso$cvm, cv_enet$cvm))\nmax_y <- max(c(cv_ridge$cvm, cv_lasso$cvm, cv_enet$cvm))\n\n\npar(mfrow = c(1, 3), \n    mar = c(4, 4.5, 6, 2), \n    oma = c(0, 0, 2, 0))\n\nplot(cv_ridge, ylim = c(min_y, max_y))\ntitle(\"Ridge\", line = 2.5)\n\nplot(cv_enet, ylim = c(min_y, max_y))\ntitle(\"Elastic Net\", line = 2.5)\n\nplot(cv_lasso, ylim = c(min_y, max_y))\ntitle(\"LASSO\", line = 2.5)\n```\n\n::: {.cell-output-display}\n![](exemplo_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Inferência e Esparsidade\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_metrics <- function(cv_fit, name) {\n  lambda_optimal <- cv_fit$lambda.min\n  \n  coefs <- coef(cv_fit, s = \"lambda.min\")\n  \n  n_nonzero <- sum(coefs != 0) - 1 \n  \n  return(data.frame(\n    Modelo = name,\n    Lambda_Opt = round(log(lambda_optimal), 3),\n    Genes_Selecionados = n_nonzero,\n    Erro_CV_Min = round(min(cv_fit$cvm), 4) \n  ))\n}\n\n\nresultados <- rbind(\n  get_metrics(cv_ridge, \"Ridge (Alpha=0)\"),\n  get_metrics(cv_enet, \"Elastic Net (Alpha=0.5)\"),\n  get_metrics(cv_lasso, \"LASSO (Alpha=1)\")\n)\n\nkable(resultados, \n      caption = \"Comparação de Desempenho: Ridge, Elastic Net e LASSO\",\n      col.names = c(\"Modelo\", \"Log($\\\\lambda$)\", \"Genes Selecionados\", \"Erro Mínimo (CV)\"),\n      align = c('l', 'c', 'c', 'c'),\n      digits = 4)\n```\n\n::: {.cell-output-display}\n\n\nTable: Comparação de Desempenho: Ridge, Elastic Net e LASSO\n\n|Modelo                  | Log($\\lambda$) | Genes Selecionados | Erro Mínimo (CV) |\n|:-----------------------|:--------------:|:------------------:|:----------------:|\n|Ridge (Alpha=0)         |     3.875      |        3571        |      0.0139      |\n|Elastic Net (Alpha=0.5) |     -2.107     |         38         |      0.0139      |\n|LASSO (Alpha=1)         |     -4.056     |         16         |      0.0278      |\n\n\n:::\n:::\n\n\n",
    "supporting": [
      "exemplo_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}