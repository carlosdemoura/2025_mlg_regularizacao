{
  "hash": "959a668c801359482a511807ea162904",
  "result": {
    "engine": "knitr",
    "markdown": "# Regularização\n\n## Seleção de variáveis naïve\n\nStepwise\n\n## Ridge\n\nA técnica Ridge foi a primeira das três técnicas a surgir, no trabalho de @ridge. Originalmente, os autores buscavam entender como superar problemas em que a matriz de covariáveis $X$ estava mal-especificada.\nRelembrando que na regressão linear normal, temos que \n\n$$\nY = X \\beta + \\epsilon, \\hspace{1cm} \\epsilon \\sim N_n\\bigr( 0, \\sigma^2 I_n \\bigr)\n$$\nO estimador de máxima verossimilhança (EMV) de $\\beta$ será o mesmo estimador de mínimos quadrados (EMQ)\n\n$$\n\\hat\\beta = (X^\\top X)^{-1} X^\\top Y,\n$$\n\nse:\n\n* $X^\\top X$ for inversível;\n* A matriz de covariáveis é ortogonalizável, i.e., os dados foram coletados de maneira independente;\n* há menos betas que observações, isto é `ncol(X)` << `nrow(X)`.\n\nAlém disso, $\\hat\\beta$ é não viciado, consistente e tem matriz de covariâncias dada por\n\n$$\ncov\\bigr(\\hat\\beta\\bigr) = \\sigma^2 (X^\\top X)^{-1}.\n$$\n\nUma vez que temos a distribuição do estimador, fica fácil fazer inferência via intervalos de confiança, por exemplo.\n\n$$\n\\hat\\beta \\sim N_p\\left(\\beta,\\; \\sigma^2 (X^\\top X)^{-1}\\right).\n$$\n\nNesse sentido, se temos uma matriz de dados problemática - no sentido em que $(X^\\top X)^{-1}$ não está bem definida, teremos problema de estimação via EMQ.\n\nVeja o exemplo numérico abaixo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\nn = 10\nbeta = c(1, 0)\nX = cbind(1:n, 2*(1:n))\nY = X %*% beta + rnorm(n)\n```\n:::\n\n\n<details>\n<summary><strong>Ver matriz X</strong></summary>\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    4\n[3,]    3    6\n[4,]    4    8\n[5,]    5   10\n[6,]    6   12\n```\n\n\n:::\n\n```{.r .cell-code}\nmatrixcalc::is.singular.matrix(t(X)%*%X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n</details>\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nplot(Y~X[,1])\nplot(Y~X[,2])\n```\n\n::: {.cell-output-display}\n![](regularização_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(Y~0+X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n    X1      X2  \n0.9544      NA  \n```\n\n\n:::\n:::\n\n<details>\n<summary><strong>E se a regressão estivesse na outra covariável?</strong></summary>\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\nbeta = c(0, 1)\nX = cbind(1:n, 2*(1:n))\nY = X %*% beta + rnorm(n)\n\nlm(Y~0+X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n   X1     X2  \n1.954     NA  \n```\n\n\n:::\n:::\n\n\nDetalhes\n\n</details>\n\nComentários sobre o modelo não saber selecionar variáveis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\nbeta = c(1, 0)\nX = cbind(1:n, 2*(1:n))\nruido = cbind(rep(0,n), rnorm(n,0,.1))\nX = X + ruido\nY = X %*% beta + rnorm(n)\n```\n:::\n\n\n<details>\n<summary><strong>Ver matriz X</strong></summary>\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]      [,2]\n[1,]    1  2.058553\n[2,]    2  4.070947\n[3,]    3  5.989070\n[4,]    4  7.954650\n[5,]    5 10.060589\n[6,]    6 11.818204\n```\n\n\n:::\n\n```{.r .cell-code}\nmatrixcalc::is.singular.matrix(t(X)%*%X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n</details>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(Y~0+X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n    X1      X2  \n 6.658  -2.820  \n```\n\n\n:::\n:::\n\n\nComentários sobre combinações lineares, parcimônia e inflação da variância na inferência.\n\nA proposta de @ridge\n\n\n\n\n\n\n\n## Lasso\n\nA regressão lasso surgiu com o artigo de @lasso.\n\n\n\n\n\n\n\n## Comparação das técnicas\n\n::: {style=\"text-align: right\"}\n\"You Can't Always Get What You Want\"\n:::\n\n\n\n\n\n\n\n## Elastic net\n\nO artigo @en introduz o elastic net, que nada mais é que uma mistura das técnicas ridge e lasso.\n\n\n\n\n\n\n\n## Lidando com outliers\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345); x = rnorm(20); e = rt(20, 1); y = 2*x + e; plot(y~x); lm(y~0+x)\n```\n\n::: {.cell-output-display}\n![](regularização_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ 0 + x)\n\nCoefficients:\n    x  \n6.136  \n```\n\n\n:::\n:::\n\n\n\n",
    "supporting": [
      "regularização_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}