{
  "hash": "3bea03bda0921b453fa80ea46cea7f67",
  "result": {
    "engine": "knitr",
    "markdown": "# Regularização\n\n## Seleção de variáveis naïve\n\nStepwise\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nf = function(n) {\n  s = 0\n  for (i in 1:n) {\n    s = s + choose(n, i)\n  }\n  s\n}\n\nseq(1, 20, 1) |>\n  lapply(f) |>\n  unlist() |>\n  plot(type = 'l',\n       main = \"\",\n       xlab = \"número de covariáveis numéricas\",\n       ylab = \"número de possíveis modelos\"\n       )\n```\n\n::: {.cell-output-display}\n![](regularização_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n## Ridge\n\nA técnica Ridge foi a primeira das três técnicas a surgir, no trabalho de @ridge. Originalmente, os autores buscavam entender como superar problemas em que a matriz de covariáveis $X$ estava mal-especificada.\nRelembrando que na regressão linear normal, temos que \n\n$$\nY = X \\beta + \\epsilon, \\hspace{1cm} \\epsilon \\sim N_n\\bigr( 0, \\sigma^2 I_n \\bigr)\n$$\nO estimador de máxima verossimilhança (EMV) de $\\beta$ será o mesmo estimador de mínimos quadrados (EMQ)\n\n$$\n\\hat\\beta = (X^\\top X)^{-1} X^\\top Y,\n$$\n\nse:\n\n* $X^\\top X$ for inversível;\n* A matriz de covariáveis é ortogonalizável, i.e., os dados foram coletados de maneira independente;\n* há menos betas que observações, isto é `ncol(X)` << `nrow(X)`.\n\nAlém disso, $\\hat\\beta$ é não viciado, consistente e tem matriz de covariâncias dada por\n\n$$\ncov\\bigr(\\hat\\beta\\bigr) = \\sigma^2 (X^\\top X)^{-1}.\n$$\n\nUma vez que temos a distribuição do estimador, fica fácil fazer inferência via intervalos de confiança, por exemplo.\n\n$$\n\\hat\\beta \\sim N_p\\left(\\beta,\\; \\sigma^2 (X^\\top X)^{-1}\\right).\n$$\n\nNesse sentido, se temos uma matriz de dados problemática - no sentido em que $(X^\\top X)^{-1}$ não está bem definida, teremos problema de estimação via EMQ.\n\nVeja o exemplo numérico abaixo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\nn = 10\nbeta = c(1, 0)\nX = cbind(1:n, 2*(1:n))\nY = X %*% beta + rnorm(n)\n```\n:::\n\n\n<details>\n<summary><strong>Ver matriz X</strong></summary>\n<div class=\"details-content\">\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    4\n[3,]    3    6\n[4,]    4    8\n[5,]    5   10\n[6,]    6   12\n```\n\n\n:::\n\n```{.r .cell-code}\nmatrixcalc::is.singular.matrix(t(X)%*%X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n</div>\n</details>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow=c(1,2))\nplot(Y~X[,1])\nplot(Y~X[,2])\n```\n\n::: {.cell-output-display}\n![](regularização_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(Y~0+X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n    X1      X2  \n0.9544      NA  \n```\n\n\n:::\n:::\n\n<details>\n<summary><strong>E se a regressão estivesse na outra covariável?</strong></summary>\n<div class=\"details-content\">\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\nbeta = c(0, 1)\nX = cbind(1:n, 2*(1:n))\nY = X %*% beta + rnorm(n)\n\nlm(Y~0+X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n   X1     X2  \n1.954     NA  \n```\n\n\n:::\n:::\n\n\nDetalhes\n\n</div>\n</details>\n\nComentários sobre o modelo não saber selecionar variáveis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\nbeta = c(1, 0)\nX = cbind(1:n, 2*(1:n))\nruido = cbind(rep(0,n), rnorm(n,0,.1))\nX = X + ruido\nY = X %*% beta + rnorm(n)\n```\n:::\n\n\n<details>\n<summary><strong>Ver matriz X</strong></summary>\n<div class=\"details-content\">\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]      [,2]\n[1,]    1  2.058553\n[2,]    2  4.070947\n[3,]    3  5.989070\n[4,]    4  7.954650\n[5,]    5 10.060589\n[6,]    6 11.818204\n```\n\n\n:::\n\n```{.r .cell-code}\nmatrixcalc::is.singular.matrix(t(X)%*%X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\neigen(t(X)%*%X)$values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.918025e+03 1.070613e-02\n```\n\n\n:::\n:::\n\n</div>\n</details>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(Y~0+X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n    X1      X2  \n 6.658  -2.820  \n```\n\n\n:::\n:::\n\n\nComentários sobre combinações lineares, parcimônia e inflação da variância na inferência.\n\nA proposta de @ridge é adicionar um certo múltiplo da matriz identidade à $X^\\top X$, de modo que seja possível superar os problemas de uma matriz $X$ mal especificada. @islr mostra que isso é equivalente a adicionar uma punição no processo de estimação por mínimos quadrados. Seja $SQRes = (Y - X\\beta)^\\top(Y - X\\beta)= \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2$, assim:\n\n$$\n\\hat\\beta_{ridge} =\n\\arg\\min_{\\beta} \\left\\{ SQRes + \\lambda_{ridge}\\sum_{j=1}^p \\beta_j^2 \\right\\},\n$$\n\nem que $\\lambda_{ridge}$ é uma constante positiva (escolhida pelo pesquisador) que dá o peso dessa punição.\n\nNote que:\n\n* À medida que $\\lambda$ cresce menores ficam as estimativas dos betas, $\\hat\\beta_{ridge} \\rightarrow 0,\\ \\lambda_{ridge} \\rightarrow \\infty$.\n* Não faz sentido incluir o intercepto no processo de shrinkage, pois $g(\\beta_0)$ é o valor esperado da variável resposta dado que as demais covariáveis são zero, em que $g$ é uma função de ligação.\n* Fica claro que a depender do valor de $\\lambda$ que escolhemos, estamos incluindo algum viés no modelo.\n* A estimação ridge diminui as estimativas dos betas na mesma proporção (se comparada com o MMQ tradicional).\n* Em diminuindo a estimativa dos betas, diminui-se também a variância envolvida nas estimativas dos parâmetros.\n\n::: {.callout-warning}\nAo usar métodos de regularização, estamos fazendo um trade-off entre variância e viés. Grosso modo, estamos inserido algum viés em nossas estimativas, a fim de diminuir a variância na estimação dos parâmetros e assim fazer inferências mais precisas.\n\nUm bom método de escolha do parâmetro de shrinkage é crucial para o bom funcionamento da estimação ridge.\nFalaremos disso nas próximas seções.\n_Spoiler: faremos validação cruzada._\n:::\n\n[![Analogia dos dardos para pensar variância e viés.](../img/bias_variance_dartboard.png){width=60%}](https://neuroimaging-data-science.org/content/007-ml/005-model-selection.html)\n\nO exemplo abaixo está no livro @islr, `Credit` é um conjunto de dados incluído no pacote ISLR, contendo informações sobre 4000 clientes de um banco, com o objetivo de modelar e entender fatores associados ao limite de crédito de cada pessoa.\n\n[![Exemplo de regressão ridge.](../img/islr_ridge.png){width=90%}](https://www.statlearning.com/)\n\n::: {.callout-tip}\n## Padronização das covariáveis\n\nA técnica ridge pode ser muito sensível à escala das covariáveis. Nesse sentido, vale a pena padronizar as covariáveis pelo desvio-padrão para evitar a influência desse efeito.\n:::\n\n\n\n\n\n\n## Lasso\n\nA regressão lasso surgiu com o artigo de @lasso. No caso da regressão linear normal, a diferença entre lasso e ridge pode ser vista na função de perda da estimação mínimos quadrados. Em vez de usar a norma $\\ell_2$ (como faz a regressão ridge), a regressão lasso usa a norma $\\ell_1$ de beta.\n\n$$\n\\hat\\beta_{lasso} =\n\\arg\\min_{\\beta} \\left\\{ SQRes + \\lambda_{lasso}\\sum_{j=1}^p |\\beta_j| \\right\\},\n$$\n\nMuitas das observações que fizemos para a estimação ridge se aplicam tambpem à regressão lasso. A principal diferênça entre essas técnicas está no fato que a regressão lasso faz, de fato, uma seleção de variáveis. Isso se dá pois a regressão lasso pode zerar os coeficientes de algumas covariáveis.\n\n::: {.callout-note collapse=\"true\"}\nO fato de a regressão lasso poder zerar os coeficientes das covariáveis está associado com a geometria imposta no espaço paramétrico.\n[![@islr pg. 222.](../img/islr_corner.png){width=90%}](https://www.statlearning.com/)\n:::\n\n\nNo mesmo banco de dados `Credit`, @islr aplicou a regressão lasso:\n\n[![Exemplo de regressão lasso.](../img/islr_lasso.png){width=90%}](https://www.statlearning.com/)\n\n\n\n\n\n\n## Comparação das técnicas\n\n::: {style=\"text-align: right\"}\n\"You Can't Always Get What You Want\"\n:::\n\nA\n\n### Estudo sobre MSE com dados sintéticos\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqres = function(beta, X, Y) {\n  res = Y - X %*% beta\n  drop(crossprod(res))\n}\n\nsqres_ridge = function(beta, X, Y, lambda) {\n  res = Y - X %*% beta\n  drop(crossprod(res) + lambda * crossprod(beta))\n}\n\nsqres_lasso = function(beta, X, Y, lambda) {\n  res = Y - X %*% beta\n  drop(crossprod(res) + lambda * sum(abs(beta)))\n}\n```\n:::\n\n\nA\n\n::: {.callout-note collapse=\"true\"}\nNote que minimizar `sqres` deve dar o mesmo resultado que `lm`.\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = optim(\n  par    = rep(0,ncol(X)),\n  fn     = sqres,\n  X      = X,\n  Y      = Y,\n  method = \"BFGS\"\n)\n\nround(fit$par, 5) |> `names<-`(paste0(\"X\", 1:ncol(X)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      X1       X2 \n 6.65825 -2.81988 \n```\n\n\n:::\n\n```{.r .cell-code}\nlm(Y~0+X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Y ~ 0 + X)\n\nCoefficients:\n    X1      X2  \n 6.658  -2.820  \n```\n\n\n:::\n:::\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(12345)\nn = 20\nbeta = exp(-seq(-2,2,.5)^2)\nX = rnorm(n*length(beta)) |> matrix(nrow = n, ncol = length(beta))\nY = X %*% beta + rnorm(n,0,1)\n\nmse_ridge = mse_lasso = numeric()\n\nfor (lambda in 0:20) {\n  beta_ridge =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_ridge,\n      X      = X,\n      Y      = Y,\n      lambda = lambda,\n      method = \"BFGS\"\n    )$par\n\n    mse_ridge[lambda + 1] = mean((beta_ridge - beta)^2)\n\n  beta_lasso =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_lasso,\n      X      = X,\n      Y      = Y,\n      lambda = lambda,\n      method = \"BFGS\"\n    )$par\n\n    mse_lasso[lambda + 1] = mean((beta_lasso - beta)^2)\n}\n\nplot(mse_ridge,  type=\"l\", lwd = 2, col = 1,\n     ylim = c(0, 0.1),\n     main = \"alguns parâmetros muito pequenos\",\n     ylab = \"MSE\", xlab = \"lambda\")\nlines(mse_lasso, type=\"l\", lwd = 2, col = 2, lty = \"aa\")\n\nmse_lm = mean((lm(Y~0+X)$coeff - beta)^2)\nabline(h = mse_lm, lwd = 2, col = 3, lty = \"44\")\n\nlegend(\"topright\",\n       legend = c(\"Ridge\", \"Lasso\", \"OLS\"),\n       col = c(1, 2, 3),\n       lty = c(1, 2, 4),\n       lwd = 2,\n       bty = \"n\")\n```\n\n::: {.cell-output-display}\n![](regularização_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=70%}\n:::\n:::\n\n<details>\n<summary><strong>Comparar estimativas</strong></summary>\n<div class=\"details-content\">\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_lasso =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_lasso,\n      X      = X,\n      Y      = Y,\n      lambda = which.min(mse_lasso)-1,\n      method = \"BFGS\"\n    )$par\nbeta_ridge =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_ridge,\n      X      = X,\n      Y      = Y,\n      lambda = which.min(mse_ridge)-1,\n      method = \"BFGS\"\n    )$par\nbeta_lm = lm(Y~0+X)$coef\n\ncbind(beta, beta_lasso, beta_ridge, beta_lm) |>\n  `colnames<-`(c(\"real\", \"lasso\", \"ridge\", \"lm\")) |>\n  knitr::kable(digits = 3, caption = \"Estimativas para betas (caso betas pequenos)\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Estimativas para betas (caso betas pequenos)\n\n|   |  real| lasso|  ridge|     lm|\n|:--|-----:|-----:|------:|------:|\n|X1 | 0.018| 0.000| -0.072|  0.009|\n|X2 | 0.105| 0.062|  0.107|  0.168|\n|X3 | 0.368| 0.307|  0.344|  0.241|\n|X4 | 0.779| 1.022|  0.936|  1.213|\n|X5 | 1.000| 1.070|  0.966|  1.299|\n|X6 | 0.779| 0.536|  0.515|  0.561|\n|X7 | 0.368| 0.270|  0.277|  0.390|\n|X8 | 0.105| 0.001|  0.024|  0.285|\n|X9 | 0.018| 0.000| -0.056| -0.027|\n\n\n:::\n:::\n\n</div>\n</details>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(12345)\nn = 20\nbeta = c( rep(1,3), rep(0,5) )\nX = rnorm(n*length(beta)) |> matrix(nrow = n, ncol = length(beta))\nY = X %*% beta + rnorm(n,0,1)\n\nmse_ridge = mse_lasso = numeric()\n\nfor (lambda in 0:20) {\n  beta_ridge =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_ridge,\n      X      = X,\n      Y      = Y,\n      lambda = lambda,\n      method = \"BFGS\"\n    )$par\n\n    mse_ridge[lambda + 1] = mean((beta_ridge - beta)^2)\n\n  beta_lasso =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_lasso,\n      X      = X,\n      Y      = Y,\n      lambda = lambda,\n      method = \"BFGS\"\n    )$par\n\n    mse_lasso[lambda + 1] = mean((beta_lasso - beta)^2)\n}\n\nplot(mse_ridge,  type=\"l\", lwd = 2, col = 1,\n     main = \"esparsidade nos parâmetros reais\",\n     ylim = c(0, 0.1),\n     ylab = \"MSE\", xlab = \"lambda\")\nlines(mse_lasso, type=\"l\", lwd = 2, col = 2, lty = \"aa\")\nlegend(\"topright\",\n       legend = c(\"Ridge\", \"Lasso\"),\n       col = c(1, 2),\n       lty = c(1, 2),\n       lwd = 2,\n       bty = \"n\")\n```\n\n::: {.cell-output-display}\n![](regularização_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=70%}\n:::\n:::\n\n<details>\n<summary><strong>Comparar estimativas</strong></summary>\n<div class=\"details-content\">\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_lasso =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_lasso,\n      X      = X,\n      Y      = Y,\n      lambda = which.min(mse_lasso)-1,\n      method = \"BFGS\"\n    )$par\nbeta_ridge =\n    optim(\n      par    = rep(0,ncol(X)),\n      fn     = sqres_ridge,\n      X      = X,\n      Y      = Y,\n      lambda = which.min(mse_ridge)-1,\n      method = \"BFGS\"\n    )$par\nbeta_lm = lm(Y~0+X)$coef\n\ncbind(beta, beta_lasso, beta_ridge, beta_lm) |>\n  `colnames<-`(c(\"real\", \"lasso\", \"ridge\", \"lm\")) |>\n  knitr::kable(digits = 3, caption = \"Estimativas para betas (caso betas esparsos)\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Estimativas para betas (caso betas esparsos)\n\n|   | real|  lasso|  ridge|     lm|\n|:--|----:|------:|------:|------:|\n|X1 |    1|  0.832|  0.821|  1.228|\n|X2 |    1|  1.046|  0.997|  1.144|\n|X3 |    1|  1.104|  1.027|  1.200|\n|X4 |    0|  0.000| -0.005|  0.057|\n|X5 |    0|  0.000| -0.030|  0.033|\n|X6 |    0| -0.001| -0.174| -0.379|\n|X7 |    0| -0.232| -0.292| -0.380|\n|X8 |    0|  0.000|  0.123|  0.215|\n\n\n:::\n:::\n\n</div>\n</details>\n\n\n\n\n## Elastic net\n\n::: {style=\"text-align: right\"}\n\"Can you?\"\n:::\n\nO artigo @en introduz o elastic net, que nada mais é que uma mistura das técnicas ridge e lasso. A ideia inicial considera o seguinte estimador:\n\n\n$$\n\\begin{aligned}\n\\hat\\beta_{en}^{naïve}\n&= \\arg\\min_{\\beta} \\left\\{ SQRes + \\lambda_{ridge}\\sum_{j=1}^p \\beta_j^2 + \\lambda_{lasso}\\sum_{j=1}^p |\\beta_j| \\right\\}\\\\\n&= \\arg\\min_{\\beta} \\left\\{ SQRes + \\lambda_{en} \\left( \\alpha \\sum_{j=1}^p \\beta_j^2 + (1 - \\alpha)\\sum_{j=1}^p |\\beta_j| \\right) \\right\\},\n\\end{aligned}\n$$\n\n\n\n\n\n\n\n## Lidando com outliers\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345); x = rnorm(20); e = rt(20, 1); y = 2*x + e; plot(y~x); lm(y~0+x)\n```\n\n::: {.cell-output-display}\n![](regularização_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ 0 + x)\n\nCoefficients:\n    x  \n6.136  \n```\n\n\n:::\n:::\n\n\n\n",
    "supporting": [
      "regularização_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}