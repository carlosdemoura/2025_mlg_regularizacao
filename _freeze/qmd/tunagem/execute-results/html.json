{
  "hash": "cd43687820576a2fa81d607d70573fda",
  "result": {
    "engine": "knitr",
    "markdown": "# Tunagem\n\n[![Alice in Wonderland](../img/alice-shrinking.gif){width=50%}](https://tenor.com/view/alice-shrinking-drink-me-alice-in-wonderland-wonderland-gif-19278503)\n\n## Validação cruzada\n\nComo já vimos anteriormente, a seleção dos parâmetros de regularização é um passo importante na regularização. Não deve ser surpresa para o leitor mais atento que isso será feito por validação cruzada.\nUma vez que já apresentamos os principais passos do IRLS-EN, doravante usaremos o pacote `glmnet` (@glmnet), que foi desenvolvido - dentre outros - pelas mesmas mentes por trás da regressão lasso e elastic net, como Tibshirani e Hastie. O pacote conta com diversas rotinas para a implementação da regularização em muitas famílias comuns de MLG (normal, binomial, poisson, etc.).\n\n\n\n\n\n\n::: {.callout-tip}\n# Outros pacotes\n\nOutros pacotes podem ser usados para ajustar regularizações em GLMs, cada um com suas particularidades. O `glmnet`, por exemplo, é um dos poucos pacotes que permitem a implementação em qualquer família de GLMs, mas o pacote não é compatível com a sintaxe de fórmulas do R. Uma série de alternativas ao `glmnet` são apresentadas em @en_glm.\n:::\n\nConsidere a penalidade do elastic net\n\n$$\nP_{\\alpha, \\lambda}(\\beta) = \n\\lambda \\left( \\frac{1 - \\alpha}{2}  \\sum_{j=1}^p \\beta_j^2 + \\alpha\\sum_{j=1}^p |\\beta_j| \\right).\n$$\n\nPara diferentes escolhas de $\\alpha \\in [0,1]$, serão gerados diferentes penalidades, todas potencialmente razoáveis. Como essas regras podem ser vistas como restrições do espaço paramétrico, a força dessa restrição será dada por $\\lambda$. \n\n\n\n\n\n\n\n\n\n\n### Selecionando $\\lambda$\n\nFixado um $\\alpha$, a seleção de $\\lambda$ feita pelo `glmnet` @en_glm\n\nLambda pode ser selecionado como aquele que dá um modelo com tantas covariáveis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-10\n```\n\n\n:::\n\n```{.r .cell-code}\nx = cbind(1, 1:10)\ny = 1:10+1\nfit = glmnet(x, y)\nplot(fit)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in plotCoef(x$beta, lambda = x$lambda, df = x$df, dev = x$dev.ratio, :\n1 or less nonzero coefficients; glmnet plot is not meaningful\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](tunagem_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncvfit = cv.glmnet(x, y)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per\nfold\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(cvfit)\n```\n\n::: {.cell-output-display}\n![](tunagem_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nO EQM é calculado usado o K-fold cross-validation - mais sobre isso pode ser visto no capítulo 10 de @tmwr. Grosso modo, os dados são divididos em $K$ subconjuntos de igual tamanho (default $K = 10$) e para cada $\\lambda$ no caminho de lambdas e $k' \\in 1:K$ faça:\n\n1. Ajuste o modelo com os $K-1$ outros subconjuntos;\n2. Preveja o os valores do subconjunto $k'$ a partir dos coeficientes obtidos;\n3. Calcule o EQM nesse subconkunto que ficou de fora.\n\nO EQM de $\\lambda'$ será a média do EQM dos $K$ subconjuntos.\n\nAs linhas verticais pontilhadas trazem valores especiais de $\\lambda$. `lambda.min` é o valor de $\\lambda$ que dá o modelo com menor EQM. Com isso, o programa calcula `lambda.1se`, o $\\lambda$ que está um desvio padrão acima de `lambda.min`. Isso permite que o modelo cresça ao máximo as estimativas dos betas sem crescer muito o MSE.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncvfit$lambda.min\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.08372852\n```\n\n\n:::\n\n```{.r .cell-code}\ncvfit$lambda.1se\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09189195\n```\n\n\n:::\n:::\n\n\nA predição de novos valores é feita por meio da função `predict`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(cvfit, newx = cbind(1, 12), s = \"lambda.min\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     lambda.min\n[1,]   12.81052\n```\n\n\n:::\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\nn = 200\nbloco = 10\n\nX1 = matrix(rnorm(n*bloco*2), ncol = bloco*2)\nbeta1 = c( runif(bloco,-0.7,0.7), rep(0,bloco) )\n\nsigma =\n  matrix(.9, nrow = bloco, ncol = bloco) |>\n  `diag<-`(1)\nX2 = mvtnorm::rmvnorm(n, sigma = sigma)\nbeta2 = c(0.6, rep(0,bloco-1))\n\nX = cbind(1,X1,X2)\nbeta = c(0.1,beta1,beta2)\neta = exp(X %*% beta)\nY = rpois(n, eta)\n```\n:::\n\n\n::: {.callout-tip}\n# sparse matrices\n\nsparse matrix format\n:::\n\n\n### Selecionando $\\alpha$\n",
    "supporting": [
      "tunagem_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}