{
  "hash": "5d561d95183e9235c3f6452c59cb2515",
  "result": {
    "engine": "knitr",
    "markdown": "# Tunagem\n\n[![Alice in Wonderland](../img/alice-shrinking.gif){width=50%}](https://tenor.com/view/alice-shrinking-drink-me-alice-in-wonderland-wonderland-gif-19278503)\n\n## Validação cruzada\n\nComo já vimos anteriormente, a seleção dos parâmetros de regularização é um passo importante na regularização. Não deve ser surpresa para o leitor mais atento que isso será feito por validação cruzada.\nUma vez que já apresentamos os principais passos do IRLS-EN, doravante usaremos o pacote `glmnet` (@glmnet), que foi desenvolvido - dentre outros - pelas mesmas mentes por trás da regressão lasso e elastic net, como Tibshirani e Hastie. O pacote conta com diversas rotinas para a implementação da regularização em muitas famílias comuns de MLG (normal, binomial, poisson, etc.).\n\n\n::: {.callout-tip}\n# Outros pacotes\n\nOutros pacotes podem ser usados para ajustar regularizações em GLMs, cada um com suas particularidades. O `glmnet`, por exemplo, é um dos poucos pacotes que permitem a implementação em qualquer família de GLMs, mas o pacote não é compatível com a sintaxe de fórmulas do R. Uma série de alternativas ao `glmnet` são apresentadas em @en_glm.\n:::\n\nConsidere a penalidade do elastic net\n\n$$\nP_{\\alpha, \\lambda}(\\beta) = \n\\lambda \\left( \\frac{1 - \\alpha}{2}  \\sum_{j=1}^p \\beta_j^2 + \\alpha\\sum_{j=1}^p |\\beta_j| \\right).\n$$\n\nPara diferentes escolhas de $\\alpha \\in [0,1]$, serão gerados diferentes penalidades, todas potencialmente razoáveis. Como essas regras podem ser vistas como restrições do espaço paramétrico, a força dessa restrição será dada por $\\lambda$. Abaixo temos uma aplicação `shiny` que mostra a importância de uma boa escolha de parâmetros.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Aqui tratamos t = 1 / lambda, embora isso não seja necessariamente verdade,\n# é claro que essas duas grandezas são inversamente proporcionais.\nlibrary(shiny)\n\nui = fluidPage(\n  titlePanel(\"Região do Elastic Net em R²\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"alpha\",  \"α\", min = 0,   max = 1, value = 0.5, step = 0.01),\n      sliderInput(\"lambda\", \"λ\", min = 0.2, max = 1, value = .9,  step = 0.1)\n    ),\n    mainPanel(\n      plotOutput(\"plot\", height = \"600px\")\n    )\n  )\n)\n\nserver = function(input, output, session) {\n  output$plot = renderPlot({\n    alpha = input$alpha\n    tval  = 1/input$lambda\n    \n    res = 300\n    R   = 10\n    \n    b = seq(-R, R, length.out = res)\n    grid = expand.grid(b1 = b, b2 = b)\n    grid$val = (1 - alpha) * (abs(grid$b1) + abs(grid$b2)) / 2 + alpha * (grid$b1^2 + grid$b2^2)\n    \n    Z = matrix(grid$val, nrow = res, ncol = res, byrow = FALSE)\n    \n    image(x = b, y = b, z = Z <= tval,\n          xlim = c(-10, 10), ylim = c(-10, 10),\n          xlab = expression(beta[1]),\n          ylab = expression(beta[2]),\n          axes = FALSE)\n    \n    axis(1); axis(2); box()\n    abline(h = 0, v = 0, lty = 2)\n    contour(x = b, y = b, z = Z, levels = tval, add = TRUE)\n  })\n}\n\nshinyApp(ui, server)\n```\n\n::: {.cell-output-display}\n`<div style=\"width: 100% ; height: 400px ; text-align: center; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box;\" class=\"muted well\">Shiny applications not supported in static R Markdown documents</div>`{=html}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Tunando parâmetros com `glmnet`\n\nFixado um $\\alpha$, a seleção de $\\lambda$ feita pelo `glmnet` segue as ideias de @en_glm.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(12345)\n\nn = 250\nbloco = 5\n\nX1 = matrix(rnorm(n*bloco*2), ncol = bloco*2)\nbeta1 = c( runif(bloco,-0.7,0.7), rep(0,bloco) )\n\nsigma =\n  matrix(.9, nrow = bloco, ncol = bloco) |>\n  `diag<-`(1)\nX2 = mvtnorm::rmvnorm(n, sigma = sigma)\nbeta2 = c(0.6, rep(0,bloco-1))\n\nX = cbind(1,X1,X2)\nbeta = c(0.1,beta1,beta2)\neta = exp(X %*% beta)\n\nY = rpois(n, eta)\n```\n:::\n\n\nÉ possível aplicar ideias de validação cruzada (que serão apresentadas adiante para a escolha de $\\lambda$) para escolher o melhor $\\alpha$, mas em geral esse valor pode ser fixado pelo pesquisador de acordo com o tipo de problema. Vide os casos extremos abaixo de regressão lasso e ridge.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\nfit = glmnet(X, Y, alpha = 1, family = \"poisson\")\nplot(fit)\n```\n\n::: {.cell-output-display}\n![](tunagem_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = glmnet(X, Y, alpha = 0, family = \"poisson\")\nplot(fit)\n```\n\n::: {.cell-output-display}\n![](tunagem_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nLambda pode ser selecionado como aquele que dá um modelo com um número pré-selecionado de covariáveis. Outra possibilidade (mais comum) é selecionar o $\\lambda$ que minimiza o erro quadrático médio.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncvfit = cv.glmnet(X[1:200,], Y[1:200], family = \"poisson\")\nplot(cvfit)\n```\n\n::: {.cell-output-display}\n![](tunagem_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nO EQM é calculado usado o K-fold cross-validation - mais sobre isso pode ser visto no capítulo 10 de @tmwr. Grosso modo, os dados são divididos em $K$ subconjuntos de igual tamanho (default $K = 10$) e para cada $\\lambda$ no caminho de lambdas e $k' \\in 1:K$ faça:\n\n1. Ajuste o modelo com os $K-1$ outros subconjuntos;\n2. Preveja o os valores do subconjunto $k'$ a partir dos coeficientes obtidos;\n3. Calcule o EQM nesse subconkunto que ficou de fora.\n\nO EQM de $\\lambda'$ será a média do EQM dos $K$ subconjuntos.\n\nAs linhas verticais pontilhadas trazem valores especiais de $\\lambda$. `lambda.min` é o valor de $\\lambda$ que dá o modelo com menor EQM. Com isso, o programa calcula `lambda.1se`, o $\\lambda$ que está um desvio padrão acima de `lambda.min`. Isso permite que o modelo cresça ao máximo as estimativas dos betas sem crescer muito o MSE.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncvfit$lambda.min\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04464306\n```\n\n\n:::\n\n```{.r .cell-code}\ncvfit$lambda.1se\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1642141\n```\n\n\n:::\n:::\n\n\nA predição de novos valores é feita por meio da função `predict`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX_new = predict(cvfit, newx = X[201:250,], s = \"lambda.1se\", type = \"response\")\nhead(cbind(X_new, Y[201:250]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     lambda.1se   \n[1,] 12.4055827 20\n[2,]  1.3814449  2\n[3,]  1.1193229  2\n[4,]  1.5140474  1\n[5,]  0.3706630  0\n[6,]  0.6943404  2\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(cbind(X_new, Y[201:250]))\n```\n\n::: {.cell-output-display}\n![](tunagem_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-tip}\n# Sparse matrices\n\nNo R, um objeto do tipo `sparse matrix` aparece como saída do `glmnet` e é, em geral, uma matriz da classe `dgCMatrix` do pacote `Matrix`. Nesse formato, apenas os coeficientes não nulos são armazenados, o que otimiza operações de produto matricial e norma das colunas.\n:::\n",
    "supporting": [
      "tunagem_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}