# Estimação

::: {style="text-align: right"}
"Se isto for possível,<br>
Pois, me contem,<br>
Como escrever de novo,<br>
Um jornal de ontem"<br>
Tom Zé
:::

## Regularização como um limiar suave

Até agora vimos exemplos de como a regularização pode ser vista como uma função de perda na estimação de mínimos quadrados. Essa ideia pode ser estendida para a estimação via máxima verossimilhança (como veremos adiante).
Antes disso, mostraremos um exemplo (retirado de @islr) de como a regressão ridge e lasso diminuem os coeficiente na prática.

Considere o caso em que $n = p$, $X = I_n$, e não temos intercepto. Assim $\hat\beta_{OLS} = Y$. Nesse caso, é possível mostrar que 

$$
\hat\beta_j^{ridge} = \frac{Y_j}{1 + \lambda}
$$
e

$$
\hat\beta_j^{lasso} = 
\begin{cases}
Y_j - \lambda/2, & \text{se } Y_j > \lambda/2;\\
Y_j + \lambda/2, & \text{se } Y_j < - \lambda/2;\\
0, & \text{se } -\lambda/2 \leq Y_j \leq \lambda/2.
\end{cases}
$$

Com esse exemplo simples, podemos entender porque a regressão ridge nunca iguala os coeficientes a zero, coisa que a lasso pode fazer. Essa característica de zerar coeficientes a depender do valor que os dados assumem é conhecido como _soft-tresholding_ (limiar suave). De acordo com @en, a elastic net mistura tanto a shinkage da regressão ridge quanto o soft tresholding da lasso.


[![Gráfico das condições acima.](../img/islr_soft-tresholding.png){width=90%}](https://www.statlearning.com/)









## Regularização como uma restrição do espaço paramétrico

É possível entender cada um dos processos de regularização descritos anteriormente como uma restrição do espaço paramétrico dos coeficientes de regressão. Se não fazer seleção é considerer que $\beta \in \mathbb R^d$, é possível mostrar que - escolhidos os parâmetros de shrinkage - então minimar a soma de quadrados do resíduo penalizada é a mesma coisa que minimizar a soma de quadrado da regressão num espaço paramétrico menor (que depende dos parâmetros de shrinkage escolhidos).

Assim, (na regressão linear normal) vale que:


$$
\hat\beta_{ridge} = 
\arg\min_{\beta} \{SQRes\}
\quad \text{com } \beta \text{ tal que} \quad
\sum_{j=1}^p \beta_j^2 \le t_{ridge},
$$

$$
\hat\beta_{lasso} = 
\arg\min_{\beta} \{SQRes\}
\quad \text{com } \beta \text{ tal que} \quad
\sum_{j=1}^p |\beta_j| \le t_{lasso},
$$

$$
\hat\beta_{elastic\ net} = 
\arg\min_{\beta} \{SQRes\}
\quad \text{com } \beta \text{ tal que} \quad
(1-\alpha)\sum_{j=1}^p |\beta_j| + \alpha \sum_{j=1}^p \beta_j^2 \le t_{elastic\ net}.
$$

```{r}
#| code-fold:  TRUE
#| fig-width:  10
#| fig-height: 4
desenhar_espaço_paramétrico = function(alpha, t, título = NULL) {
  stopifnot(
    "alpha deve estar entre 0 e 1" = all(alpha >= 0, alpha <= 1),
    "t deve ser positivo"          = t > 0
  )
  
  F = function(x, y) alpha*(x^2 + y^2 - t) + (1-alpha)*(abs(x) + abs(y) - t)
  
  x = seq(-2, 2, length = 400)
  y = seq(-2, 2, length = 400)
  g = outer(x, y, F)
  
  contour(x, y, g,
          levels = 0,
          drawlabels = FALSE,
          lwd = 2, asp = 1,
          main = título,
          cex.main = 2
          )
}

par(mfrow = c(1,3))
desenhar_espaço_paramétrico(1,   1, "ridge")
desenhar_espaço_paramétrico(0,   1, "lasso")
desenhar_espaço_paramétrico(1/2, 1, "elastic net")
```


## Regularização nos MLGs

O modelo linear normal pode ser especificado diretamente por meio de seus resíduos, mas - em geral - isso não é possível em todos os MLGs. Aqui, apresentaremos a técnica descrita em @en_glm, que generaliza a regularização elastic net para os MLGs.
Grosso modo, em vez de minimizar a SQRes penalizada (que nem sempre está bem definida), vamos minimizar o inverso aditivo da log-verossimilhança penalizada (que sempre está bem definida num MLG).

$$
\hat{\beta}
= \arg\min_{\beta}
\left\{
  -\frac{1}{n}\sum_{i=1}^n \text{loglik}\left(y_i, X \beta \right) + P_{\alpha, \lambda}(\beta)
\right\},
$$

onde $P_{\alpha, \lambda}(\beta)$ é a função de penalização elastic net, ou seja
$$
P_{\alpha, \lambda}(\beta) = 
\lambda \left( \frac{1 - \alpha}{2}  \sum_{j=1}^p \beta_j^2 + \alpha\sum_{j=1}^p |\beta_j| \right).
$$

Com isso, temos um algoritmo de mínimos quadrados ponderados (IRLS) dado a seguir.

::: {.callout-tip}
## IRLS elastic net (simplificado)

Selecione um valor de $\alpha \in [0,1]$ e valor de $\lambda \in \mathbb R$.

Inicialize o algritmo de maneira razoável - com $\hat\beta^{(0)} = 0$, ou $\eta^{(0)} = Y$, ou qualquer coisa que faça sentido. Doravante para $t = 1, 2, \dots$ (até que se atinja convergência) faça:

1. Compute $\eta_i^{(t)} = X\hat{\beta}^{(t)}$ e também $\ \mu_i^{(t)} = g^{-1}(\eta_i^{(t)})$, para $i \in 1:n$;

2. Compute $z_i^{(t)} = \eta_i^{(t)} + (y_i - \mu_i^{(t)}) \left( \frac{d\mu_i^{(t)}}{d\eta_i^{(t)}} \right)^{-1}$
e também
$w_i^{(t)} = \frac{1}{V(\mu_i^{(t)})} \left( \frac{d\mu_i^{(t)}}{d\eta_i^{(t)}} \right)^2$,
para $i \in 1:n$;

3. Resolva

$$
\hat{\beta}^{(t+1)}
=
\arg\min_{\beta}
\left\{
\frac{1}{2n}
\sum_{i=1}^n w_i^{(t)}
\bigl( z_i^{(t)} - X_i \beta \bigr)^2
+ P_{\alpha, \lambda}(\beta)
\right\}.
$$

O algoritmo de @en_glm (que aqui simplificamos) faz uma validação cruzada para estimar o melhor $\lambda$. Isso é feito considerando um "caminho de lambdas" (_regularization path_) e otimizando o algoritmo acima com warm-starts, o que será tratado na próxima seção.
:::









## Estudo com dados simulados

```{r}
#| message: false
#| warning: false

set.seed(12345)

n = 200
bloco = 10

X1 = matrix(rnorm(n*bloco*2), ncol = bloco*2)
beta1 = c( runif(bloco,-0.7,0.7), rep(0,bloco) )

sigma =
  matrix(.9, nrow = bloco, ncol = bloco) |>
  `diag<-`(1)
X2 = mvtnorm::rmvnorm(n, sigma = sigma)  # covariáveis com alta dependência, mas só a primeira é significativa
beta2 = c(0.6, rep(0,bloco-1))

X = cbind(1,X1,X2)
beta = c(0.1,beta1,beta2)
eta = exp(X %*% beta)

index_outliers = sample(1:n, 5)
eta[index_outliers] = eta[index_outliers] * 3

Y = rpois(n, eta)

df =
  cbind(Y, X[,-1]) |>
  tibble::as_tibble() |>
  `colnames<-`(c("Y", paste0("X", 1:(3*bloco))))
```

::: {.callout-note collapse="true"}
## Resultado do `stats::glm`
```{r}
mod = glm(Y~., data = df, family = poisson())
summary(mod)
```
:::

::: {.callout-tip}
Em modelos com função de ligação canônica, o algoritmo acima simplica. O segundo passo da iteração fica:
Compute $z_i^{(t)} = \eta_i^{(t)} + \frac{(y_i - \mu_i^{(t)})}{V(\mu_i^{})}$
e também
$w_i^{(t)} = V(\mu_i^{(t)})$,
para $i \in 1:n$.
:::

Sabemos que no caso da Poisson, a função de ligação canônica é o log e a função de variância é $\mu_i$. Assim, nosso algoritmo IRLS-EN fica da seguinte forma.


::: {.callout-warning collapse="true"}

## Uma pequena trapaça

Como dito acima, a estimação elastic net depende de uma boa escolha dos parâmetros de regularização, especialmente $\lambda$. Como isso é feita via validação cruzada - assunto da próxima seção - demos uma trapaceada didática ao escolher o melhor $\lambda$ usando o `glmnet`, pacote que falaremos mais à frente.

```{r}
cv = glmnet::cv.glmnet(
  X[,-1], Y,
  family = "poisson",
  intercept = TRUE,
  alpha = 0.9,
  nfolds = 10
)

cv$lambda.1se
```
:::


```{r}
sqres_en = function(beta, w, z) {
  sum(  w * (z - X %*% beta)^2  ) / (2 * nrow(X)) +
    lambda * (  alpha * sum(abs(beta[-1])) + ((1 - alpha)/2) * crossprod(beta[-1])  )
}

iter = 20
alpha = 0.9; lambda = cv$lambda.1se
beta_hat = matrix(1, nrow = iter, ncol = 3*bloco+1)

for (t in 2:iter) {
  eta = X %*% beta_hat[t-1,]
  mu = exp(eta)
  z = eta + (Y - mu)/mu
  beta_hat[t,] = 
    optim(
      par    = rep(0,ncol(X)),
      fn     = sqres_en,
      w      = mu,
      z      = z,
      method = "BFGS"
    )$par
}
```

```{r}
#| code-fold: true
estimativas = 
  cbind(  0:(3*bloco),  beta_hat[iter,],  beta,  unname(mod$coefficients)  ) |>
  tibble::as_tibble() |>
  `colnames<-`(c("index", "estimativa en", "real", "estimativa glm"))

estimativas[0:10+1,] |>
  knitr::kable(digits = 3,
    caption = "Estimativas vs. beta real\n(originalmente significativos)")
```

```{r}
#| code-fold: true
estimativas[11:20+1,] |>
  knitr::kable(digits = 3,
    caption = "Estimativas vs. real\n(originalmente NÃO significativos)")
```

```{r}
#| code-fold: true
estimativas[21:30+1,] |>
  knitr::kable(digits = 3,
    caption = "Estimativas vs. real\n(originalmente só o 1º significativo e com covariáveis correlacionadas)")
```

::: {.callout-important collapse="true"}

## Lidando com outliers

```{r}
#| code-fold: true

mod_sem_outliers = glm(Y~., data = df[-index_outliers,], family = poisson())
# summary(mod_sem_outliers)
col = 11

x_seq = seq(min(X[,col+1]), max(X[,col+1]), length.out = 200)
y_hat_sem_outlier = exp(summary(mod_sem_outliers)$coef[1] + summary(mod_sem_outliers)$coef[col+1] * x_seq)
y_hat_com_outlier = exp(summary(mod)$coef[1] + summary(mod)$coef[col+1] * x_seq)

par(mfrow = c(1,1))
plot(Y~X[,col+1],
     #ylim = c(0,10),
     main = "predição com outliers vs. sem outliers",
     xlab = paste("X", col + 1),
     ylab = "Y"
     )
points(
  X[index_outliers,col+1],
  Y[index_outliers],
  col = "red",
  pch = 4,
  cex = 1.5,
  lwd = 2
)
lines(x_seq, y_hat_sem_outlier, col = "blue", lwd = 2)
lines(x_seq, y_hat_com_outlier, col = "red",  lwd = 2)
```


Pra ver depois.

```{r}
set.seed(12345)
x = rnorm(20)
e = rt(20, 1)
y = 2*x + e
plot(y~x)
#lm(y~0+x) |> summary()
```

:::


Comentários !!!

### Bootstrap

Como um exemplo computaciona, optamos por fazer um bootstrap paramétrico a fim de entender o comportamento dos estimadores tradicional (IRLS via `glm`) e regularizado (IRLS-EN adaptado de @en_glm).
Opatamos por manter a mesma configuração de blocos e parâmetros descrita anteriormente. Poderiamos ter considerando a aleatoriedade das covariáveis no processo (dentro das especificações anteriores) - o que pode ser feito simplesmente passando o sorteio dos xizes para dentro do loop -, mas vimos que isso produziu um resultado similar ao com $X$ fixado (só um pouco menos dramático).

Seria interessante também fazer um estudo Monte Carlo, sorteando diferentes betas dentro da mesma estrutura e diferentes xizes para cada iteração Monte Carlo. Convidamos o leitor a fazer isso em seu tempo livre.

Por fim, vale ressaltar que o bootstrap se faz necessário pois não sabemos qual é a distribuição analítica nem dos betas estimados via IRLS nem dos estimados via IRLS-EN.

```{r}
set.seed(12345)

iter = 400
n = 200
bloco = 10

beta1 = c( runif(bloco,-0.7,0.7), rep(0,bloco) )
beta2 = c(0.6, rep(0,bloco-1))
beta = c(0.1,beta1,beta2)

estimativas = array(dim = c(2,length(beta),iter), dimnames = list(c("glm", "en")))

sigma =
  matrix(.9, nrow = bloco, ncol = bloco) |>
  `diag<-`(1)
X1 = matrix(rnorm(n*bloco*2), ncol = bloco*2)
X2 = mvtnorm::rmvnorm(n, sigma = sigma)
X = cbind(1,X1,X2)
eta = exp(X %*% beta)

for (i in 1:iter) {
  Y = rpois(n, eta)
  
  coef_en =
    glmnet::cv.glmnet(
      X[,-1], Y,
      family = "poisson",
      intercept = TRUE,
      alpha = 0.9,
      nfolds = 10
    ) |>
    coef(s = "lambda.1se") |>
    as.matrix() |>
    `dimnames<-`(NULL) |>
    c()
  
  coef_glm = 
    glm(Y~X[,-1], family = poisson())$coef |>
    unname()
  
  estimativas["en",,i]  = coef_en
  estimativas["glm",,i] = coef_glm
}
```

```{r}
#| code-fold: true
#| fig-cap: "Variâncias dos betas do grupo 1 (significativos)"

plot_hist_block = function(idxs, xlims) {
  par(mfrow = c(2, 3))
  for (metodo in c("glm", "en")) {
    for (i in seq_along(idxs)) {
      beta_idx = idxs[i]
      beta_number = beta_idx - 1   # como pedido
      
      hist(
        estimativas[metodo, beta_idx, ],
        main = paste0(metodo, " - β", beta_number),
        xlab = "", ylab = "",
        xlim = xlims[[i]]
      )
    }
    
  }
}

plot_hist_block(
  idxs = c(1, 4, 11),
  xlims = list(c(-0.2, 0.5), c(0.1, 0.5), c(0.5, 0.9))
)
```

```{r}
#| code-fold: true
#| fig-cap: "Variâncias dos betas do grupo 2 (não significativos)"

plot_hist_block(
  idxs = c(12, 14, 19),
  xlims = list(c(-0.2, 0.2), c(-0.2, 0.2), c(-0.2, 0.2))
)
```

```{r}
#| code-fold: true
#| fig-cap: "Variâncias dos betas do grupo 3 (covariáveis autocorrelacionadas)"

plot_hist_block(
  idxs = c(22, 23, 27),
  xlims = list(c(0, 1.2), c(-0.5, 0.5), c(-0.5, 0.5))
)
```

Os histogramas acima vão ao encontro do que a teoria diz. No primeiro bloco de covariáveis (intercepto mais covariáveis de 1 a 10) os betas reais são significativos e ambos os modelos captam isso - com um grau similar de variabilidade.

No segundo bloco de covariáveis, ambos os modelos captam a insignificância real dos betas, mas as estimativas do elastic net têm bem menos variabilidade que aquelas do IRLS tradicional.

No terceiro bloco de covariáveis - alta correlação das covariáveis e apenas o $\beta_{21}$ realmente diferente de zero -, ambos os modelos captam qual é a covariável significativa, mas - pelo menos nas variáveis não siginificativas - as estimativas do elastic net são menos dispersas que as do IRLS tradicional.


```{r}
#| warning: false
#| message: false
#| fig-width:  10
#| fig-height: 5
#| code-fold: true

variancias =
  apply(estimativas, c(1,2), var) |>
  t() |>
  tibble::as_tibble() |>
  cbind(c("intercepto", rep(c(1,2,3), each=10))) |>
  tibble::as_tibble() |>
  `colnames<-`(c("glm", "en", "grupo"))

library(ggplot2)

variancias_long =
  variancias[-1,] |>
  tidyr::pivot_longer(
    cols = c("glm", "en"),
    names_to = "metodo",
    values_to = "variancia")

ggplot(variancias_long,
       aes(x = grupo, y = variancia, fill = metodo)) +
  geom_boxplot(alpha = 0.7) +
  labs(x = "Grupo", y = "Variância",
       title = "Variância dos Betas por Grupo e Método") +
  theme_minimal(base_size = 14)
```

Os boxplots acima refletem o que já dissemos. As variabilidade das estimativas EN e tradicional são muito diferentes nos grupos 2 e 3 e razoavelmente similares no grupo 1.
Nesse plot foi ignorado o intercepto.

```{r}
#| warning: false
#| message: false
#| fig-width:  10
#| fig-height: 5
#| code-fold: true

vies2 =
  sweep(estimativas, 2, beta, FUN = "-")^2 |>
  apply(c(1,2), mean) |>
  t() |>
  tibble::as_tibble() |>
  cbind(c("intercepto", rep(c(1,2,3), each=10))) |>
  tibble::as_tibble() |>
  `colnames<-`(c("glm", "en", "grupo"))

vies2_long =
  vies2[-1,] |>
  tidyr::pivot_longer(
    cols = c("glm", "en"),
    names_to = "metodo",
    values_to = "vies2")

ggplot(vies2_long,
       aes(x = grupo, y = vies2, fill = metodo)) +
  geom_boxplot(alpha = 0.7) +
  labs(x = "Grupo", y = "Viés²",
       title = "Viés² dos Betas por Grupo e Método") +
  theme_minimal(base_size = 14)
```

O gráfico de boxplots acima mostra como os dois modelos se comportam em termos de viés ao quadrado. É possível notar que o viés da estatística EN é mais enviesada que a estatística tradicional no grupo com coeficientes diferentes de zero.

```{r}
#| warning: false
#| message: false
#| fig-width:  10
#| fig-height: 5
#| code-fold: true

norma =
  apply(estimativas, c(1,2), crossprod) |>
  t() |>
  tibble::as_tibble() |>
  cbind(c("intercepto", rep(c(1,2,3), each=10))) |>
  tibble::as_tibble() |>
  `colnames<-`(c("glm", "en", "grupo"))

norma_long =
  norma[-1,] |>
  tidyr::pivot_longer(
    cols = c("glm", "en"),
    names_to = "metodo",
    values_to = "norma")

ggplot(norma_long,
       aes(x = grupo, y = norma, fill = metodo)) +
  geom_boxplot(alpha = 0.7) +
  labs(x = "Grupo", y = "Norma L2",
       title = "Norma dos Betas por Grupo e Método") +
  theme_minimal(base_size = 14)
```