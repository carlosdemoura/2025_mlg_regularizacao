# Estimação

::: {style="text-align: right"}
"Se isto for possível,<br>
Pois, me contem,<br>
Como escrever de novo,<br>
Um jornal de ontem"<br>
Tom Zé
:::

## Regularização como um limiar suave

Até agora vimos exemplos de como a regularização pode ser vista como uma função de perda na estimação de mínimos quadrados. Essa ideia pode ser estendida para a estimação via máxima verossimilhança (como veremos adiante).
Antes disso, mostraremos um exemplo (retirado de @islr) de como a regressão ridge e lasso diminuem os coeficiente na prática.

Considere o caso em que $n = p$, $X = I_n$, e não temos intercepto. Assim $\hat\beta_{OLS} = Y$. Nesse caso, é possível mostrar que 

$$
\hat\beta_j^{ridge} = \frac{Y_j}{1 + \lambda}
$$
e

$$
\hat\beta_j^{lasso} = 
\begin{cases}
Y_j - \lambda/2, & \text{se } Y_j > \lambda/2;\\
Y_j + \lambda/2, & \text{se } Y_j < - \lambda/2;\\
0, & \text{se } -\lambda/2 \leq Y_j \leq \lambda/2.
\end{cases}
$$

Com esse exemplo simples, podemos entender porque a regressão ridge nunca iguala os coeficientes a zero, coisa que a lasso pode fazer. Essa característica de zerar coeficientes a depender do valor que os dados assumem é conhecido como _soft-tresholding_ (limiar suave). De acordo com @en, a elastic net mistura tanto a shinkage da regressão ridge quanto o soft tresholding da lasso.


[![Gráfico das condições acima.](../img/islr_soft-tresholding.png){width=90%}](https://www.statlearning.com/)









## Regularização como uma restrição do espaço paramétrico

É possível entender cada um dos processos de regularização descritos anteriormente como uma restrição do espaço paramétrico dos coeficientes de regressão. Se não fazer seleção é considerer que $\beta \in \mathbb R^d$, é possível mostrar que - escolhidos os parâmetros de shrinkage - então minimar a soma de quadrados do resíduo penalizada é a mesma coisa que minimizar a soma de quadrado da regressão num espaço paramétrico menor (que depende dos parâmetros de shrinkage escolhidos).

Assim, (na regressão linear normal) vale que:


$$
\hat\beta_{ridge} = 
\arg\min_{\beta} \{SQRes\}
\quad \text{com } \beta \text{ tal que} \quad
\sum_{j=1}^p \beta_j^2 \le t_{ridge},
$$

$$
\hat\beta_{lasso} = 
\arg\min_{\beta} \{SQRes\}
\quad \text{com } \beta \text{ tal que} \quad
\sum_{j=1}^p |\beta_j| \le t_{lasso},
$$

$$
\hat\beta_{elastic\ net} = 
\arg\min_{\beta} \{SQRes\}
\quad \text{com } \beta \text{ tal que} \quad
(1-\alpha)\sum_{j=1}^p |\beta_j| + \alpha \sum_{j=1}^p \beta_j^2 \le t_{elastic\ net}.
$$

```{r}
#| code-fold:  TRUE
#| fig-width:  10
#| fig-height: 4
desenhar_espaço_paramétrico = function(alpha, t, título = NULL) {
  stopifnot(
    "alpha deve estar entre 0 e 1" = all(alpha >= 0, alpha <= 1),
    "t deve ser positivo"          = t > 0
  )
  
  F = function(x, y) alpha*(x^2 + y^2 - t) + (1-alpha)*(abs(x) + abs(y) - t)
  
  x = seq(-2, 2, length = 400)
  y = seq(-2, 2, length = 400)
  g = outer(x, y, F)
  
  contour(x, y, g,
          levels = 0,
          drawlabels = FALSE,
          lwd = 2, asp = 1,
          main = título,
          cex.main = 2
          )
}

par(mfrow = c(1,3))
desenhar_espaço_paramétrico(1,   1, "ridge")
desenhar_espaço_paramétrico(0,   1, "lasso")
desenhar_espaço_paramétrico(1/2, 1, "elastic net")
```


## Regularização nos MLGs

O modelo linear normal pode ser especificado diretamente por meio de seus resíduos, mas - em geral - isso não é possível em todos os MLGs. Aqui, apresentaremos a técnica descrita em @en_glm, que generaliza a regularização elastic net para os MLGs.
Grosso modo, em vez de minimizar a SQRes penalizada (que nem sempre está bem definida), vamos minimizar o inverso aditivo da log-verossimilhança penalizada (que sempre está bem definida num MLG).

$$
\hat{\beta}
= \arg\min_{\beta}
\left\{
  -\frac{1}{n}\sum_{i=1}^n \text{loglik}\left(y_i, X \beta \right) + P_{\alpha, \lambda}(\beta)
\right\},
$$

onde $P_{\alpha, \lambda}(\beta)$ é a função de penalização elastic net, ou seja
$$
P_{\alpha, \lambda}(\beta) = 
\lambda \left( \frac{1 - \alpha}{2}  \sum_{j=1}^p \beta_j^2 + \alpha\sum_{j=1}^p |\beta_j| \right).
$$

Com isso, temos um algoritmo de mínimos quadrados ponderados (IRLS) dado a seguir.

::: {.callout-tip}
## IRLS elastic net (simplificado)

Selecione um valor de $\alpha \in [0,1]$ e valor de $\lambda \in \mathbb R$.

Inicialize o algritmo de maneira razoável - com $\hat\beta^{(0)} = 0$, ou $\eta^{(0)} = Y$, ou qualquer coisa que faça sentido. Doravante para $t = 1, 2, \dots$ (até que se atinja convergência) faça:

1. Compute $\eta_i^{(t)} = X\hat{\beta}^{(t)}$ e também $\ \mu_i^{(t)} = g^{-1}(\eta_i^{(t)})$, para $i \in 1:n$;

2. Compute $z_i^{(t)} = \eta_i^{(t)} + (y_i - \mu_i^{(t)}) \left( \frac{d\mu_i^{(t)}}{d\eta_i^{(t)}} \right)^{-1}$
e também
$w_i^{(t)} = \frac{1}{V(\mu_i^{(t)})} \left( \frac{d\mu_i^{(t)}}{d\eta_i^{(t)}} \right)^2$,
para $i \in 1:n$;

3. Resolva

$$
\hat{\beta}^{(t+1)}
=
\arg\min_{\beta}
\left\{
\frac{1}{2n}
\sum_{i=1}^n w_i^{(t)}
\bigl( z_i^{(t)} - X_i \beta \bigr)^2
+ P_{\alpha, \lambda}(\beta)
\right\}.
$$

O algoritmo de @en_glm (que aqui simplificamos) faz uma validação cruzada para estimar o melhor $\lambda$. Isso é feito considerando um "caminho de lambdas" (_regularization path_) e otimizando o algoritmo acima com warm-starts, o que será tratado na próxima seção.
:::









## Estudo com dados simulados

```{r}
#| message: false
#| warning: false

set.seed(12345)

n = 200
bloco = 10

X1 = matrix(rnorm(n*bloco*2), ncol = bloco*2)
beta1 = c( runif(bloco,-0.7,0.7), rep(0,bloco) )

sigma =
  matrix(.9, nrow = bloco, ncol = bloco) |>
  `diag<-`(1)
X2 = mvtnorm::rmvnorm(n, sigma = sigma)  # covariáveis com alta dependência, mas só a primeira é significativa
beta2 = c(0.6, rep(0,bloco-1))

X = cbind(1,X1,X2)
beta = c(0.1,beta1,beta2)
eta = exp(X %*% beta)

index_outliers = sample(1:n, 5)
eta[index_outliers] = eta[index_outliers] * 3

Y = rpois(n, eta)

df =
  cbind(Y, X[,-1]) |>
  tibble::as_tibble() |>
  `colnames<-`(c("Y", paste0("X", 1:(3*bloco))))
```

::: {.callout-note collapse="true"}
## Resultado do `stats::glm`
```{r}
mod = glm(Y~., data = df, family = poisson())
summary(mod)
```
:::

::: {.callout-tip}
Em modelos com função de ligação canônica, o algoritmo acima simplica. O segundo passo da iteração fica:
Compute $z_i^{(t)} = \eta_i^{(t)} + \frac{(y_i - \mu_i^{(t)})}{V(\mu_i^{})}$
e também
$w_i^{(t)} = V(\mu_i^{(t)})$,
para $i \in 1:n$.
:::

Sabemos que no caso da Poisson, a função de ligação canônica é o log e a função de variância é $\mu_i$. Assim, nosso algoritmo IRLS-EN fica da seguinte forma.

::: {.callout-warning collapse="true"}

## Uma pequena trapaça

Como dito acima, a estimação elastic net depende de uma boa escolha dos parâmetros de regularização, especialmente $\lambda$. Como isso é feita via validação cruzada - assunto da próxima seção - demos uma trapaceada didática ao escolher o melhor $\lambda$ usando o `glmnet`, pacote que falaremos mais à frente.

```{r}
cv = glmnet::cv.glmnet(
  X[,-1], Y,
  family = "poisson",
  intercept = TRUE,
  alpha = 0.9,
  nfolds = 10
)

cv$lambda.1se
```
:::

```{r}
estimativa_en = function(Y, X, alpha, lambda, iter = 20, init = 1) {
  sqres_en = function(beta, w, z) {
    sum(  w * (z - X %*% beta)^2  ) / (2 * nrow(X)) +
      lambda * (  alpha * sum(abs(beta[-1])) + ((1 - alpha)/2) * crossprod(beta[-1])  )
  }
  
  beta_hat = matrix(nrow = iter, ncol = 3*bloco+1)
  beta_hat[1,] = init
  
  for (t in 2:iter) {
    eta = X %*% beta_hat[t-1,]
    mu = exp(eta)
    z = eta + (Y - mu)/mu
    beta_hat[t,] = 
      optim(
        par    = rep(0,ncol(X)),
        fn     = sqres_en,
        w      = mu,
        z      = z,
        method = "BFGS"
      )$par
  }
  
  beta_hat[t,]
}

beta_en = estimativa_en(Y, X, alpha = 0.9, lambda = cv$lambda.1se)
```


```{r}
#| code-fold: true
estimativas = 
  cbind(  0:(3*bloco),  beta_en,  beta,  unname(mod$coefficients)  ) |>
  tibble::as_tibble() |>
  `colnames<-`(c("index", "estimativa en", "real", "estimativa glm"))

estimativas[0:10+1,] |>
  knitr::kable(digits = 3,
    caption = "Estimativas vs. beta real\n(originalmente significativos)")
```

```{r}
#| code-fold: true
estimativas[11:20+1,] |>
  knitr::kable(digits = 3,
    caption = "Estimativas vs. real\n(originalmente NÃO significativos)")
```

```{r}
#| code-fold: true
estimativas[21:30+1,] |>
  knitr::kable(digits = 3,
    caption = "Estimativas vs. real\n(originalmente só o 1º significativo e com covariáveis correlacionadas)")
```

Nos primeiro grupo (betas reais não nulos e covariáveis independentes), tanto o `glm` quanto o EN recuperam bem tanto os sinais quanto as magnitudes dos betas reais e podemos ver bem a ocorrência da _shrinkage_. Já no segundo bloco (betas reais nulos e covariáveis independentes), a EN se destaca ao manter as estimativas próximas de zero, enquanto o `glm` frequentemente gera pequenos falsos positivos. Finalmente, no terceiro bloco (betas reais não nulos e covariáveis fortemente dependentes), oEN novamente aplica _shrinkage_ forte nos coeficientes irrelevantes, enquanto o `glm` tem maior dificuldade em lidar com a colinearidade.




### Lidando com outliers

Os dados acima foram gerados com uma certa poluição de outliers, como podemos ver nos gráficos abaixo.

```{r}
#| code-fold: true
#| fig-cap: "Predição do `glm` com outliers (linha vermelha) vs. sem outliers (linha azul). Outliers inseridos intencionalmente marcados com um x vermelho."
#| eval: false

mod_sem_outliers = glm(Y~., data = df[-index_outliers,], family = poisson())
# summary(mod_sem_outliers)
col = 11

x_seq = seq(min(X[,col+1]), max(X[,col+1]), length.out = 200)
y_hat_sem_outlier = exp(summary(mod_sem_outliers)$coef[1] + summary(mod_sem_outliers)$coef[col+1] * x_seq)
y_hat_com_outlier = exp(summary(mod)$coef[1] + summary(mod)$coef[col+1] * x_seq)

plot(Y~X[,col+1],
     #ylim = c(0,10),
     main = "",
     xlab = paste("X", col),
     ylab = "Y"
     )
points(
  X[index_outliers,col+1],
  Y[index_outliers],
  col = "red",
  pch = 4,
  cex = 1.5,
  lwd = 2
)
lines(x_seq, y_hat_sem_outlier, col = "blue", lwd = 2)
lines(x_seq, y_hat_com_outlier, col = "red",  lwd = 2)
```



```{r}
#| include: false

mod_sem_outliers = glm(Y~., data = df[-index_outliers,], family = poisson())
col = 11
x_seq = seq(min(X[,col+1]), max(X[,col+1]), length.out = 200)
y_hat_sem_outlier = exp(summary(mod_sem_outliers)$coef[1] + summary(mod_sem_outliers)$coef[col+1] * x_seq)
y_hat_com_outlier = exp(summary(mod)$coef[1] + summary(mod)$coef[col+1] * x_seq)

png("../img/tmp/pois1.png")
plot(Y~X[,col+1],
     main = "",
     xlab = paste("X", col),
     ylab = "Y"
     )
points(
  X[index_outliers,col+1],
  Y[index_outliers],
  col = "red",
  pch = 4,
  cex = 1.5,
  lwd = 2
)
lines(x_seq, y_hat_sem_outlier, col = "blue", lwd = 2)
lines(x_seq, y_hat_com_outlier, col = "red",  lwd = 2)
dev.off()

png("../img/tmp/pois2.png")
plot(Y~X[,col+1],
     ylim = c(0,10),
     main = "",
     xlab = paste("X", col),
     ylab = "Y"
     )
points(
  X[index_outliers,col+1],
  Y[index_outliers],
  col = "red",
  pch = 4,
  cex = 1.5,
  lwd = 2
)
lines(x_seq, y_hat_sem_outlier, col = "blue", lwd = 2)
lines(x_seq, y_hat_com_outlier, col = "red",  lwd = 2)
dev.off()
```

<img class="flip"
     src="../img/tmp/pois1.png"
     data-src1="../img/tmp/pois1.png"
     data-src2="../img/tmp/pois2.png"
     width="400" height="500"
     onclick="trocaImagem(this)">



Quando estimamos usando a função `glm`, a estimativa do coeficiente da covariável `r col+1` no modelo com os outliers é de `r round(summary(mod)$coef[col+1], 3)`, já no modelo sem outliers essa estimativa passa a ser de  `r round(summary(mod_sem_outliers)$coef[col+1], 3)`. Além disso, esse coeficiente (originalmente nulo) tornou-se siginificativo com a inclusão dos outliers. Já na estimação EN, a estimativa pontual é `r round(estimativas[col+1,]$"estimativa en", 3)`, o que é mais razoável que a estimativa do `glm` com outliers (o único que temos acesso). Embora isso não indique se a variável é significativa (segundo a métrica EN), isso produz um modelo mais parcimonioso que o IRLS tradicional

Podemos ver abaixo um exemplo mais comovente sobre o poder da regularização na contenção do efeito de outliers. 

```{r}
set.seed(12345)
x = cbind(1, rnorm(20))
e = rt(20, 1)
beta = c(5, 2)
y = x %*% beta + e
```

```{r}
#| fig-width:  5
#| fig-height: 4
#| fig-align: "center"
plot(y~x[,-1])
```

```{r}
lm(y~x[,-1]) |> summary()
```

```{r}
boot::boot(
  data = data.frame(y = y, x = x[,-1]),
  statistic = {\(data, indices) lm(y ~ x, data = data[indices, ])$coef},
  R = 200
  )
```

```{r}
#| fig-width:  10
#| fig-height: 4
#| code-fold: true
# Para entender melhor  o que está acontecendo no bootstrap

set.seed(12345)
B = 2e3
coef = matrix(nrow = B, ncol = 2)
data = data.frame(y = y, x = x[,-1])
fun = {\(indices) lm(y ~ x, data = data[indices, ])$coef}

for (b in 1:B) {
  coef[b, ] = fun(  sample(1:length(y), length(y), replace = T)  )
}

par(mfrow = c(1,2))
hist(coef[,1], main = "estimativas bootstrap - β0")
abline(v = beta[1], lwd = 3, col = 2)
hist(coef[,2], xlim = c(0,20), breaks = (-100):100,
     main = "estimativas bootstrap - β1")
abline(v = beta[2], lwd = 3, col = 2)
```

```{r}
sqres_lasso = function(beta, X, Y, lambda) {
  res = Y - X %*% beta
  drop(crossprod(res) + lambda * sum(abs(beta)))
}

optim(
  par    = rep(0,2),
  fn     = sqres_lasso,
  X      = x,
  Y      = y,
  lambda = 100,
  method = "BFGS"
)$par
```

```{r}
#| fig-width:  5
#| fig-height: 4
#| fig-align: "center"
#| code-fold: true
beta_lasso = 
  optim(
    par    = rep(0,2),
    fn     = sqres_lasso,
    X      = x,
    Y      = y,
    lambda = 100,
    method = "BFGS"
  )$par
beta_lm = lm(y~x[,-1])$coef |> unname()

x_seq = seq(-2, 2, length.out = 100)
y_hat_lm = beta_lm[1] + x_seq * beta_lm[2]
y_hat_lasso = beta_lasso[1] + x_seq * beta_lasso[2]

plot(y~x[,-1])
lines(x_seq, y_hat_lasso, col = "blue", lwd = 2)
lines(x_seq, y_hat_lm, col = "red", lwd = 2)
```

É possível notar que o EMQ é fortemente impactado pela presença do outlier, que afeta não só a estimação pontual dos parâmetros como sua significância no modelo.
Coisa parecida ocorre quando usamos o bootstrap para estimar os coeficientes de regressão.
Apesar disso, a regressão lasso produz estimativas razoáveis.








### Análise de resíduos - parte 1

É razoável que uma mudança na estimação dos coeficientes mude também a distribuição dos resíduos.

```{r}
beta_en  = estimativa_en(Y, X, alpha = 0.9, lambda = cv$lambda.1se)
beta_glm = mod$coef |> unname()

res_en  = ( Y - exp(X %*% beta_en) ) / sqrt(exp(X %*% beta_en))
res_glm = ( Y - exp(X %*% beta_glm) ) / sqrt(exp(X %*% beta_glm))
```

```{r}
#| eval: false
#| code-fold: true

par(mfrow = c(1,2))
plot(sort(res_glm))
plot(sort(res_en))
```

```{r}
#| include: false

png("../img/tmp/env11.png", width = 800, height = 400)
layout(matrix(c(1, 2), nrow = 1, ncol = 2))
plot(sort(res_glm))
plot(sort(res_en))
dev.off()

png("../img/tmp/env12.png", width = 800, height = 400)
layout(matrix(c(1, 2), nrow = 1, ncol = 2))
plot(sort(res_glm), ylim = c(-5,5))
plot(sort(res_en), ylim = c(-5,5))
dev.off()
```

<img class="flip"
     src="../img/tmp/env11.png"
     data-src1="../img/tmp/env11.png"
     data-src2="../img/tmp/env12.png"
     width="800" height="400"
     onclick="trocaImagem(this)">




```{r}
#| eval: false
set.seed(12345)
B = 1000

res_boot_glm = matrix(nrow = B, ncol = length(Y))
for (b in 1:B) {
  Yb = rpois(length(Y), exp(X %*% beta_glm))
  beta_b = glm(Yb ~ X[,-1], family = poisson())$coef |> unname()
  res_boot_glm[b,] = ( Yb - exp(X %*% beta_b) ) / sqrt(exp(X %*% beta_b))
}
envelope_glm =
  res_boot_glm |>
  apply(2,
        \(col) quantile(col, probs = c(0.05, 0.95))
    ) |>
  t()


res_boot_en = matrix(nrow = B, ncol = length(Y))
for (b in 1:B) {
  Yb = rpois(length(Y), exp(X %*% beta_en))
  beta_b = estimativa_en(Yb, X, alpha = 0.9, lambda = cv$lambda.1se, iter = 10, init = beta_en)
  #beta_b = estimativa_en(Yb, X, alpha = 0.9, lambda = cv$lambda.1se)
  res_boot_en[b,] = ( Yb - exp(X %*% beta_b) ) / sqrt(exp(X %*% beta_b))
}
envelope_en =
  res_boot_en |>
  apply(2,
        \(col) quantile(col, probs = c(0.05, 0.95), na.rm = T)
    ) |>
  t()

saveRDS(list("glm" = envelope_glm, "en" = envelope_en), "../data/envelopes.rds")
```

```{r}
#| include: false

envelopes = readRDS("../data/envelopes.rds")
envelope_glm = envelopes$glm
envelope_en = envelopes$en
```



```{r}
#| eval: false
#| code-fold: true
par(mfrow=c(1,2))
plot(sort(res_glm), ylim = c(-5,5))
lines(sort(envelope_glm[,1]))
lines(sort(envelope_glm[,2]))

plot(sort(res_en), ylim = c(-5,5))
lines(sort(envelope_en[,1]))
lines(sort(envelope_en[,2]))
```

```{r}
#| include: false

png("../img/tmp/env21.png", width = 800, height = 400)
layout(matrix(c(1, 2), nrow = 1, ncol = 2))
plot(sort(res_glm))
lines(sort(envelope_glm[,1]))
lines(sort(envelope_glm[,2]))

plot(sort(res_en))
lines(sort(envelope_en[,1]))
lines(sort(envelope_en[,2]))
dev.off()

png("../img/tmp/env22.png", width = 800, height = 400)
layout(matrix(c(1, 2), nrow = 1, ncol = 2))
plot(sort(res_glm), ylim = c(-5,5))
lines(sort(envelope_glm[,1]))
lines(sort(envelope_glm[,2]))

plot(sort(res_en), ylim = c(-5,5))
lines(sort(envelope_en[,1]))
lines(sort(envelope_en[,2]))
dev.off()
```

<img class="flip"
     src="../img/tmp/env21.png"
     data-src1="../img/tmp/env21.png"
     data-src2="../img/tmp/env22.png"
     width="800" height="400"
     onclick="trocaImagem(this)">




```{r}
#| code-fold: true
cat("porcentagem dentro do envelope\n",
    "EN\t",
    sum(res_en > envelope_en[,1] & res_en < envelope_en[,2]) / length(res_en) * 100,
    "\nMLE\t",
    sum(res_glm > envelope_glm[,1] & res_glm < envelope_glm[,2]) / length(res_glm) * 100
)
```

```{r}
set.seed(12345)
x = cbind(1, rnorm(20))
e = rt(20, 1)
beta = c(5, 2)
y = x %*% beta + e

beta_lasso = 
  optim(
    par    = rep(0,2),
    fn     = sqres_lasso,
    X      = x,
    Y      = y,
    lambda = 100,
    method = "BFGS"
  )$par
beta_lm = lm(y~x[,-1])$coef |> unname()

res_lasso = y - x %*% beta_lasso
res_lm    = y - x %*% beta_lm
```

```{r}
#| eval: false
#| code-fold: true
par(mfrow=c(1,2))
qqnorm(res_lm, ylim = c(-10,10))
qqnorm(res_lasso, ylim = c(-10,10))
```

```{r}
#| include: false
png("../img/tmp/env31.png", width = 800, height = 400)
layout(matrix(c(1, 2), nrow = 1, ncol = 2))
qqnorm(res_lm,
       ylab = "lm")
qqnorm(res_lasso,
       ylab = "lasso")
dev.off()

png("../img/tmp/env32.png", width = 800, height = 400)
layout(matrix(c(1, 2), nrow = 1, ncol = 2))
qqnorm(res_lm,
       ylab = "lm",
       ylim = c(-10,10))
qqnorm(res_lasso,
       ylab = "lasso",
       ylim = c(-10,10))
dev.off()
```

<img class="flip"
     src="../img/tmp/env31.png"
     data-src1="../img/tmp/env31.png"
     data-src2="../img/tmp/env32.png"
     width="800" height="400"
     onclick="trocaImagem(this)">




### Bootstrap

Como um exemplo computaciona, optamos por fazer um bootstrap paramétrico a fim de entender o comportamento dos estimadores tradicional (IRLS via `glm`) e regularizado (IRLS-EN adaptado de @en_glm).
Opatamos por manter a mesma configuração de blocos e parâmetros descrita anteriormente. Poderiamos ter considerando a aleatoriedade das covariáveis no processo (dentro das especificações anteriores) - o que pode ser feito simplesmente passando o sorteio dos xizes para dentro do loop -, mas vimos que isso produziu um resultado similar ao com $X$ fixado (só um pouco menos dramático).

Seria interessante também fazer um estudo Monte Carlo, sorteando diferentes betas dentro da mesma estrutura e diferentes xizes para cada iteração Monte Carlo. Convidamos o leitor a fazer isso em seu tempo livre.

Por fim, vale ressaltar que o bootstrap se faz necessário pois não sabemos qual é a distribuição analítica nem dos betas estimados via IRLS nem dos estimados via IRLS-EN.

```{r}
set.seed(12345)

iter = 200
n = 200
bloco = 10

beta1 = c( runif(bloco,-0.7,0.7), rep(0,bloco) )
beta2 = c(0.6, rep(0,bloco-1))
beta = c(0.1,beta1,beta2)

estimativas = array(dim = c(2,length(beta),iter), dimnames = list(c("glm", "en")))

sigma =
  matrix(.9, nrow = bloco, ncol = bloco) |>
  `diag<-`(1)
X1 = matrix(rnorm(n*bloco*2), ncol = bloco*2)
X2 = mvtnorm::rmvnorm(n, sigma = sigma)
X = cbind(1,X1,X2)
eta = exp(X %*% beta)

for (i in 1:iter) {
  Y = rpois(n, eta)
  
  coef_en =
    glmnet::cv.glmnet(
      X[,-1], Y,
      family = "poisson",
      intercept = TRUE,
      alpha = 0.9,
      nfolds = 10
    ) |>
    coef(s = "lambda.1se") |>
    as.matrix() |>
    `dimnames<-`(NULL) |>
    c()
  
  coef_glm = 
    glm(Y~X[,-1], family = poisson())$coef |>
    unname()
  
  estimativas["en",,i]  = coef_en
  estimativas["glm",,i] = coef_glm
}
```

```{r}
#| code-fold: true
#| fig-cap: "Variâncias dos betas do grupo 1 (significativos)"

plot_hist_block = function(idxs, xlims) {
  par(mfrow = c(2, 3))
  for (metodo in c("glm", "en")) {
    for (i in seq_along(idxs)) {
      beta_idx = idxs[i]
      beta_number = beta_idx - 1   # como pedido
      
      hist(
        estimativas[metodo, beta_idx, ],
        main = paste0(metodo, " - β", beta_number),
        xlab = "", ylab = "",
        xlim = xlims[[i]]
      )
    }
    
  }
}

plot_hist_block(
  idxs = c(1, 4, 11),
  xlims = list(c(-0.2, 0.5), c(0.1, 0.5), c(0.5, 0.9))
)
```

```{r}
#| code-fold: true
#| fig-cap: "Variâncias dos betas do grupo 2 (não significativos)"

plot_hist_block(
  idxs = c(12, 14, 19),
  xlims = list(c(-0.2, 0.2), c(-0.2, 0.2), c(-0.2, 0.2))
)
```

```{r}
#| code-fold: true
#| fig-cap: "Variâncias dos betas do grupo 3 (covariáveis autocorrelacionadas)"

plot_hist_block(
  idxs = c(22, 23, 27),
  xlims = list(c(0, 1.2), c(-0.5, 0.5), c(-0.5, 0.5))
)
```

Os histogramas acima vão ao encontro do que a teoria diz. No primeiro bloco de covariáveis (intercepto mais covariáveis de 1 a 10) os betas reais são significativos e ambos os modelos captam isso - com um grau similar de variabilidade.

No segundo bloco de covariáveis, ambos os modelos captam a insignificância real dos betas, mas as estimativas do elastic net têm bem menos variabilidade que aquelas do IRLS tradicional.

No terceiro bloco de covariáveis - alta correlação das covariáveis e apenas o $\beta_{21}$ realmente diferente de zero -, ambos os modelos captam qual é a covariável significativa, mas - pelo menos nas variáveis não siginificativas - as estimativas do elastic net são menos dispersas que as do IRLS tradicional.

```{r}
#| warning: false
#| message: false
#| fig-width:  10
#| fig-height: 5
#| code-fold: true

variancias =
  apply(estimativas, c(1,2), var) |>
  t() |>
  tibble::as_tibble() |>
  cbind(c("intercepto", rep(c(1,2,3), each=10))) |>
  tibble::as_tibble() |>
  `colnames<-`(c("glm", "en", "grupo"))

library(ggplot2)

variancias_long =
  variancias[-1,] |>
  tidyr::pivot_longer(
    cols = c("glm", "en"),
    names_to = "metodo",
    values_to = "variancia")

ggplot(variancias_long,
       aes(x = grupo, y = variancia, fill = metodo)) +
  geom_boxplot(alpha = 0.7) +
  labs(x = "Grupo", y = "Variância",
       title = "Variância dos Betas por Grupo e Método") +
  theme_minimal(base_size = 14)
```

Os boxplots acima refletem o que já dissemos. As variabilidade das estimativas EN e tradicional são muito diferentes nos grupos 2 e 3 e razoavelmente similares no grupo 1.
Nesse plot foi ignorado o intercepto.

```{r}
#| warning: false
#| message: false
#| fig-width:  10
#| fig-height: 5
#| code-fold: true

vies2 =
  sweep(estimativas, 2, beta, FUN = "-")^2 |>
  apply(c(1,2), mean) |>
  t() |>
  tibble::as_tibble() |>
  cbind(c("intercepto", rep(c(1,2,3), each=10))) |>
  tibble::as_tibble() |>
  `colnames<-`(c("glm", "en", "grupo"))

vies2_long =
  vies2[-1,] |>
  tidyr::pivot_longer(
    cols = c("glm", "en"),
    names_to = "metodo",
    values_to = "vies2")

ggplot(vies2_long,
       aes(x = grupo, y = vies2, fill = metodo)) +
  geom_boxplot(alpha = 0.7) +
  labs(x = "Grupo", y = "Viés²",
       title = "Viés² dos Betas por Grupo e Método") +
  theme_minimal(base_size = 14)
```

O gráfico de boxplots acima mostra como os dois modelos se comportam em termos de viés ao quadrado. É possível notar que o viés da estatística EN é mais enviesada que a estatística tradicional no grupo com coeficientes diferentes de zero.

```{r}
#| warning: false
#| message: false
#| fig-width:  10
#| fig-height: 5
#| code-fold: true

norma =
  estimativas |>
  apply(c(1,3),
        function (x) c( l2 = crossprod(x[-1]), l1 = sum(abs(x[-1])) )) |>
  tibble::as_tibble() |>
  dplyr::mutate(
    norma = c("L2^2", "L1")
  ) |>
  tidyr::pivot_longer(
    cols = -norma,
    names_to = c("metodo", "iter"),
    names_sep = "\\.",
    values_to = "valor"
  ) |>
  dplyr::mutate(iter = as.integer(iter))

ggplot(norma,
       aes(x = metodo, y = valor, fill = norma)) +
  geom_boxplot(alpha = 0.7) +
  labs(x = "Método", y = "Norma",
       title = "Norma dos Betas (ao longo das iterações bootstrap)") +
  theme_minimal(base_size = 14)
```
